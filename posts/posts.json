[
  {
    "path": "posts/2021-08-03-tokyo-2020-iii/",
    "title": "Tokyo 2020 Betting III: From Matches to Medals... and Bookies",
    "description": "In this post I will go from making individual match predictions using the Bradley-Terry model through to predicting the Gold medal winners of the track cycling Individual Sprint, and derive an optimal betting strategy based on a spread-bet Kelly Criterion optimised under posterior uncertainty.",
    "author": [],
    "date": "2021-08-03",
    "categories": [
      "Bayesian",
      "Optimisation",
      "Decision Analysis",
      "Cycling"
    ],
    "contents": "\n\nContents\nTournament Forecasting\nDeriving the Betting Strategy\nBetting Log\nComments\nAcknowledgments\n\nThis is the third post in a series: Click for links.\nTokyo 2020 Betting I: Predictive Models for Pairwise Matches\nTokyo 2020 Betting II: Model Refinement and Feature Engineering\nTokyo 2020 Betting III: From Matches to Medals… and Bookies (This Post)\nDue to a lag in drafting vs analysis time, this post was originally published with betting stakes only. I have retrospectively added the detail of how these stakes were derived.\nSo far in this series I’ve focused on predicting the outcome of single matches between two athletes, and derived a bespoke Bradley-Terry model for this purpose.\nTo construct a betting strategy I will need to turn the probability that a given rider wins a single match, into the probability that they will win the whole tournament - and hence the gold medal.\nIn the first section of this post I introduce the tournament format for the Tokyo 2020 Individual Sprint, and how the outputs of the Bradley-Terry model are used to derive a distribution for the gold medal winner.\nIn the second part I will introduce the Kelly Criterion, and derive generalisations that allow for multiple outcome bets, handling posterior uncertainty, and accounting for additional caution.\nAt the end of the post I’ve included the log of the bets that I ended up placing, that were originally published in a holding post whilst the event was running.\nAssumptions\nThis post makes some assumptions about you!\nI’ll assume you’ve read the previous posts in the series, though this post should work as a standalone.\nI’ll also assume you have some knowledge of basic betting terminology (fractional odds, stakes). To derive my betting strategy I will use some discrete probability, and formalise a non-linear optimisation model.\nIf you’re interested in reading the underlying code, this is in R..\n\nThe full model code, from model fitting to betting stakes is here.\nTournament Forecasting\nThe Individual Sprint has a complex tournament structure, which will see the winning athlete compete between 10 and 16 sprints before they can claim the medal!\nThere are four main parts to the tournament, which in Tokyo 2020 will see 30 athletes compete:\n\n\nOverview\nA qualifying round that sees all athletes competing individually to set the fastest time, with the six slowest athletes eliminated.\n1/32, 1/16 and 1/8 Finals that see the athletes compete in pairs to win a single sprint. The winner automatically qualifies for the next round (eg. 1/32 Finals winners qualify for 1/16 Finals).\nRepechage races that see the losers from the previous round competing to take take any remaining places in the next round (eg. losers of 1/32 Finals compete for four remaining places in the 1/16 Finals).\nQuarter-, Semi-, and Finals raced between pairs of riders. At this stage each match is contested in a best of three sprints format.\n\n\nTokyo 2020 Summary\nRound\nAthletes Competing\nMatches x Athletes per Match\nSprints per Match\nAthletes Qualifying\nQualifying\n30\n30 x 1\n1\n24\n1/32 Finals\n24\n12 x 2\n1\n12\nRepechage 1\n12\n4 x 3\n1\n4\n1/16 Finals\n16\n8 x 2\n1\n8\nRepechage 2\n8\n4 x 2\n1\n4\n1/8 Finals\n12\n6 x 2\n1\n6\nRepechage 3\n6\n2 x 3\n1\n2\nQuarterfinals\n8\n4 x 2\nBest of 3\n4\nSemifinals\n4\n2 x 2\nBest of 3\n2\nFinals\n2\n2 x 1\nBest of 3\n1\n\n\nTokyo 2020 Detail The tables below provide the detail that determines which riders face each other in each round; its adapted from the table published by the UCI in their Track Regulations.\nThe initial rider codes N1-N24 are in order of the time posted in the qualifying round: N1 is the fastest qualifier, N24 the slowest.\n\n1/32 Finals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\n1/32 Finals\n1\nN1\nN24\nNA\n1A1\n1A2\n1\n1/32 Finals\n2\nN2\nN23\nNA\n2A1\n2A2\n1\n1/32 Finals\n3\nN3\nN22\nNA\n3A1\n3A2\n1\n1/32 Finals\n4\nN4\nN21\nNA\n4A1\n4A2\n1\n1/32 Finals\n5\nN5\nN20\nNA\n5A1\n5A2\n1\n1/32 Finals\n6\nN6\nN19\nNA\n6A1\n6A2\n1\n1/32 Finals\n7\nN7\nN18\nNA\n7A1\n7A2\n1\n1/32 Finals\n8\nN8\nN17\nNA\n8A1\n8A2\n1\n1/32 Finals\n9\nN9\nN16\nNA\n9A1\n9A2\n1\n1/32 Finals\n10\nN10\nN15\nNA\n10A1\n10A2\n1\n1/32 Finals\n11\nN11\nN14\nNA\n11A1\n11A2\n1\n1/32 Finals\n12\nN12\nN13\nNA\n12A1\n12A2\n1\nRepechage 1\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nRepechage 1\n13\n1A2\n8A2\n9A2\n1B\nNA\n1\nRepechage 1\n14\n2A2\n7A2\n10A2\n2B\nNA\n1\nRepechage 1\n15\n3A2\n6A2\n11A2\n3B\nNA\n1\nRepechage 1\n16\n4A2\n5A2\n12A2\n4B\nNA\n1\n1/16 Finals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\n1/16 Finals\n17\n1A1\n4B\nNA\n1C1\n1C2\n1\n1/16 Finals\n18\n2A1\n3B\nNA\n2C1\n2C2\n1\n1/16 Finals\n19\n3A1\n2B\nNA\n3C1\n3C2\n1\n1/16 Finals\n20\n4A1\n1B\nNA\n4C1\n4C2\n1\n1/16 Finals\n21\n5A1\n12A1\nNA\n5C1\n5C2\n1\n1/16 Finals\n22\n6A1\n11A1\nNA\n6C1\n6C2\n1\n1/16 Finals\n23\n7A1\n10A1\nNA\n7C1\n7C2\n1\n1/16 Finals\n24\n8A1\n9A1\nNA\n8C1\n8C2\n1\nRepechage 2\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nRepechage 2\n25\n1C2\n8C2\nNA\n1D1\nNA\n1\nRepechage 2\n26\n2C2\n7C2\nNA\n2D1\nNA\n1\nRepechage 2\n27\n3C2\n6C2\nNA\n3D1\nNA\n1\nRepechage 2\n28\n4C2\n5C2\nNA\n4D1\nNA\n1\n1/8 Finals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\n1/8 Finals\n29\n1C1\n4D1\nNA\n10\n100\n1\n1/8 Finals\n30\n2C1\n3D1\nNA\n20\n200\n1\n1/8 Finals\n31\n3C1\n2D1\nNA\n30\n300\n1\n1/8 Finals\n32\n4C1\n1D1\nNA\n40\n400\n1\n1/8 Finals\n33\n5C1\n8C1\nNA\n50\n500\n1\n1/8 Finals\n34\n6C1\n7C1\nNA\n60\n600\n1\nRepechage 3\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nRepechage 3\n35\n100\n400\n500\n1F1\nNA\n1\nRepechage 3\n36\n200\n300\n600\n2F1\nNA\n1\nQuarterfinals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nQuarterfinals\n37\n10\n2F1\nNA\n1G1\nNA\n2\nQuarterfinals\n38\n20\n1F1\nNA\n2G1\nNA\n2\nQuarterfinals\n39\n30\n60\nNA\n3G1\nNA\n2\nQuarterfinals\n40\n40\n50\nNA\n4G1\nNA\n2\nSemifinals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nSemifinals\n41\n1G1\n4G1\nNA\n1H1\n1H2\n2\nSemifinals\n42\n2G1\n3G1\nNA\n2H1\n2H2\n2\nFinals\nround\nmatch_no\nrider_code_1\nrider_code_2\nrider_code_3\nwinner_code\nloser_code\nsprints\nFinals\n43\n1H1\n2H1\nNA\nGold\nSilver\n2\nFinals\n44\n1H2\n2H2\nNA\nBronze\nNA\n2\n\n\n\nTo forecast the gold medal winner I will simulate results for each match in the tournament, using the detailed tournament structure tables above.\nIn short, simulating the tournament will involve:\nUsing the rider qualifying times to pair riders based on the details in the 1/32 Finals table (in the Tokyo 2020 Detail tab above).\nSample the winner/loser of each match using the Bernoulli distribution that is implied by the Bradley-Terry model (recapped below). Assign each rider the appropriate winner/loser code, from the detailed table.\nRepeat the above for each of the successive rounds, until the Gold medal winner is decided.\nThat sounds simple enough, but there are a few things for us to unpack here.\nSampling a Single Match (Recap of the Bradley-Terry model)\nThe Bradley-Terry model assumes that in a match between athletes \\(r\\) and \\(s\\) then\n\\[\\mathbf P[r \\text{ beats } s] = \\frac{\\beta_r}{\\beta_r + \\beta_s}.\\]\nGiven a set of parameters \\((\\beta_r)\\) I can use the formula above to sample the winner of any given match, as required in step two above.\nThe previous posts have focused on estimating the parameters \\(\\beta_r\\), with the final model taking the form \\(\\beta_r = \\exp \\left( \\alpha_r^{(m)} + \\kappa t_r \\right)\\), where\n\\(\\alpha_r^{(m)}\\) is the estimated athlete strength going into the match, \\(m\\), taking into account time varying effects and a home advantage (in the case of Tokyo only affecting the two Japanese competitors).\n\\(t_r\\) is the athlete’s qualifying time in the current competition, and along with the estimated coefficient \\(\\kappa\\) this allows us to take into account the athlete’s current form.\nSampling a Tournament\nSampling a tournament is just a case of sampling match outcomes, and then using the detailed tournament information in the tabs above to identify who to pair in the next round of matches. The animation below gives an example of this dynamic.\n\n\n\nThere are two nuances that require minor tweaks. The first is that some of the repechage races, are contested by three athletes not two. This has a simple resolution as the Bradley-Terry model can be extended to consider matches between three (or more) athletes as follows\n\\[\n\\begin{align}\n\\mathbf P[r \\text{ beats } s \\text{ and } q\\,] = \\frac{\\beta_r}{\\beta_r + \\beta_s + \\beta_q}.\n\\end{align}\n\\]\nThe second is that from the quarterfinals onward, matches are contested in a best-of-three format, and this changes the probability of winning. This was covered in detail in the previous post so I won’t recap here.\nMonte-Carlo Uncertainty\nTournament results derived using the simulation algorithm set out above will have uncertainty since I am sampling the winner of each match from a probability distribution.\nTo account for this, I won’t just sample a tournament once - I’ll repeat the tournament multiple times (independently) from which I can say not only the most likely winner of the Gold medal, but also the probability that they will win: this probability will be key for informing when to bet.\n\nThis is a simple example of a Monte Carlo algorithm.\nRunning the simulation 1000 times on the same set of parameters as were used above, we can report the frequency with which each rider wins the gold medal.\n\nShow code\nolympic_rounds <- read_remote_target('fcst_rounds_Men')\nstrength_draw <- read_remote_target('fcst_strength_draws_Men') %>% filter(.draw == 1)\n\n# set time to 0, assuming that the qualifying times are not yet observed\nstrength_draw <- strength_draw %>% mutate(time = 0)\n\nset.seed(14142)\ngold_samples <- forecast_tournament(strength_draw, olympic_rounds, samples = 5000, accumulate = FALSE, gold_only = TRUE)\n\ngold_probs <- gold_samples %>%\n  count(rider, sort = TRUE) %>%\n  mutate(p = percent(n/sum(n),digits=0)) %>%\n  filter(p > 0.005)\n  \ngold_probs %>%\n  kable(\"pipe\", col.names = c('Athlete', 'Gold Frequency', 'Gold %')) %>%\n  kable_styling(bootstrap_options = \"condensed\", full_width = FALSE, position = \"center\", font_size = 14)\n\n\nAthlete\nGold Frequency\nGold %\nLAVREYSEN HARRIE\n4601\n92%\nHOOGLAND JEFFREY\n255\n5%\nGLAETZER MATTHEW\n68\n1%\nCARLIN JACK\n35\n1%\nHART NATHAN\n28\n1%\n\nUnder the assumption that the model is correct and we’ve learnt the exact true parameter values, then the percentage figures can be interpreted as the probability that the athlete will win the gold medal.\nBayesian Uncertainty\nIn reality my model is just that - a model, so it won’t exactly capture reality, and further the parameters are estimates based on historic data: so in addition to the uncertainty implied by the model (the Monte Carlo uncertainty, above), I have reason to be uncertain about the model itself.\nThis uncertainty in the model is captured by the Bayesian posterior distribution of the model parameters. Since I fit the model in Stan, I have samples from the posterior distribution so I can propagate my uncertainty about the model parameters to uncertainty about the gold medal probabilities by running tournament simulations against each draw from the posterior distribution.\nThe examples below show the gold medal distributions for the top three athletes going into the Tokyo 2020 Olympics, with the vertical lines denoting the posterior mean.\n\nShow code\nbayes_gold_probs <- read_remote_target(\"fcst_gold_probs_Men\")\n\nmost_probable <-  bayes_gold_probs %>%\n  group_by(rider) %>%\n  summarise(\n    mean_gold_prob = mean(gold_prob)\n  ) %>%\n  ungroup() %>%\n  mutate(rank = rank(-mean_gold_prob))\n\nbayes_gold_probs <- bayes_gold_probs %>%\n  left_join(most_probable) %>%\n  mutate(rider = fct_reorder(rider, rank))\n\nggplot(bayes_gold_probs %>% filter(rank <= 3)) +\n  geom_histogram(aes(gold_prob), binwidth = 0.1, color = infreq_palette[\"beige\"]) +\n  geom_vline(data = most_probable %>% filter(rank <= 3), aes(xintercept = mean_gold_prob), color = infreq_palette[\"darkblue\"], linetype = \"dashed\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1), labels = scales::percent_format(accuracy=1)) +\n  facet_wrap(vars(rider), strip.position = \"bottom\") +\n  theme(\n    axis.title = element_blank(),\n    axis.line = element_blank(),\n    axis.ticks = element_blank(),\n    axis.text.y = element_blank(),\n    strip.text=element_text(size = 12, color = infreq_palette[\"darkblue\"]),\n    strip.background = element_rect(color=infreq_palette[\"beige\"], fill=infreq_palette[\"beige\"]),\n    strip.placement = \"outside\"\n  )\n\n\n\n\n\nPosterior gold medal probabilities, based on a model run prior to qualifying - so all athletes were given the same qualifying time.\nThe distributions above are a good reminder that uncertainty in the real world isn’t all about normal distributions; in particular their bimodal nature indicates that an analysis based on a single point estimate for each athlete is bound to gloss over this structural nuance - further justifying the need to account for uncertainty in the modelling.\nThis is one of the main benefits of fitting a Bayesian model - If my Bradley-Terry model was in the frequentist framework, I would have single parameter estimates for the athlete strenghts and confidence intervals for those values. Unlike Bayesian posteriors, frequentist confidence intervals don’t imply distributions, so it isn’t possible to sample from them. Ultimately this means its not clear how to derive uncertainty intervals for the gold medal probability.\n\nShort of writing a likelihood that encapsulates both the Bradley-Terry model and the tournament dynamic - but that is daunting, if doable at all.\nThis ability to propagate the uncertainty from the original Bradley-Terry model, to downstream calculations is one of my motivations for preferring Bayesian models over their frequentist equivalents:\nDeriving the Betting Strategy\nWith predictions for the likely winners, its time to think about how best to use these predictions to inform a betting strategy.\nThe simple strategy would be to place bets on the athletes who you believe are most likely to win, but this is sub-optimal for a number of reasons:\nThis strategy does not inform us of how much to bet, only who to bet on.\nDepending on the odds offered by the bookies, the risk may not be worth the return: i.e. if I forecast that Lavreyson has a probability 0.9 of winning, but the bookies are offering odds of 1/9, then the expected return on a £1 bet is £0.90, so I can expect to lose money.\n\nFractional odds of \\(a/b\\) mean that a winning bet of 1 unit will receive back \\(1 + a/b\\), the original stake plus the fractional odds.\nFrom the histograms above we can see that the most likely winner varies across our posterior uncertainty - so I’d at least need to define a way to identify this person across all draws.\nFortunately there’s pre-existing literature on betting strategies that I can draw on for my application, namely I’ll consider a variant of the classic Kelly Criterion.\nBetting Strategy for Single Bets\nTo set-up the theory, I’ll consider a simplified scenario with just two outcomes.\nSuppose I’m offered odds \\(O\\) to play a game for which I know ahead of time that the probability of winning is \\(p\\). If I have a starting budget \\(W^{(0)}\\) that I’m willing to invest in gambling, then what fraction \\(f\\) of this should I be willing to bet?\nIf I’m only ever going to place a single bet, intuition would suggest that I should either bet my entire budget, \\(f = 1\\), (if I think the odds are favourable), or I should refrain from betting entirely, \\(f = 0\\).\nThis aligns to the strategy in which my aim is to maximise my expected return. To see this, my expected wealth after betting \\(0 \\leq f \\leq 1\\) of my budget is\n\\[\n\\begin{align}\n\\mathbf E \\left[ W^{(1)} \\right ] & = \\underbrace{(1-p) \\times (1-f)W^{(0)}}_{\\text{I lose the bet}} + \\underbrace{p \\times (W^{(0)} + fW^{(0)}O)}_{\\text{I win}} \\\\\n\\\\\n& = (1-f)W^{(0)} + fW^{(0)}p(1+O)\n\\end{align}\n\\]\nSince the formula is linear in \\(f\\) the maximum will be achieved at one of \\(f = 0\\) (if the derivative, in \\(f\\), of the expression above is negative) or \\(f = 1\\) (if its positive). Calculating the derivative we find that the strategy that maximises expected wealth is:\n\\[\\text{If $p > (1+O)^{-1}$ bet the full budget ($f = 1$), otherwise do not bet ($f = 0$).}\\]\nNow suppose however that I intend to place more than one bet, and moreover that I’m willing to reinvest my previous winnings into future bets: is it still wise that so long as the odds are favourable, I should bet my entire budget each time?\nProceeding as above, one can show that the expected wealth after playing \\(n\\) games (or until I go bust, whichever happens first) is still maximised by betting my full budget, including reinvestment, and this has an expected return of\n\\[\\mathbf E \\big[ W^{(n)} \\big] = \\left\\{pW^{(0)}(1+O)\\right\\}^n.\\]\nSince this tends to infinity with \\(n\\), it would suggest that not only should I bet my entire budget each time I win, but I should be willing to keep playing for arbitrarily long.\nBut that strategy is absurd: eventually I am guaranteed to lose one of the bets, losing my entire accrued budget, and I won’t be able to bet anymore.\n\nThis is similar to the St. Petersburg paradox.\nThis is demonstrated in the example below where I’ve simulated the outcome of 1,000 independent gamblers with a starting budget \\(W^{(0)} = 1\\), using this strategy to play a game with probability \\(p = 2/3\\) of winning, offered at odds of 2 - 1, \\(O = 2\\), (so that the expected return from a single game is 2). For the simulation I’ve run to a maximum of 20 games.\n\nShow code\nset.seed(1414214)\n\n# samples the outcome of betting on a single game, and returns the winnings\n# based on using the strategy that maximises expected wealth.\nr_game_max_exp_wealth <- function(B, p, O){\n  \n  # if no wealth, don't bet\n  if(B == 0){return(0)}\n  \n  # don't bet if the odds are not in favour\n  if(p < (1+O)^{-1}){ return(B)}\n  \n  # otherwise bet all of the budget\n  outcome <- rbinom(1, size = 1, prob = p)\n  \n  wealth <- B * (1+O) * outcome\n\n  return(wealth)\n}\n\n# samples a sequence of n identical games assuming returns are reinvested into \n# the budget, and that the strategy is to maximise expected wealth\nr_seq_games_max_exp_wealth <- function(n, B, p, O){\n  rep(O,n) %>% accumulate(.f = function(B,O){r_game_max_exp_wealth(B,p,O)}, .init = 1)  \n}\n\n# multiple draws for a gambler who plays the game 20 times, withan initial budget of 1\n# probability of winning 2/3 and odds of 2-1.\ngame_samples <- tibble(sample = 1:1000) %>%\n  rowwise() %>%\n  mutate(run = map(sample, ~tibble(game = 0:20, budget = r_seq_games_max_exp_wealth(20,1,2/3,2)))) %>%\n  unnest(run)\n\n# for plotting, add zeros so that vertical lines show where the budget goes to zero.\nadd_zeros <- game_samples %>%\n  filter(budget == 0) %>%\n  group_by(sample) %>%\n  slice(which.min(game)) %>%\n  mutate(game = game - 1) %>%\n  ungroup()\n\ngame_samples <- bind_rows(game_samples, add_zeros)  \n\n# plot game outcomes\np_max_exp_wealth <- ggplot(game_samples) + geom_line(aes(game, log10(budget), group = sample), alpha = 0.05) +\n  scale_y_continuous(breaks = seq(0,9,by=3), labels = scales::math_format(10^.x)) +\n  xlab(\"Game\") + ylab(\"Expected Wealth\")\n\np_max_exp_wealth\n\n\n\n\nOf the 1,000 gamblers sampled, 0 made it to the 20th game without going bust.\nThe message to take away is that maximising expected wealth is not a sensible strategy for repeated betting with reinvestment.\nThe Kelly Criterion (for Binary Outcomes)\nThe optimal strategy for the scenario of repeated bets with reinvestment was identified by John Kelly in 1956; the original paper is relatively accessible reading.\nKelly’s observation is that rather than trying to maximise expected wealth, we should aim to maximise the growth rate of wealth. Typically average growth rate is defined by the geometric mean:\n\\[\\left( \\frac{W^{(1)}}{W^{(0)}} \\frac{W^{(2)}}{W^{(1)}} \\cdots \\frac{W^{(n)}}{W^{(n-1)}} \\right)^{\\frac1n} = \\left(\\frac{W^{(n)}}{W^{(0)}}\\right)^{\\frac1n}\\] This in turn can be simplified by noting that the maximum of this expression will be achieved by the same value \\(f\\) as maximising the logarithm\n\\[ \\log \\left(\\frac{W^{(n)}}{W^{(0)}}\\right)^{\\frac1n} = \\frac1n \\log W^{(n)} - \\frac1nW^{(0)},\\] and further we can drop the last term and the multiplication by \\(\\frac1n\\) as they’re both constant. So that means that the strategy \\(f\\) that maximises expected growth rate is the same as the strategy that maximises log wealth.\nThe formula for log wealth after a single game is \\[\n\\begin{align}\n\\mathbf E \\left[ \\log W^{(1)} \\right] &= (1-p) \\log\\left((1-f)W^{(0)}\\right) + p \\log\\left( W^{(0)} + fW^{(0)}O\\right)  \\\\\n& = \\log\\left(W^{(0)}\\right) + (1-p)\\log\\left(1-f\\right) + p\\log\\left(1 + fO\\right)\n\\end{align}\n\\]\nThe plot below shows how expected log wealth varies as we change the fraction of wealth invested, \\(f\\), assuming the same parameter values in the simulation above: \\(W^{(0)} = 1,\\, p = 2/3, \\, O = 2\\).\n\nShow code\nggplot(tibble(f=c(0,0.99), y = c(-1,1)), aes(f,y)) +\n  stat_function(aes(f), fun = function(f) 1/3 *log(1-f) + 2/3 * log(1 + 2*f), color = infreq_palette[\"darkblue\"]) +\n  # maximum achieved at f = 1/2\n  geom_segment(x = 0.5, xend = 0.5, y = -1, yend= 1/3 *log(1/2) + 2/3 * log(2), color = infreq_palette[\"orange\"], linetype = \"dashed\") +\n  xlab(\"Fraction of Wealth Gambled\") + ylab(\"Expected Log Wealth\")\n\n\n\n\nFor this particular set of parameters the maximum is achieved at \\(f = \\frac12\\), so the Kelly bet would be to always bet half of your current budget in the next round. More generally, the equation above can be solved by hand, and the optimal bet is given by\n\nUsing the standard method of solving for zeros of the derivative.\n\\[f^* = p - O^{-1}(1-p).\\]\nThe below overlays the outcome of a further 1,000 gamblers playing the same game as before, but adopting the Kelly Criterion.\n\nShow code\nset.seed(1414214)\n\n# samples the outcome of betting on a single game, and returns the winnings\n# based on using the strategy that maximises expected log wealth (Kelly bet).\nr_game_max_log_wealth <- function(B, p, O){\n  \n  # if no wealth, don't bet\n  if(B == 0){return(0)}\n  \n  # outcome of the game (1 = win, 0 = lose)\n  outcome <- rbinom(1, size = 1, prob = p)\n  \n  kelly_bet <- B * (p -(1-p)/O)\n  \n  wealth <- if_else(outcome == 0, B - kelly_bet, B + kelly_bet * O)\n\n  return(wealth)\n}\n\n# samples a sequence of n identical games assuming returns are reinvested into \n# the budget, and that the strategy is to maximise expected wealth\nr_seq_games_max_log_wealth <- function(n, B, p, O){\n  rep(O,n) %>% accumulate(.f = function(B,O){r_game_max_log_wealth(B,p,O)}, .init = 1)  \n}\n\n# multiple draws for a gambler who plays the game 20 times, withan initial budget of 1\n# probability of winning 2/3 and odds of 2-1.\ngame_kelly_samples <- tibble(sample = 1:1000) %>%\n  rowwise() %>%\n  mutate(run = map(sample, ~tibble(game = 0:20, budget = r_seq_games_max_log_wealth(20,1,2/3,2)))) %>%\n  unnest(run)\n\n\n# plot game outcomes\np_kelly <- p_max_exp_wealth +\n  geom_line(data = game_kelly_samples, aes(game, log10(budget), group = sample), alpha = 0.025, color = infreq_palette[\"darkblue\"])\n\np_kelly\n\n\n\n\nWe previously saw that 0 of the gamblers maximising expected wealth had any money at the end of 20 games, whereas 909 of the 1,000 gamblers employing the Kelly strategy had increased their budget by the end of 20 games.\nKelly’s result is that using this strategy is guaranteed in the limit (i.e. if you were to have infinitely many opportunities to play the game) to lead to higher wealth than any other betting strategy.\nIts worth noting of course that there are two major assumptions in the theoretical model that won’t hold in the real world:\nthe gambler is given the opportunity to play the game infinitely often: nobody has that much time on the planet, let alone time to play games,\nthe gambler knows the true probability of winning the game.\nWhilst the theoretical Kelly strategy optimises risk and reward - the limitations above mean that in the real world it is actually more aggressive than is perhaps appropriate. A common way to bet with more caution is to use a fractional Kelly strategy: betting say half the proposed Kelly strategy.\nI’ll look at a Bayesian way to introduce caution at the end of the post.\nAllowing for Spread Betting\nI’ll now turn to the specifics of implementing the Kelly Strategy for my Olympic betting: first looking at how to generalise the criteria for games with more than two outcomes\nThe bets I’m looking to place are on the Gold medal winner for each of the men’s and women’s Individual Sprints; so for now that’s a total of two games, far from the infinite number underpinning the theory, but if all goes well that’s not to say I won’t bet on the World Championships in October!\nFor now I’ll consider a single one of the two events, let’s say the Men’s, and suppose that I have a fixed set of probabilities that describe the likelihood of each rider winning: \\((p_r)_{r = 1}^R\\).\n\nHow to account for the Bayesian uncertainty is the content of the next section.\nExtending the notation from before, I’ll suppose I’ve found a bookmaker (or even a number of independent bookmakers) willing to offer odds \\(O_r\\) on rider \\(r\\). Given a starting budget \\(W^{(0)}\\), what fraction \\(f_r\\) of this should I invest in rider \\(r\\)?\nApplying the heuristic of Kelly’s criterion, I’ll aim to maximise my log wealth, which after a single game in the multi-outcome scenario is given by:\n\\[\n\\begin{align}\n\\mathbf E \\left[ \\log W^{(1)} \\right] & = \\sum_{r = 1}^R \\, p_r \\, \\times \\, \\log \\bigg(\\underbrace{W^{(0)}(1-F)}_{\\text{Not gambled}} + \\underbrace{ W^{(0)}f_rO_r}_{\\text{r wins}}\\bigg) \\\\\n\\\\\n&=  \\log W^{(0)} + \\sum_{r = 1}^R  p_r \\log(1 - F + f_rO_r)\n\\end{align}\n\\]\nwhere \\(F = \\sum_r f_r\\) is the total fraction of wealth invested across all athletes.\nThe spread-bet Kelly criterion will therefore look to find the fractions \\((f_r)_{r=1}^R\\) that maximise this expression, subject to the constraints that the fractions must be positive (I can’t bet a negative proportion of my wealth) and must sum to less than \\(1\\) (I cannot exceed my budget).\nThat is we have the following non-linear constrained optimisation problem:\n\\[\n\\begin{aligned}\n& \\text{maximize}\n& &  \\textstyle{\\sum_{r=1}^R} p_r \\log (1 - F + f_rO_r) \\\\\n& \\text{subject to}\n& &   f_r \\geq 0,  \\,\\qquad 1 \\leq r \\leq R \\\\\n& & & \\textstyle{\\sum_{r=1}^R} f_r = F \\leq 1 \n\\end{aligned}\n\\]\nRather than trying to solve this analytically I’ll us computational solvers; in this instance I’ll use the NLopt package of solvers through the R library nloptr.\nNLopt is a powerful package, with a range of different optimisation algorithms; I opted for the COBYLA (Constrained Optimisation by Linear Approximations) algorithm, which benefits from not requiring the user to derive and supply the gradient of the function to be optimised.\nThe code below provides a minimal example showing how to set-up NLopt to run the COBYLA algorithm for the simple two-outcome Kelly strategy described in the previous section.\nnloptR: Minimal Example I’ll find the solution to the original Kelly Criterion, based on the example game discussed in previous sections. The aim is to solve:\n\\[\n\\begin{aligned}\n& \\text{maximize}\n& &  \\frac13 \\log(1-f) + \\frac23 \\log(1 + 2f) \\\\\n& \\text{subject to}\n& &   0 \\leq f \\leq 1\n\\end{aligned}\n\\] We already saw that this can be solved by hand, and that the maximum is achieved at \\(f^* = \\frac12\\).\nThe code below provides a computational approximation to the solution:\n\n\nlibrary(nloptr)\n\n# expected logarithm of wealth after one round of the game described above.\nexpected_log_wealth <- function(f){ 1/3 * log(1-f) + 2/3 * log(1 + 2*f)}\n\n# nloptr is configured to always find minima; so we will need to use the\n# negative of the above (the minima of this coincides with the maxima of\n# the original function).\nneg_expected_log_wealth <- function(f){ -1 * expected_log_wealth(f)}\n\nsolve <- nloptr(\n  # guess of the value f at which the maximum might be achieved; for\n  # demonstration purposes deliberately avoiding 0.5.\n  x0 = 0.1,\n  \n  # the function to minimise\n  eval_f = neg_expected_log_wealth,\n  \n  # lower/upper bound constraint on f\n  lb = 0,\n  ub = 1,\n  \n  opts = list(\n    # the algorithm to use; I'm using COBYLA.\n    \"algorithm\"=\"NLOPT_LN_COBYLA\",\n    \n    # the tolerance at which the algorithm will decide it has converged\n    \"xtol_rel\"=1.0e-4,\n    \n    # the maximum number of iterations to try before stopping\n    \"maxeval\"=1e5\n  )\n) \n\nprint(solve)\n\n\n\nCall:\n\nnloptr(x0 = 0.1, eval_f = neg_expected_log_wealth, lb = 0, ub = 1, \n    opts = list(algorithm = \"NLOPT_LN_COBYLA\", xtol_rel = 1e-04, \n        maxeval = 1e+05))\n\n\nMinimization using NLopt version 2.4.2 \n\nNLopt solver status: 4 ( NLOPT_XTOL_REACHED: Optimization stopped \nbecause xtol_rel or xtol_abs (above) was reached. )\n\nNumber of Iterations....: 28 \nTermination conditions:  xtol_rel: 1e-04    maxeval: 1e+05 \nNumber of inequality constraints:  0 \nNumber of equality constraints:    0 \nOptimal value of objective function:  -0.2310490601075 \nOptimal value of controls: 0.5000089\n\nLooking at the output above, the algorithm ran for a total of solve[[\"iterations\"]] before converging to a solution within the provided tolerance; moreover it estimates \\(f^* \\approx 0.5000089\\), which is a good approximation to the true solution.\nReturning to the cycling model, we’re now in a position to optimise a betting strategy based on a fixed point estimate of the winning probabilities. For this I’ll use the posterior mean winning probabilities returned from the Bayesian model run with historic data, and prior to the qualifying round at Tokyo 2020. I’m using the means as unlike the posterior mode (MAP) or median, these have the property that summing over the means of all riders returns 1, so they define a proper probability distribution.\nOdds were available at Unibet.com, and for this analysis I’m using the odds offered on 3rd August 2020 - the day before the qualifying round for the Men’s event.\nThe plot below shows my point estimates of the athlete’s winning probabilities, against the odds on offer; the curve denotes the boundary at which betting on the rider is expected to have a positive return, and our intuition would say that any betting strategy should avoid placing money on athletes who are below this line.\n\nShow code\nodds <- read_csv(\"../../../track-cycling/data/202021_2020-TOKYO-OLYMPICS_Odds.csv\") %>%\n  filter(odds_date == ymd(20210803), gender == 'Men') %>%\n  select(rider, fractional_odds) %>%\n  mutate(\n    odds = 1 + fractional_odds,\n    # the probability the bookies imply they believe\n    implied_probability = 1/odds,\n    # account for the fact that they run a 'Dutch Book'\n    normalised_implied_probability = implied_probability / sum(implied_probability)\n  )\n\nodds <- odds %>% left_join(most_probable) %>%\n  mutate(\n    annotation = if_else(\n      mean_gold_prob > odds^(-1),\n      # rider %in% c(\"LAVREYSEN HARRIE\", \"GLAETZER MATTHEW\"),\n      str_extract(rider, \"^([^ ])+\"), \"\"),\n  )\n\nggplot(odds) +\n  geom_point(aes(mean_gold_prob, fractional_odds), size =2) +\n  geom_text(aes(mean_gold_prob, fractional_odds, label = annotation), hjust =-0.2, color = infreq_palette[\"darkblue\"]) +\n  stat_function(data = tibble(p = c(0.05,1)), aes(p), fun = function(p) 1/p -1, color = infreq_palette[\"darkblue\"], linetype = \"dashed\") +\n  scale_x_continuous(breaks = seq(0,1,by = 0.2)) +\n  coord_cartesian(xlim = c(0,1)) +\n  xlab(\"Posterior Mean Gold Probability\") +\n  ylab(\"Odds Offered\")\n\n\n\n\nSolving the optimisation problem for these inputs results in the following proposed bets:\n\nThe full code for optimising the multi-outcome bet is available on GitHub.\n\nShow code\n# summarise point estimate (mean posterior probability) winning probabilities\npoint_estimate_probs <- most_probable %>%\n  transmute(.draw = 1, rider, gold_prob = mean_gold_prob)\n\n# solve the multi-outcome spread-bet Kelly strategy; note since only a single draw\n# is provided using the optimise_kelly_bayes function does optimisation without\n# uncertainty (eg. ignore the Bayes bit)\npoint_estimate_strategy <- optimise_kelly_bayes(odds, point_estimate_probs)\n\npoint_estimate_strategy <- point_estimate_strategy %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2)) %>%\n  select(rider, fractional_odds, mean_gold_prob, kelly_stakes)\n\n\n\n\nAthlete\nOdds\nModelled Gold Prob.\nStake\nGLAETZER MATTHEW\n14.0\n0.22\n0.17\nLAVREYSEN HARRIE\n0.7\n0.52\n0.07\n\nAll Other Odds\nAthlete\nOdds\nModelled Gold Prob.\nStake\nHOOGLAND JEFFREY\n3.0\n0.15\n0\nKENNY JASON\n6.5\n0.00\n0\nDMITRIEV DENIS\n12.0\n0.01\n0\nRUDYK MATEUSZ\n14.0\n0.02\n0\nAWANG MOHD AZIZULHASNI\n18.0\n0.00\n0\nCARLIN JACK\n20.0\n0.03\n0\nBOTTICHER STEFAN\n25.0\n0.00\n0\nWEBSTER SAM\n25.0\n0.00\n0\nLEVY MAXIMILIAN\n33.0\n0.01\n0\nMITCHELL ETHAN\n33.0\n0.00\n0\nNITTA YUDAI\n40.0\n0.01\n0\nVIGIER SEBASTIEN\n40.0\n0.01\n0\nHART NATHAN\n50.0\n0.01\n0\nHELAL RAYAN\n66.0\n0.01\n0\nBABEK TOMAS\n80.0\n0.00\n0\nQUINTERO CHAVARRO KEVIN SANTIAGO\n80.0\n0.00\n0\nBARRETTE HUGO\n100.0\n0.00\n0\nPAUL NICHOLAS\n100.0\n0.00\n0\nRAJKOWSKI PATRYK\n100.0\n0.00\n0\nXU CHAO\n100.0\n0.00\n0\nBROWNE KWESI\n150.0\n0.00\n0\nSPIES JEAN\n150.0\n0.00\n0\nTJON EN FA JAIR\n150.0\n0.00\n0\nWAKIMOTO YUTA\n150.0\n0.00\n0\nYAKUSHEVSKIY PAVEL\n150.0\n0.00\n0\nPONOMARYOV SERGEY\n200.0\n0.00\n0\nSAHROM MUHAMMAD SHAH FIRDAUS\n200.0\n0.00\n0\nWAMMES NICK\n200.0\n0.00\n0\n\nOverall the strategy suggests I should bet on just 2 of the athletes, and invest 24% of my total budget.\nInterestingly the solution is proposing that I should bet on Harrie Lavreysen, even though the plot above indicates that this bet is expected to lose money. My intuition here is that perhaps this is to offset the riskier bet on Matthew Glaetzer - though I haven’t fully worked through this yet.\n\nMy alternative intuition is that I have a bug in my code, or the solver hit its tolerance.\nAccounting for Bayesian Uncertainty\nEarlier in the post we saw that the point estimates for each athlete’s probability of winning are a somewhat unsatisfactory summary of the underlying distributions which are bi-model.\nThe plot below replicates the above - but replacing the single point estimate with all the draws from the posterior. For clarity I’m only showing the four riders with the highest posterior mean.\n\nShow code\nbayes_gold_probs <- bayes_gold_probs %>% left_join(odds) %>%\n  mutate(annotate = str_extract(rider, \"^([^ ])+\"))\n\ntop_4 <- bayes_gold_probs %>% filter(rank <= 4)\n\nggplot(top_4) + \n  # decision boundary\n  stat_function(\n    data = tibble(p = c(0.05,1)), aes(p),\n    fun = function(p) 1/p -1, color = infreq_palette[\"darkblue\"], linetype = \"dashed\"\n  ) +\n  # posterior draws\n  geom_point(aes(x=gold_prob, y= fractional_odds), alpha = 0.2) +\n  # posterior mean\n  geom_point(\n    data = distinct(top_4, rider, fractional_odds, mean_gold_prob),\n    aes(x = mean_gold_prob, y= fractional_odds), size = 5,\n    color = infreq_palette[\"darkblue\"],\n    fill=infreq_palette[\"orange\"],pch=21\n  ) +\n  # rider annotation\n  geom_text(\n    data = distinct(top_4, rider, fractional_odds, annotate),\n    aes(x = 1, y= fractional_odds, label = annotate),\n    vjust=-0.5,hjust=1,color = infreq_palette[\"darkblue\"]\n  ) +\n  scale_x_continuous(breaks = seq(0,1,by = 0.2)) +\n  coord_cartesian(xlim = c(0,1), ylim = c(0,25)) +\n  xlab(\"Posterior Gold Probability\") +\n  ylab(\"Odds Offered\")\n\n\n\n\n\nThe data displayed is the same as in the posterior histograms shown previously.\nAll four riders have a non-negligible proportion of their density above the break-even boundary, suggesting that we should be open to considering bets on any of them.\nWe can calculate the expected log wealth taking into account our Bayesian uncertainty by integrating the previous expression for the expected log wealth (for a fixed set of probabilities) over the posterior distribution. In computational terms this is none other than averaging the expected log wealth of a particular strategy over all the draws from the posterior.\n\nDenoting \\(\\pi(\\underline p)\\) for the posterior distribution, the expected log wealth is \\[ \\mathbf E[\\log(W)] = \\qquad \\qquad \\\\ \\int \\mathbf E_{\\underline p}[\\log(W) ]  \\pi(d \\underline p).\\]\nLetting \\(p_r^{(d)}\\) be the gold medal probability for rider \\(r\\) in draw \\(d\\), \\(d=1,\\ldots, D\\), then we are looking to solve the optimisation problem:\n\\[\n\\begin{aligned}\n& \\text{maximize}\n& &  D^{-1} \\textstyle{\\sum_{d=1}^D \\, \\, \\sum_{r=1}^R} \\, \\, \\, p_r^{(d)} \\, \\,\\log (1 - F + f_rO_r) \\\\\n& \\text{subject to}\n& &   f_r \\geq 0,  \\,\\qquad 1 \\leq r \\leq R \\\\\n& & & \\textstyle{\\sum_{r=1}^R} f_r = F \\leq 1 \n\\end{aligned}\n\\]\n\nShow code\nbayesian_strategy <- optimise_kelly_bayes(odds, bayes_gold_probs)\n\nbayesian_strategy <- bayesian_strategy %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2)) %>%\n  select(rider, fractional_odds, mean_gold_prob, kelly_stakes)\n\n\n\n\nAthlete\nOdds\nModelled Gold Prob.\nStake\nGLAETZER MATTHEW\n14.0\n0.22\n0.17\nLAVREYSEN HARRIE\n0.7\n0.52\n0.07\n\nTaking into account our uncertainty in the probabilities has resulted in exactly the same proposed betting strategy as the simpler optimisation using only the posterior means!\nThere are of course three potential reasons we’ve arrived at this result: by chance, because of a bug in the code, or because of maths!\nI’ll admit when I first saw this, I instantly assumed I had a bug - but in fact its easy to see that this is purely down to the maths; rearranging the objective function above:\n\\[\n\\begin{align}\nD^{-1} {\\sum_{d=1}^D \\, \\, \\sum_{r=1}^R} \\, \\, \\, p_r^{(d)} \\, \\,\\log (1 - F + f_rO_r)  & = \\sum_{r=1}^r \\log(1 - F + f_rO_r)  \\left\\{ \\frac1D \\sum_{d} p_r^{(d)} \\right\\}\n\\\\ & =  \\sum_{r=1}^r \\log(1 - F + f_rO_r) \\overline p_r\n\\end{align}\n\\]\n\nThe same holds in the analytical setting on replacing sums over draws with integrating over the posterior distribution.\nwhere \\(\\overline p_r\\) is the posterior mean that we used in the point optimisation above, and so the two objective functions are equivalent.\nFor full disclosure - I only noticed this equivalence when writing up this detailed post, which was after placing my bets for Tokyo 2020. Of course had I realised before, I could have implemented the optimisation function in a simpler way.\nThis cancelation is possible because the objective function is linear in the probabilities \\(p_r^{(d)}\\); more generally whenever we want to maximise a linear objective over Bayesian uncertainty we can rely on this trick of using the posterior means rather than optimising over the posterior distributions.\nBut objectives need not be linear, and as a nice example I’ll consider a Bayesian way of implementing a more cautious betting strategy.\nA Bayesian Approach to Caution\nI previously mentioned that a common way to introduce caution to the Kelly strategy is to bet only a fraction of the proposed wager. In this section I’ll look at caution from a Bayesian perspective, and propose a revised Kelly strategy to take this into account.\n\nAs above - for full disclosure, my thinking behind this came about after I’d placed my bets for Tokyo 2020.\nThe histogram below depicts my expected wealth from betting using the stakes proposed above, over the different draws from my posterior distribution.\n\nShow code\nget_expected_wealth_draws <- function(strategy, prob_draws){\n  strategy %>%\n  filter(kelly_stakes > 0) %>%\n  left_join(prob_draws) %>%\n  group_by(.draw) %>%\n  summarise(\n    investment =sum(kelly_stakes),\n    expected_wealth = (1-investment) + sum( gold_prob * kelly_stakes * (1+fractional_odds)),\n    expected_log_wealth = sum( gold_prob * log((1-investment) + kelly_stakes * (1+fractional_odds))) \n  )\n}\n\npoint_estimate_expected_wealth_draws <- get_expected_wealth_draws(point_estimate_strategy, bayes_gold_probs)\n\nggplot(point_estimate_expected_wealth_draws) +\n  geom_histogram(aes(expected_wealth), binwidth = 0.05, color = infreq_palette[\"beige\"]) +\n  geom_vline(xintercept = 1,  color = infreq_palette[\"darkblue\"], linetype = \"dashed\") +\n  xlab(\"Expected Wealth\") +\n  expand_limits(x=0) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n  )\n\n\n\n\nThe posterior median is 0.95, meaning that even though I’ve chosen the optimal strategy to maximise wealth in the long run, I should still be going into this bet expecting to lose! This feels a bit surprising, but on reflection it isn’t: the strategy is putting money on Matthew Glaetzer because it expects him to win (the point estimate puts him winning at 0.22), but because the odds are favourable.\nOverall the histogram shows that the strategy is being driven by the long tail - which indicates that its a high variance strategy. The variance can be thought of as describing the risk in the strategy. For instance if I decide not to bet at all, my expected wealth will be my initial budget, and since I’m not betting I know I’ll get this return - so the variance is 0.\nMy idea then is to add a term to my existing utility function that prefers strategies that are lower variance:\n\\[\\mathbf E \\bigg[\\log W^{(1)}\\bigg] - \\lambda \\text{Var}\\bigg(\\log W^{(1)}\\bigg)\\]\nThe parameter \\(\\lambda \\geq 0\\) is to be chosen by the individual bettor: if \\(\\lambda = 0\\) this is the usual Kelly strategy and there is no risk aversion, as \\(\\lambda \\rightarrow \\infty\\) the objective function is focused on minimising variance and so will settle on placing no bets at all.\nThe variance above is the variance under the Monte-Carlo uncertainty - i.e. for a fixed set of winning probabilities \\(p_r\\) - so I can still fit this model using the point estimate probabilities. There are two ways to lower this: to place bets on outcomes that are more likely to happen (but which will no doubt have less favourable odds) or to place fewer/lower value bets.\nOptimising the cautious strategy with \\(\\lambda = 1\\) and using the point estimates for the winning probabilities returns the following revised strategy\n\nShow code\npoint_estimate_strategy_var1 <-\n  optimise_kelly_bayes(\n    odds, point_estimate_probs,\n    function(stake,fractional_odds, prob_mat){kelly_var_evaluation(stake,fractional_odds, prob_mat, lambda = 1)}\n  ) %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2))\n\n\n\n\nAthlete\nOdds\nModelled Gold Prob.\nStake\nGLAETZER MATTHEW\n14.0\n0.22\n0.03\nLAVREYSEN HARRIE\n0.7\n0.52\n0.03\n\nThis revised strategy is much more cautious: investing only 0.06 of the budget as opposed to the original 0.24. Further - this is now rebalanced with proportionally more of the investment being placed on Harrie Lavreysen than before.\nThe histogram below compares the posterior distribution for expected wealth from the two strategies, confirming that the variance is now much tighter than the Kelly Strategy.\n\nShow code\npoint_estimate_var1_expected_wealth_draws <- get_expected_wealth_draws(point_estimate_strategy_var1, bayes_gold_probs)\n\nwealth_draws <- bind_rows(\n  point_estimate_expected_wealth_draws %>% add_column(strategy = 'Kelly'),\n  point_estimate_var1_expected_wealth_draws %>% add_column(strategy = 'Conservative, λ = 1')\n)\n\nggplot(wealth_draws) +\n  geom_histogram(aes(expected_wealth), binwidth = 0.05, color = infreq_palette[\"beige\"]) +\n  geom_vline(xintercept = 1,  color = infreq_palette[\"darkblue\"], linetype = \"dashed\") +\n  geom_text(data = distinct(wealth_draws, strategy),aes(x = 0, y = 20, hjust=0, label = strategy)) +\n  facet_grid(rows = vars(strategy)) +\n  xlab(\"Expected Wealth\") +\n  expand_limits(x=0) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.y = element_blank(),\n    strip.text = element_blank(),\n\n    strip.background = element_rect(color=infreq_palette[\"beige\"], fill=infreq_palette[\"beige\"]),\n    strip.placement = \"outside\"\n  )\n\n\n\n\nThe optimisation above was conducted using the posterior mean winning probabilities - but to optimise under uncertainty we should perform the optimisation as described in the previous section: averaging over all draws from the posterior distribution. Whereas before the linearity of the utility function meant that the two problems were equivalent, the addition of the variance breaks this linearity.\nOptimising the cautious strategy with \\(\\lambda = 1\\) over all posterior draws proposes the following strategy:\n\nShow code\nbayesian_strategy_var1 <-\n  optimise_kelly_bayes(\n    odds, bayes_gold_probs,\n    function(stake,fractional_odds, prob_mat){kelly_var_evaluation(stake,fractional_odds, prob_mat, lambda =0.1)}\n  ) %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2))\n\n\n\n\nAthlete\nOdds\nModelled Gold Prob.\nStake\nGLAETZER MATTHEW\n14.0\n0.22\n0.15\nLAVREYSEN HARRIE\n0.7\n0.52\n0.06\n\nAlthough this used the same level of caution, \\(\\lambda = 1\\), as for the point estimate model, the proposed strategy is much more willing to bet - and barely deviates from the original Kelly strategy.\nI’ve not fully grasped this behaviour, but my intuition is that the point estimate under values the fact that Matthew Glaetzer actually has a lot of probability mass on scenarios where he is likely to win.\nThe plot below shows how the choice of \\(\\lambda\\) affects the expected returns: as anticipated, as \\(\\lambda\\) increases we increase our certainty in the return, at the cost of lowering our opportunity. Models fit to the point estimate are more senstive to \\(\\lambda\\) than those fit to the full posterior distribution.\n\nShow code\nlambda <- seq(0,5,by=0.5)\n\nlambda_point_estimate <- map(lambda, .f = function(lambda){\n  \n  stakes <- \n    optimise_kelly_bayes(\n    odds, point_estimate_probs,\n    function(stake,fractional_odds, prob_mat){kelly_var_evaluation(stake,fractional_odds, prob_mat, lambda = lambda)}\n  ) %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2)) %>%\n  select(rider, kelly_stakes)\n  \n  wealth_draws <- get_expected_wealth_draws(stakes, bayes_gold_probs)\n  \n  wealth_draws_summ <- wealth_draws %>%\n    summarise(\n      method = 'Posterior Mean',\n      lambda = lambda + 0.05,\n      q05 = quantile(expected_wealth, 0.05),\n      mean = mean(expected_wealth),\n      q95 = quantile(expected_wealth, 0.95)\n    )\n}) %>% bind_rows()\n\nlambda_bayes_estimate <- map(lambda, .f = function(lambda){\n  \n  stakes <- \n    optimise_kelly_bayes(\n    odds, bayes_gold_probs,\n    function(stake,fractional_odds, prob_mat){kelly_var_evaluation(stake,fractional_odds, prob_mat, lambda = lambda)}\n  ) %>%\n  mutate(kelly_stakes = round(kelly_stakes, 2)) %>%\n  select(rider, kelly_stakes)\n  \n  wealth_draws <- get_expected_wealth_draws(stakes, bayes_gold_probs)\n  \n  wealth_draws_summ <- wealth_draws %>%\n    summarise(\n      method = 'Full Posterior',\n      lambda = lambda - 0.05,\n      q05 = quantile(expected_wealth, 0.05),\n      mean = mean(expected_wealth),\n      q95 = quantile(expected_wealth, 0.95)\n    )\n}) %>% bind_rows()\n\nlambda_summary <- bind_rows(lambda_point_estimate, lambda_bayes_estimate) %>%\n  mutate(method = factor(method, levels= c ('Full Posterior','Posterior Mean')))\n\nggplot(lambda_summary) +\n  geom_point(aes(x=lambda, y = mean,color = method), size = 2) +\n  geom_errorbar(aes(x=lambda , ymin = q05, ymax = q95, color=method),width = 0.1) +\n  geom_hline(yintercept = 1, color = infreq_palette[\"darkblue\"], linetype = \"dashed\", alpha = 0.5) +\n  ylab(\"Expected Wealth\") +\n  xlab(\"λ\") +\n  theme(legend.title = element_blank())\n\n\n\n\nBetting Log\nThe log below reproduces the content from a holding post that I was updating whilst the events in the velodrome were running (I hadn’t had time to draft the full post before events started).\nTo avoid any spoilers for anyone who feels like re-watching the track events at Tokyo I’ll refrain from sharing how the model faired for now, and include that in a final wrap-up post.\nOne point of note was my realisation late in the day (see my entry against the Women’s Post Qualifying bets below) that I hadn’t really thought through the fact that within each gender my bets were going to be dependent. I’ll touch on this in more detail in the wrap-up.\nBudget and Strategy Summary\nIn total I allowed myself a budget of £20 - not exactly betting big, but also a non-negligible amount which will give me the flexibility to make a number of bets.\nFor each of the men’s and women’s events I bet three times:\nPrior to qualifying - using historic data only.\nImmediately after qualifying - including the qualifying times in the model.\nPrior to the semi-finals - re-training the model to include the now observed preliminary rounds.\nAt the point of the competition I had not thought about the conservative strategy above, so all bets were made using the spread-bet Kelly strategy. To account for caution, and so as to avoid blowing all my budget on the very first bet, I initially capped my bet at 1/6 of the total (£3.40).\n\n\nMen\nPre-Qualifying (4th August 2021)\n\n\n\nThe idea of betting at this point is that hopefully the bookies odds are naive, as they won’t have adjusted for qualifying times. We saw in the previous post qualifying data is highly predictive of match results.\nSince qualifying times are not available I’ve fitted the model assuming all riders have the same qualifying time, and seeded them for matches in order of strength.\n\nAthlete\nOdds\nStake\nModelled Gold Prob.\nGLAETZER MATTHEW\n14.0\n2.4\n0.22\nLAVREYSEN HARRIE\n0.7\n1.0\n0.52\n\nAll Other Odds\nAthlete\nOdds\nStake\nModelled Gold Prob.\nExpected Return\nHOOGLAND JEFFREY\n3.0\n0\n0.15\n0\nCARLIN JACK\n20.0\n0\n0.03\n0\nRUDYK MATEUSZ\n14.0\n0\n0.02\n0\nHART NATHAN\n50.0\n0\n0.01\n0\nVIGIER SEBASTIEN\n40.0\n0\n0.01\n0\nLEVY MAXIMILIAN\n33.0\n0\n0.01\n0\nDMITRIEV DENIS\n12.0\n0\n0.01\n0\nNITTA YUDAI\n40.0\n0\n0.01\n0\nHELAL RAYAN\n66.0\n0\n0.01\n0\nBROWNE KWESI\n150.0\n0\n0.00\n0\nKENNY JASON\n6.5\n0\n0.00\n0\nPAUL NICHOLAS\n100.0\n0\n0.00\n0\nWEBSTER SAM\n25.0\n0\n0.00\n0\nTJON EN FA JAIR\n150.0\n0\n0.00\n0\nWAKIMOTO YUTA\n150.0\n0\n0.00\n0\nXU CHAO\n100.0\n0\n0.00\n0\nMITCHELL ETHAN\n33.0\n0\n0.00\n0\nAWANG MOHD AZIZULHASNI\n18.0\n0\n0.00\n0\nYAKUSHEVSKIY PAVEL\n150.0\n0\n0.00\n0\nBOTTICHER STEFAN\n25.0\n0\n0.00\n0\nWAMMES NICK\n200.0\n0\n0.00\n0\nBABEK TOMAS\n80.0\n0\n0.00\n0\nQUINTERO CHAVARRO KEVIN SANTIAGO\n80.0\n0\n0.00\n0\nBARRETTE HUGO\n100.0\n0\n0.00\n0\nPONOMARYOV SERGEY\n200.0\n0\n0.00\n0\nRAJKOWSKI PATRYK\n100.0\n0\n0.00\n0\nSAHROM MUHAMMAD SHAH FIRDAUS\n200.0\n0\n0.00\n0\nSPIES JEAN\n150.0\n0\n0.00\n0\n\nPost Qualifying (4th August 2021)\nUnfortunately, once the start list was published this morning it transpired that Matthew Glaetzer pulled out of competition meaning that I’m already £2.40 down, and have some catching-up to do.\n\nAt no fault of the model - rather my not checking for revised start lists!\nThese bets were placed ahead of the 1/32 finals. I maintained the cap of £3.40 used in the first round.\n\n\n\n\nAthlete\nOdds\nStake\nModelled Gold Prob.\nLAVREYSEN HARRIE\n0.75\n2.0\n0.60\nHOOGLAND JEFFREY\n2.00\n1.3\n0.38\n\nAll Other Odds\nAthlete\nOdds\nStake\nModelled Gold Prob.\nExpected Return\nCARLIN JACK\n10.0\n0\n0.01\n0\nDMITRIEV DENIS\n12.0\n0\n0.00\n0\nPAUL NICHOLAS\n100.0\n0\n0.00\n0\nRUDYK MATEUSZ\n100.0\n0\n0.00\n0\nKENNY JASON\n6.5\n0\n0.00\n0\nHELAL RAYAN\n66.0\n0\n0.00\n0\nVIGIER SEBASTIEN\n40.0\n0\n0.00\n0\nRICHARDSON MATTHEW\n0.0\n0\n0.00\n0\nTJON EN FA JAIR\n150.0\n0\n0.00\n0\nXU CHAO\n100.0\n0\n0.00\n0\nHART NATHAN\n50.0\n0\n0.00\n0\nLEVY MAXIMILIAN\n33.0\n0\n0.00\n0\nWAKIMOTO YUTA\n150.0\n0\n0.00\n0\nWEBSTER SAM\n25.0\n0\n0.00\n0\nAWANG MOHD AZIZULHASNI\n20.0\n0\n0.00\n0\nBABEK TOMAS\n0.0\n0\n0.00\n0\nBARRETTE HUGO\n100.0\n0\n0.00\n0\nBOTTICHER STEFAN\n25.0\n0\n0.00\n0\nBROWNE KWESI\n0.0\n0\n0.00\n0\nMITCHELL ETHAN\n50.0\n0\n0.00\n0\nNITTA YUDAI\n0.0\n0\n0.00\n0\nPONOMARYOV SERGEY\n0.0\n0\n0.00\n0\nQUINTERO CHAVARRO KEVIN SANTIAGO\n80.0\n0\n0.00\n0\nRAJKOWSKI PATRYK\n100.0\n0\n0.00\n0\nSAHROM MUHAMMAD SHAH FIRDAUS\n200.0\n0\n0.00\n0\nSPIES JEAN\n0.0\n0\n0.00\n0\nWAMMES NICK\n200.0\n0\n0.00\n0\nYAKUSHEVSKIY PAVEL\n0.0\n0\n0.00\n0\n\nPost Quarterfinals (5th August 2021)\n\n\n\nThese bets were placed following the quarterfinals, with only four riders left competing. At this point the model is very stable and is predicting almost identical win probabilities for the leading athletes as it did in my previous round of bets.\nIn an effort to catch-up on the early loss on Glaetzer I’m going to lift my cap to £6, putting my total investment in the Men’s event at £12.70.\n\nAthlete\nOdds\nStake\nModelled Gold Prob.\nLAVREYSEN HARRIE\n0.75\n3.6\n0.60\nHOOGLAND JEFFREY\n1.60\n2.4\n0.39\n\nAll Other Odds\nAthlete\nOdds\nStake\nModelled Gold Prob.\nExpected Return\nCARLIN JACK\n5.5\n0\n0.01\n0\nDMITRIEV DENIS\n6.0\n0\n0.00\n0\n\n\n\nWomen\nPre-Qualifying (5th August 2021)\n\n\n\nAs with the Men’s event for this run I have fitted the model assuming all riders have the same qualifying time, and seeded them for matches in order of strength.\n\nAthlete\nOdds\nStake\nModelled Gold Prob.\nLEE WAI SZE\n4\n3.2\n0.75\nSHMELEVA DARIA\n40\n0.1\n0.02\n\nAll Other Odds\nAthlete\nOdds\nStake\nModelled Gold Prob.\nExpected Return\nHINZE EMMA\n2\n0\n0.07\n0\nVOINOVA ANASTASIIA\n5\n0\n0.05\n0\nMITCHELL KELSEY\n16\n0\n0.02\n0\nVAN RIESSEN LAURINE\n11\n0\n0.01\n0\nBRASPENNINCX SHANNE\n22\n0\n0.01\n0\nGROS MATHILDE\n20\n0\n0.01\n0\nFRIEDRICH LEA SOPHIE\n7\n0\n0.01\n0\nMCCULLOCH KAARLE\n28\n0\n0.01\n0\nKRUPECKAITE SIMONA\n40\n0\n0.01\n0\nLEE HYEJIN\n22\n0\n0.01\n0\nJAMES KIRSTIE\n100\n0\n0.01\n0\nSTARIKOVA OLENA\n50\n0\n0.00\n0\nDEMAY CORALIE\n80\n0\n0.00\n0\nZHONG TIANSHI\n8\n0\n0.00\n0\nGAXIOLA GONZALEZ LUZ DANIELA\n100\n0\n0.00\n0\nGENEST LAURIANE\n66\n0\n0.00\n0\nVERDUGO OSUNA YULI\n200\n0\n0.00\n0\nBASOVA LIUBOV\n28\n0\n0.00\n0\nMARCHANT KATY\n33\n0\n0.00\n0\nBAO SHANJU\n20\n0\n0.00\n0\nKOBAYASHI YUKA\n80\n0\n0.00\n0\nANDREWS ELLESSE\n14\n0\n0.00\n0\nGODBY MADALYN\n66\n0\n0.00\n0\nLOS URSZULA\n150\n0\n0.00\n0\nDU PREEZ CHARLENE\n150\n0\n0.00\n0\nMAROZAITE MIGLE\n150\n0\n0.00\n0\nLEE HOI YAN JESSICA\n150\n0\n0.00\n0\nKARWACKA MARLENA\n150\n0\n0.00\n0\n\nPost Qualifying (6th August 2021)\nNow that the Men’s event is finished (no spoilers here - checkout wikipedia, or wait until the last post in the series where I’ll summarise my performance) I’m going to lift my cap to £6.70.\nLee Wai Sze doesn’t appear to be on top form based on her results in the Keirin, and the qualifying round for the sprint. As a result I’ve decided to manually drop her strength by 1 across all posterior samples as I believe the model is over estimating on historic performance.\n\n\n\n\n\nAthlete\nOdds\nStake\nModelled Gold Prob.\nHINZE EMMA\n2.5\n1.3\n0.28\nLEE WAI SZE\n4.0\n1.0\n0.22\nFRIEDRICH LEA SOPHIE\n5.0\n0.9\n0.19\nMITCHELL KELSEY\n11.0\n0.8\n0.15\nGROS MATHILDE\n18.0\n0.4\n0.08\nSTARIKOVA OLENA\n50.0\n0.1\n0.02\n\nAll Other Odds\nAthlete\nOdds\nStake\nModelled Gold Prob.\nExpected Return\nBRASPENNINCX SHANNE\n7.5\n0\n0.02\n0\nVOINOVA ANASTASIIA\n6.0\n0\n0.01\n0\nSHMELEVA DARIA\n40.0\n0\n0.01\n0\nMARCHANT KATY\n28.0\n0\n0.00\n0\nZHONG TIANSHI\n9.0\n0\n0.00\n0\nGENEST LAURIANE\n40.0\n0\n0.00\n0\nANDREWS ELLESSE\n14.0\n0\n0.00\n0\nMCCULLOCH KAARLE\n25.0\n0\n0.00\n0\nKRUPECKAITE SIMONA\n40.0\n0\n0.00\n0\nKOBAYASHI YUKA\n80.0\n0\n0.00\n0\nLEE HYEJIN\n25.0\n0\n0.00\n0\nGAXIOLA GONZALEZ LUZ DANIELA\n100.0\n0\n0.00\n0\nBAO SHANJU\n22.0\n0\n0.00\n0\nGODBY MADALYN\n66.0\n0\n0.00\n0\nBASOVA LIUBOV\n33.0\n0\n0.00\n0\nDU PREEZ CHARLENE\n150.0\n0\n0.00\n0\nMAROZAITE MIGLE\n150.0\n0\n0.00\n0\nVERDUGO OSUNA YULI\n200.0\n0\n0.00\n0\n\nPost Quarterfinals (7th August 2021)\n\n\n\nHaving run the model with the latest data, I’ve decided not to add to my bets for the womens event - of the six athletes I bet on in the previous round, four of them have gone through to the semi-finals: but the odds have now shortened.\nThe model wanted me to back Lee Wai Sze again - however an oversight in my approach is that at the moment all of the bets are being placed independently. I’m fairly confident that a model that properly accounts for the dependence wouldn’t recommend I bet again, so I’ll over ride the model’s decision.\nOther than Lee Wai Sze the model only wanted to put a few pennies on two other athletes - so it didn’t seem worth it (and again this is in absence of the dependence point above).\nThe details below have the odds offered, and the stakes that the model wanted me to make.\nAll Other Odds\nAthlete\nOdds\nProposed Stake\nModelled Gold Prob.\nExpected Return\nLEE WAI SZE\n5.5\n3.4\n0.41\n9.04\nHINZE EMMA\n1.0\n0.0\n0.24\n0.00\nSTARIKOVA OLENA\n2.5\n0.2\n0.21\n0.15\nMITCHELL KELSEY\n4.0\n0.1\n0.14\n0.07\n\n\n\nComments\nAcknowledgments\nThis post makes heavy use of R and NLOpt, and so benefits from the work of all those who contribute to their development.\n\n\n\n",
    "preview": "posts/2021-08-03-tokyo-2020-iii/img/tournament_prev.gif",
    "last_modified": "2021-09-05T18:15:32+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-28-tokyo-2020-ii/",
    "title": "Tokyo 2020 Betting II: Model Refinement and Feature Engineering",
    "description": "The second post in a series building towards betting on the Olympic track cycling. This time I'll adapt the Bradley-Terry model to use a custom likelihood to better capture the competition format, and integrate new data fields to boost model performance.",
    "author": [],
    "date": "2021-07-29",
    "categories": [
      "Bayesian",
      "Cycling",
      "Gaussian Processes"
    ],
    "contents": "\n\nContents\nThe Match Likelihood\nHome Advantage\nTime Varying Strengths\nQualifying Times\nThe Final Fit and Reflections\nComments\nAcknowledgments\n\nLast time I introduced the basic model I’ll be using to predict the results of the track cycling Individual Sprint at Tokyo 2020.\nThis post will pick up where that ended, with the focus turning to building on the basic model: adapting it to handle more of the specifics of the sport.\nThe table below summarises the changes I’ll explore:\n\n\nPost\n\n\nModel\n\n\nDescription\n\n\nTokyo 2020 Betting I\n\n\nbt1\n\n\nBasic Bradley-Terry\n\n\nbt2\n\n\nInformative (Gamma) Prior\n\n\nTokyo 2020 Betting II\n\n\nbt3\n\n\nMatch Likelihood\n\n\nbt4\n\n\nHome advantage\n\n\nbt5\n\n\nTime Dependent Strengths\n\n\nbt6\n\n\nQualifying Times\n\n\n\nThe first two were covered in the previous post.\nI’ll present the model developments in the order I made them - which matches the above - rather than the level of impact they have on evaluation metrics. In the conclusion I’ll reflect on how I could have improved my approach to prioritising the developments.\nAssumptions\nThis post makes some assumptions about you!\nI’ll assume you’ve read the previous post in the series (though this might hold-up as a stand-alone): I won’t recap the data or basic model.\nThe model variants will rely on a few technical tools that I’ll recap, but not fully introduce:\nthe role of the likelihood in regression models,\nhierarchical (random) effects, and\nGaussian processes.\nIf you’re interested in reading the underlying code, this is in R and Stan.\n\nThe full model code, including the R pipeline to handle multiple models, is here.\nThe Match Likelihood\nThe Bradley-Terry model assumes matches are won or lost, with no delineation of the scale of a win.\nIn the early heats of the individual sprint this is an accurate assumption: the format is a knockout where a single sprint is contested.\nFrom the quarterfinals, matches are contested in a best-of-three format which introduces a scale to winning: if \\(p\\) is the probability that a given athlete wins a single sprint, the outcomes that they win a best-of-three match are:\nScenario\nSprints Contested\nProb. Occurrence\nWins first two sprints outright\n2\n\\(p^2\\)\nWins first and third, loses second\n3\n\\(p(1-p)p\\)\nLoses first, wins second and third\n3\n\\((1-p)p^2\\)\nTotal\n\n\\(p^2 + 2p^2(1-p)\\)\n\nShow code\np_best_of_three <- ggplot(tibble(p=c(0,1)), aes(p)) +\n  \n  stat_function(\n    aes(linetype = '2', color = '2'),\n    fun = function(p){ p^2  },\n    size=1, color = infreq_palette[\"darkblue\"]\n  ) +\n  \n  stat_function(\n    aes(linetype = '3', color = '3'),\n    fun = function(p){(2*p^2*(1-p))},\n    size=1, color = infreq_palette[\"darkblue\"]\n  ) + \n  \n  stat_function(\n    aes(linetype = 'Total', color = 'Total'),\n    fun = function(p) p^2 + 2*p^2*(1-p) ,\n    size = 2, color = infreq_palette[\"orange\"]\n  )  +\n  \n  geom_text(\n    data = tribble(~p, ~y, ~text, ~color,\n            0.9, 0.60,'2 sprints', 'Total',\n            0.9, 0.31, '3 sprints','3',\n            0.92, 1.07, 'Total', '2'),\n    aes(p,y,label=text),\n    color = c(infreq_palette[\"darkblue\"], infreq_palette[\"darkblue\"], infreq_palette[\"orange\"]),\n    size = c(5,5,6)\n  ) +\n  \n  scale_y_continuous(limits=c(0,1.2), breaks = seq(0,1.2,by=0.2)) +\n  coord_cartesian(ylim = c(0,1.1)) +\n  labs(x=\"Prob. Wins Sprint\", y=\"Prob. Wins Match\") +\n  scale_linetype_manual(values = c(\"solid\", \"dotted\", \"solid\")) +\n  scale_color_manual(values = c(infreq_palette[\"darkblue\"],infreq_palette[\"darkblue\"],infreq_palette[\"orange\"])) +\n  theme(legend.position = \"none\")\n\np_best_of_three\n\n\n\n\n\nProbability of winning a best-of-three match.\n\n\n\nThe best-of-three format can be considered fairer than a single sprint, as it increases the probability that the stronger athlete wins.\nI’ll adapt the Bradley-Terry model to use a custom likelihood: in early rounds this will be as before, but for later rounds it will take into account the best-of-three format.\nOne convenience I’ll introduce is to formulate the model from the perspective of the winning rider. In words the likelihood I’ll consider is:\nGiven that the winning athlete, \\(w\\), beat the loser, \\(l\\), in \\(S\\) sprints: how likely is it that the strength difference between them is \\(\\delta = \\alpha_w - \\alpha_l\\)?\nTo simplify the formula for the likelihood I’ll let \\(p_\\delta = \\text{logit}^{-1}(\\delta)\\) which, as covered in the last post, is the probability of winning a single sprint given the strength difference \\(\\delta\\).\nThe match likelihood is then defined as:\n\\[\nL_{\\text{match}}\\big(\\alpha_w - \\alpha_l = \\delta \\,  |\\,  S = s\\big) =\n\\begin{cases}\np_\\delta,  & s = 1 \\\\ \\\\\np_\\delta^2, & s = 2\\\\ \\\\\n2p_\\delta^2 (1-p), & s = 3 \n\\end{cases}\n\\]\nThe formula above is not a true likelihood per se, I’m using two convenient properties of the race format and the data, that allow this reduced definition:\nFirstly, the case \\(s=1\\) gets invoked on a separate subset of the data to the cases \\(s=2,\\,3\\). So really this is a separate likelihood (equivalent to the Bernoulli, logistic regression, model). Combining the two scenarios in one formula is a convenience as I can fit all the data to one model, rather than splitting out the cases.\nSecondly, both likelihoods (\\(s=1\\) case, and \\(s=2,3\\)) are presented from the perspective of the winning rider: i.e. we don’t have the cases \\(s=-1,-2,-3\\). In reality these cases do exist - but by feeding the model data from the winning perspective the other scenarios won’t be induced.\n\n\nBT3 \\[\n\\begin{align*}\n\\bf{\\text{Priors}}\\\\\n\\sigma & \\sim \\text{Gamma}(80,60) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_w - \\alpha_l | S &\\sim L_{\\text{match}}(\\alpha_r - \\alpha_s | S)\n\\end{align*}\n\\]\n\n\nExample Data\n\nShow code\n# read match data from remote _target store\nmatches <- read_remote_target(\"matches\")\n\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id, sprints) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\nsprints\n2\n\n\n\nStan code\n\n\nfunctions {\n  real match_logit_lpmf(int[] s, vector delta){\n    real lpmf = 0;\n    vector[num_elements(delta)] p = inv_logit(delta);\n    \n    for(n in 1:num_elements(s)){\n      if (s[n] == 1) lpmf += log(p[n]);\n      else if (s[n] == 2) lpmf += 2*log(p[n]);\n      else lpmf += log(2) + 2*log(p[n]) + log(1-p[n]);\n    }\n    return lpmf;\n  }\n  \n  real accuracy(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n    real acc = 0;\n    \n    for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n    \n    return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real ll = 0;\n    \n    return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n  \n  real match_log_loss(int[] s, vector delta, int start, int end){\n    int s_seg[end - start + 1]  = segment(s, start, end - start + 1);\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real mll = 0;\n    \n    return inv(num_elements(s_seg)) * match_logit_lpmf(s_seg | delta_seg);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n  \n  // Match results\n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n  int<lower=1,upper=3> sprints[M];\n}\n\nparameters {\n  // rider ratings\n  real<lower=0> sigma;\n  vector[R] alpha0;\n}\n\ntransformed parameters {\n  // difference of winner and loser rating\n  vector[M] delta =alpha0[winner_id] - alpha0[loser_id];\n}\n\nmodel {\n  // (hierarchical) priors - strength\n  sigma ~ gamma(80,60);\n  alpha0 ~ normal(0,sigma);\n  \n  // likelihood\n  head(sprints, T) ~ match_logit(head(delta, T));\n}\n\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_match_log_loss;\n  real evaluation_match_log_loss;\n\n  training_accuracy = accuracy(delta,1, T);\n  training_match_log_loss = match_log_loss(sprints, delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_match_log_loss = match_log_loss(sprints, delta, T+1, M);\n}\n\n\n\nBT3. Bradley-Terry with Match Likelihood.\nThe official list of track cyclists competing at Tokyo has now been published so we can create the strengths plot for the new model using the riders who are expected to take part.\n\nThis may include athletes not competing the individual sprint, as the start list has not yet been published\n\nShow code\nriders <- read_remote_target(\"riders\")\n\nolympic_riders <- read_csv(\"olympic-athletes.csv\") %>%\n  mutate(Athlete = if_else(Athlete == 'ANTONOVA NATALIIA', 'ANTONOVA NATALIA', Athlete)) %>%\n  inner_join(riders,.,  by = c(\"rider_name\" = \"Athlete\"), keep = TRUE)\n\nbt3_summ <- read_remote_target(\"bt_summary_bt3\")\n\nbt3_strength_summ <- bt3_summ %>%\n  filter(str_detect(variable, 'alpha')) %>%\n  mutate(rider_id = str_match(variable, '\\\\[(.*)\\\\]')[,2] %>% as.numeric()) %>%\n  left_join(olympic_riders, .) %>%\n  mutate(rider_name = fct_reorder(rider_name,median))\n\np_strengths_bt3 <- ggplot(bt3_strength_summ) +\n  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name), color = infreq_palette[\"darkblue\"]) +\n  labs(y=\"\",x=\"Strength, α\")\n\np_strengths_bt3\n\n\n\n\nThere’s a compelling case for Lee Wai Sze to win gold - though this is based on the training data, so doesn’t include the latest results.\n\nOnce I’ve picked my final model, I’ll fit to the full data range and present this plot again\nMatch Performance Metrics\nIn light of changing the likelihood, its worth reviewing the evaluation metrics I’m using.\nI can either measure accuracy by the proportion of separate sprints that are correctly predicted, or the proportion of matches. I’ll stick with match outcomes, since when applying the model this will be the measure of interest.\nThe log-loss metric is intrinsically linked to the likelihood of the model (its the average log likelihood of the data), so we should revise this to use the new likelihood. This will also need a revised benchmark.\nFor a best-of-three match there are four possible outcomes (either athlete winning in either 2 or 3 sprints), so a random guessing strategy has a log loss of \\(\\log(\\textstyle \\frac14) \\approx -1.39\\).\n\nThe single sprint benchmark is \\(\\log(\\frac12) \\approx -0.69\\) as before.\nThe benchmark for the full data is a weighted average of the single sprint benchmark and the best-of-three. Note that this means that the match log loss benchmark differs between the training and evaluation splits.\n\n\nAccuracy\n\n\n\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n0.648\n0.716\n0.773\nbt2\nevaluation\n0.648\n0.716\n0.773\nbt3\nevaluation\n0.648\n0.716\n0.773\nbt1\ntraining\n0.764\n0.787\n0.807\nbt2\ntraining\n0.770\n0.791\n0.810\nbt3\ntraining\n0.768\n0.789\n0.808\n\n\n\n\n\n\nMatch Log Loss\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n-1.087\n-0.919\n-0.800\nbt2\nevaluation\n-1.169\n-0.951\n-0.809\nbt3\nevaluation\n-1.096\n-0.928\n-0.809\nbt1\ntraining\n-0.642\n-0.612\n-0.586\nbt2\ntraining\n-0.635\n-0.603\n-0.575\nbt3\ntraining\n-0.612\n-0.588\n-0.567\n\n\n\n\n\n\nChanging the likelihood has had a minimal impact on the evaluation metrics - with all three models defined to date performing similarly.\n\n\n\nIn hindsight this is to be expected as the volume of data that is evaluated differently under the revised likelihood is small: 86% of the training data are single sprints, for which both likelihoods agree, and only 2% have three sprints which is the scenario where the likelihoods differ the most.\nI’ll carry this likelihood into the future models.\nHome Advantage\nA common adaptation of the Bradley-Terry model is to add a term to account for home advantage.\nI didn’t have ready access to a list of each rider nationalities, but as a proxy I will use the nationality of the team they are riding for. I’ll consider a rider as being at home if the event is held in the country of their team.\n\nTeam nationality data is reported by the UCI here\nThe standard implementation of home advantage replaces the fixed strength \\(\\alpha_r\\), with a strength that varies by match:\n\\[\n\\alpha_{r}^{(m)} =\n\\begin{cases}\n \\alpha_r + \\eta, &\\text{ if $r$ is at home in match $m$,} \\\\\n \\alpha_r , & \\text{otherwise.}\n \\end{cases}\n\\]\n\\(\\eta\\) was chosen as its the Greek alphabet equivalent of \\(h\\), for home.\nThis assumes that all athletes feel the same benefitwhen competing at home, which I think is an over simplification. I’ll improve that assumption by using hierarchical modelling.\n\n\n\nHierarchical modelling allows us to vary the home advantage between riders, whilst accounting for the fact that we don’t have enough data to estimate each athlete’s effect independently.\n\nOnly 9% of matches feature a home athlete, and 72% athletes have never contested a home match.\nThe idea is to assume that there is some average effect, \\(\\eta\\), and that each rider then deviates from this average by some amount, \\(\\theta_r\\)\n\\[\\eta_r = \\eta + \\theta_r.\\]\nTo help us define priors, I’ll consider the scenario where two riders with equal strength are competing, \\(\\alpha_w - \\alpha_l = 0\\), and suppose one is at home. How much impact do we expect home advantage will have on the probability of winning?\nNo advantage would mean a win probability of 0.5, and I’d expect being at home might increase this to somewhere between 0.55 and 0.75. I can now turn that into a prior on \\(\\eta\\).\nI also need a prior for the rider deviations \\(\\theta_r\\). Its reasonable to suppose this is normally distributed with mean 0, but we don’t know the appropriate standard deviation… and when in doubt, create a parameter!\nI’ll let \\(\\upsilon\\) be this unknown standard deviation, so that \\(\\theta_r \\sim \\text{Normal}(0,\\upsilon)\\), and put a prior on \\(\\upsilon\\).\nExperimentation suggests priors \\(\\eta \\sim \\Gamma(2,4)\\) and \\(\\upsilon \\sim \\text{HalfNormal}(0,0.2)\\) align well to my heuristic above.\n\n\nParameter Priors\n\nShow code\n  home_effect_samples <- tibble(\n    eta = rgamma(1e05, shape = 2, rate = 4),\n    upsilon = abs(rnorm(1e05, mean = 0, sd =0.2))\n  ) %>%\n  mutate(\n    p_win_avg = plogis(eta),\n    theta = rnorm(n(), mean = 0, sd = upsilon),\n    eta_theta = eta + theta,\n    p_win_rider = plogis(eta_theta)\n  )\n\nhome_param_prior <- home_effect_samples %>%\n  # convert to long format \n  select(eta, theta) %>%\n  gather(measure, sample, eta, theta) %>%\n  mutate(measure = if_else(measure == \"eta\", 'Average', 'Rider Deviation'))\n\nggplot(home_param_prior, aes(sample, y = ..density.., fill = measure)) +\n  geom_histogram(binwidth = 0.03, color = infreq_palette[\"beige\"]) +\n  # scale_x_continuous(breaks = seq(0, 1, by=0.1)) +\n  coord_cartesian(xlim = c(-1,2)) +\n  labs(x = \"Home Advantage\", y = \"\") +\n  facet_grid(rows = vars(measure)) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    strip.background = element_blank(),  strip.text.y = element_text(size = 14),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\nHome Win Prior\n\nShow code\nhome_wins_prior <- home_effect_samples %>%\n  # convert to long format \n  select(p_win_avg, p_win_rider) %>%\n  gather(measure, sample, p_win_avg, p_win_rider) %>%\n  mutate(measure = if_else(measure == \"p_win_avg\", 'Average', 'Rider'))\n  \nggplot(home_wins_prior, aes(sample, y = ..density.., fill = measure)) +\n  geom_histogram(binwidth = 0.01, color = infreq_palette[\"beige\"]) +\n  scale_x_continuous(breaks = seq(0, 1, by=0.1)) +\n  coord_cartesian(xlim = c(0,1)) +\n  labs(x = \"Probability Home Rider Wins\", y = \"\") +\n  facet_grid(rows = vars(measure)) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    strip.background = element_blank(),  strip.text.y = element_text(size = 14)\n  )\n\n\n\n\n\n\n\n\nBT4\nI’ll follow the convention I introduced above, denoting \\(\\alpha_r^{(m)}\\) for a strength that varies by match. Further I’ll let \\(\\mathbf{1}_r^{(m)}\\) denote the indicator that equals 1 if the rider, r, is at home in the match, m.\n\\[\n\\begin{align*}\n\\bf{\\text{Priors}} \\\\\n\\sigma & \\sim \\text{Gamma}(80,60) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\eta & \\sim \\text{Gamma}(2,4) \\\\\n\\upsilon & \\sim \\text{Normal}(0, 0.2) \\\\\n\\theta_r & \\sim \\text{Normal}(0,\\upsilon) \\\\\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_r^{(m)} & = \\alpha_r + (\\eta + \\theta_r)\\mathbf{\\large 1}_r^{(m)}\\\\\n\\alpha_w^{(m)} - \\alpha_l^{(m)} | S & \\sim L_{\\text{match}}(\\alpha_r^{(m)} - \\alpha_s^{(m)} | S)\n\\end{align*}\n\\]\n\n\nExample Data\n\nShow code\n# read match data from remote _target store\nmatches <- read_remote_target(\"matches\")\n\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id, sprints, winner_at_home, loser_at_home) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\nsprints\n2\nwinner_at_home\n1\nloser_at_home\n0\n\n\n\nStan code\n\n\nfunctions {\n  real match_logit_lpmf(int[] s, vector delta){\n    real lpmf = 0;\n    vector[num_elements(delta)] p = inv_logit(delta);\n    \n    for(n in 1:num_elements(s)){\n      if (s[n] == 1) lpmf += log(p[n]);\n      else if (s[n] == 2) lpmf += 2*log(p[n]);\n      else lpmf += log(2) + 2*log(p[n]) + log(1-p[n]);\n    }\n    return lpmf;\n  }\n  \n  real accuracy(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n    real acc = 0;\n    \n    for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n    \n    return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real ll = 0;\n    \n    return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n  \n  real match_log_loss(int[] s, vector delta, int start, int end){\n    int s_seg[end - start + 1]  = segment(s, start, end - start + 1);\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real mll = 0;\n    \n    return inv(num_elements(s_seg)) * match_logit_lpmf(s_seg | delta_seg);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n\n  // Match results\n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n  int<lower=1,upper=3> sprints[M];\n  \n  // Home advantage effects\n  real<lower=0,upper=1> winner_at_home[M];\n  real<lower=0,upper=1> loser_at_home[M];\n}\n\nparameters {\n  // rider ratings\n  real<lower=0> sigma;\n  vector[R] alpha0;\n  \n  // home advantage effect\n  real<lower=0>eta;\n  real<lower=0>upsilon;\n  real theta[R];\n}\n\ntransformed parameters {\n  // difference of winner and loser rating\n  vector[M] delta;\n  real eta_theta[R];\n  \n  for(r in 1:R) eta_theta[r] = eta + theta[r];\n  \n  for(m in 1:M){\n    \n    delta[m] = \n      alpha0[winner_id[m]] - alpha0[loser_id[m]] +\n      (winner_at_home[m] * eta_theta[winner_id[m]]) - (loser_at_home[m] * eta_theta[loser_id[m]]);\n  }\n}\n\nmodel {\n  // (hierarchical) priors - strength\n  sigma ~ gamma(80,60);\n  alpha0 ~ normal(0,sigma); \n  \n  // (hierarchical) priors - home effect\n  eta ~ gamma(2,4);\n  upsilon ~ normal(0,0.2);\n  theta ~ normal(0, upsilon);\n  \n  // likelihood\n  head(sprints, T) ~ match_logit(head(delta, T));\n}\n\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_match_log_loss;\n  real evaluation_match_log_loss;\n\n  training_accuracy = accuracy(delta,1, T);\n  training_match_log_loss = match_log_loss(sprints, delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_match_log_loss = match_log_loss(sprints, delta, T+1, M);\n}\n\n\n\nBT4. Home advantage.\nIn Tokyo 2020 there’s a single female Japanese athlete, Yuka Kobayashi. The plot below shows the posterior ranges for the rider strengths, and shows the impact that the home advantage is expected to have (in blue).\n\nThe men’s field has a larger cohort of Japanese athletes\n\nShow code\nbt4_draws <- read_remote_target(\"bt_draws_bt4\")\n\njapanese_riders <- olympic_riders %>% filter(Nationality == 'Japan') %>%\n  mutate(variable = paste0('theta[', rider_id,']'))\n\nbt4_draws <- bt4_draws %>%\n  gather(variable, value, -starts_with(\".\"))\n\nbt4_rider_home_effect <- left_join(japanese_riders, bt4_draws) %>%\n  select(-variable) %>%\n  rename(rider_home_effect = value)\n\nbt4_home_effect <- bt4_draws %>%\n  filter(str_detect(variable, '^eta')) %>%\n  rename(home_effect = value) %>%\n  select(-variable) %>%\n  left_join(bt4_rider_home_effect) %>%\n  mutate(rider_home_effect = rider_home_effect + home_effect)\n\nbt4_strengths <- bt4_draws %>%\n  filter(str_detect(variable, 'alpha')) %>%\n  mutate(rider_id = str_match(variable, '\\\\[(.*)\\\\]')[,2] %>% as.numeric()) %>%\n  left_join(olympic_riders, .) %>%\n  left_join(bt4_home_effect) %>%\n  replace_na(list(rider_home_effect = 0))\n\nbt4_strength_summ <- bt4_strengths %>%\n  group_by(rider_name) %>%\n  summarise(\n    is_not_home = all(is.na(home_effect )),\n    q5 = quantile(value, 0.05),\n    median = quantile(value, 0.5),\n    q95 = quantile(value, 0.95),\n    ha_q5 = quantile(value + rider_home_effect, 0.05),\n    ha_q95 = quantile(value + rider_home_effect, 0.95),\n  ) %>%\n  mutate(rider_name = fct_reorder(rider_name,median))\n\nggplot(bt4_strength_summ) +\n  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name), color = infreq_palette[\"darkblue\"]) +\n  geom_segment(data = bt4_strength_summ %>% filter(is_not_home == FALSE), aes(x=ha_q5,xend=ha_q95,y=rider_name,yend=rider_name), color = infreq_palette[\"orange\"], size = 1.1) +\n  labs(y=\"\",x=\"Strength, α\")\n\n\n\n\nUnfortunately for Yuka, it would appear that the home advantage is unlikely to be sufficient to help her secure a medal.\nIn hindsight home advantage is not likely to have a significant impact on the evaluation metrics as so few of the matches have a home athlete; this is confirmed below.\n\n\nAccuracy\n\n\n\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n0.648\n0.716\n0.773\nbt2\nevaluation\n0.648\n0.716\n0.773\nbt3\nevaluation\n0.648\n0.716\n0.773\nbt4\nevaluation\n0.648\n0.716\n0.773\nbt1\ntraining\n0.764\n0.787\n0.807\nbt2\ntraining\n0.770\n0.791\n0.810\nbt3\ntraining\n0.768\n0.789\n0.808\nbt4\ntraining\n0.770\n0.791\n0.808\n\n\n\n\n\n\nMatch Log Loss\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n-1.087\n-0.919\n-0.800\nbt2\nevaluation\n-1.169\n-0.951\n-0.809\nbt3\nevaluation\n-1.096\n-0.928\n-0.809\nbt4\nevaluation\n-1.080\n-0.916\n-0.798\nbt1\ntraining\n-0.642\n-0.612\n-0.586\nbt2\ntraining\n-0.635\n-0.603\n-0.575\nbt3\ntraining\n-0.612\n-0.588\n-0.567\nbt4\ntraining\n-0.613\n-0.588\n-0.566\n\n\n\n\n\n\nTime Varying Strengths\nUp to now I’ve made the assumption that the athletes’ strengths do not vary over time. Given the data spans a period of multiple years this is a definite limitation in the model.\nIn my early attempts to introduce a time dependent component I considered a random walk dynamic, supposing that\n\\[ \\alpha_r^{(m)} = \\alpha_r^{(m-1)} + \\text{Normal}(0, \\sigma_{m}).\\]\nwhere the standard deviation \\(\\sigma_m\\) depends on the time since the last match; appealing to theory about random walks, I’d supposed this was proportionate to the square root of the time difference to the last match.\n\nThis is a continuous time equivalent of an AR(1) process.\nThe problem with the model as defined above is that it requires that we determine some initial position, \\(\\alpha_r^{(0)}\\) from which successive strengths move away from, with uncertainty growing over time. This is appropriate if there is some fixed time point at which we know the riders’ strengths - but this is not the case in our situation.\nTo account for this a more sophisticated approach is required - and for that I’ll make use of Gaussian Processes (GPs). I won’t fully introduce GPs here, but they provide a framework for putting a distribution on functions, with the property that the value that the function takes at any finite subset of points are Normally distributed with some (known) covariance matrix.\n\nGörtler et al. have a really nice introduction, along with helpful interactive plots here.\nI’ll fit a relatively simple GP, for each athlete’s strength at a given date.These will be determined by two parameters (per athlete) that determine how far the athlete’s strength is allowed to deviate from its average (the magnitude) and how fast it can change (the length scale).\n\nI’m using the squared exponential kernel.\nI’ll admit now I haven’t had a chance to fully rationalise my position on priors for the Gaussian Process (which might go some way to explaining the model performance below!) - so won’t labour the explanation for the decision. Suffice it to say that I wanted to control the magnitude so that rider strengths wouldn’t fluctuate by more than \\(\\pm 1\\) over their career, and changes would happen on the scale of years, not months.\n\nThese principles are captured by the priors \\(\\tau_0 \\sim \\text{InvGamma}(11,1)\\) and \\(\\rho_0 \\sim \\text{InvGamma}(6,3)\\) below.\nThe plot below shows draws from the prior distribution for the GP on a single rider - giving a feel for the types of trends that the prior distribution is putting weight on. The GP I’ve chosen is stationary meaning that at any point the expected value is 0.\n\nShow code\nrider_days <- read_remote_target(\"rider_days\") %>% mutate(date_index = 1:n())\n\nalphaD_prior_draws <- read_remote_target(\"bt_draws_gp_prior\")  %>%\n      filter(.draw %% 800 == 0 ) %>%\n      gather(variable, value, -starts_with(\".\")) %>%\n      filter(str_detect(variable, 'f')) %>%\n      mutate(date_index = str_match(variable, \"\\\\[(.*)\\\\]\")[,2] %>% as.numeric()) %>%\n      left_join(rider_days) %>%\n      left_join(riders) %>%\n      filter(rider_name %in% c(\"LEE WAI SZE\", \"VOINOVA ANASTASIIA\", \"HINZE EMMA\"))\n\nggplot(data = alphaD_prior_draws %>% filter(rider_name == 'LEE WAI SZE')) +\n  geom_line( aes(x=date, y=value,color=rider_name, group = .draw), alpha = 0.2) +\n  geom_line(aes(x = date, y = 0), size = 2 ) +\n  ylab(\"Strength\") + xlab(\"\") + theme(legend.position = \"none\")\n\n\n\n\n\nThe GP produces smooth curves - the lack of smoothness in the plot is due to the frequency at which I’ve sampled the functions for plotting.\n\n\nBT5 \\[\n\\begin{align*}\n\\bf{\\text{Priors}} \\\\\n\\sigma & \\sim \\text{Gamma}(80,60) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\eta & \\sim \\text{Gamma}(2,4) \\\\\n\\upsilon & \\sim \\text{Normal}(0, 0.2) \\\\\n\\theta_r & \\sim \\text{Normal}(0,\\upsilon) \\\\\n\\\\\n\\tau_0 & \\sim \\text{InvGamma}(11,1)\\\\\n\\rho_0 & \\sim \\text{InvGamma}(6,3)\\\\\n\\tau_r & \\sim \\text{Normal}(\\tau_0, 0.05)\\\\\n\\rho_r & \\sim \\text{Normal}(\\rho_0, 0.1)\\\\\nf_r^{(m)} & \\sim \\text{GaussianProcess}(0, K^{SE}(\\tau_r, \\rho_r)) \\\\\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_r^{(m)} & = \\alpha_r + (\\eta + \\theta_r)\\mathbf{\\large 1}_r^{(m)} + f_r^{(m)}\\\\\n\\alpha_w^{(m)} - \\alpha_l^{(m)} | S & \\sim L_{\\text{match}}(\\alpha_r^{(m)} - \\alpha_s^{(m)} | S)\n\\end{align*}\n\\]\n\n\nExample Data\nTo fit this model two data sets were required. The match data now has additional columns indicating a date index for each rider\n\nShow code\n# read match data from remote _target store\nmatches <- read_remote_target(\"matches\")\n\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id, sprints, winner_at_home, loser_at_home,\n         winner_date_no, loser_date_no) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\nsprints\n2\nwinner_at_home\n1\nloser_at_home\n0\nwinner_date_no\n21\nloser_date_no\n23\n\nThese then link to a separate data set that provides information about the dates on which the athlete has raced\n\nShow code\nrider_days_sample <-  rider_days[100,] %>%\n  mutate(across(everything(), as.character))\n\nrider_days_sample <- tibble(Field = names(rider_days_sample), Example = unlist(rider_days_sample[1,]))\nrider_days_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\ndate\n2021-08-02\nrider_id\n16\nrider_date_no\n1\ndays_to_prev\n1082\ndays_to_start\n1082\nyears_to_start\n2.96235455167693\ndate_index\n100\n\n\n\nStan code\nSince the model requires in excess of 100 GPs to be fit, I’ve made use of the Hilbert space approximation to GPs analysed by Riutort-Mayol, et al.\nA recent case study by Aki Vehtari makes for a lighter introduction.\n\nfunctions {\n  real match_logit_lpmf(int[] s, vector delta){\n    real lpmf = 0;\n    vector[num_elements(delta)] p = inv_logit(delta);\n    \n    for(n in 1:num_elements(s)){\n      if (s[n] == 1) lpmf += log(p[n]);\n      else if (s[n] == 2) lpmf += 2*log(p[n]);\n      else lpmf += log(2) + 2*log(p[n]) + log(1-p[n]);\n    }\n    return lpmf;\n  }\n  \n  real accuracy(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n    real acc = 0;\n    \n    for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n    \n    return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real ll = 0;\n    \n    return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n  \n  real match_log_loss(int[] s, vector delta, int start, int end){\n    int s_seg[end - start + 1]  = segment(s, start, end - start + 1);\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real mll = 0;\n    \n    return inv(num_elements(s_seg)) * match_logit_lpmf(s_seg | delta_seg);\n  }\n  \n  // basis function approx. to GPs, developed by Riutort-Mayol et al. arxiv.org/2004.11408\n  // implementation from\n  // https://github.com/avehtari/casestudies/blob/master/Motorcycle/gpbasisfun_functions.stan\n  vector diagSPD_exp_quad(real alpha, real rho, real L, int B) {\n  return sqrt((alpha^1) * sqrt(2*pi()) * rho * exp(-0.5*(rho*pi()/2/L)^2 * linspaced_vector(B, 1, B)^2));\n  }\n  \n  matrix PHI(int N, int B, real L, vector x) {\n  return sin(diag_post_multiply(rep_matrix(pi()/(2*L) * (x+L), B), linspaced_vector(B, 1, B)))/sqrt(L);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n  int<lower=0> D; // Dates\n  \n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n  int<lower=1,upper=3> sprints[M];\n    \n  // Home advantage effects\n  real<lower=0,upper=1> winner_at_home[M];\n  real<lower=0,upper=1> loser_at_home[M];\n  \n  // Time series effects\n  int<lower=0> winner_date_no[M];\n  int<lower=0> loser_date_no[M];\n  int<lower=0> date_index_R[R+1];\n  vector[D] rider_dates;\n  \n  // basis function approx.\n  int<lower=1> B; \n}\n\ntransformed data {\n  vector[D] d_sc;  // rider dates centred/scaled\n  real<lower=0> L; // GP basis function window length\n  matrix[D,B] PHI_f; // GP basis function matrix\n  \n  {\n    real mu_d = mean(rider_dates);\n    real sd_d = sd(rider_dates);\n    d_sc = (rider_dates-mu_d)/sd_d;\n    L = 1.5 * max(d_sc); // set based on https://avehtari.github.io/casestudies/Motorcycle/motorcycle.html\n  }\n  \n  PHI_f = PHI(D, B, L, d_sc);\n}\n\nparameters {\n  // Rider average strength\n  real alpha0[R];\n  real<lower=0>sigma;\n    \n  // home advantage effect\n  real<lower=0>eta;\n  real<lower=0>upsilon;\n  real theta[R];\n  \n  // GP parameters and hyperparameters\n    real<lower=0> rho_pr;   // lengthscale hyperparameter\n    real<lower=0> tau_pr;   // magnitude hyperparameter\n    vector<lower=0>[R] rho; // lengthscale\n    vector<lower=0>[R] tau; // magnitude\n    vector[R*B] zeta;       // latent variables\n}\n\ntransformed parameters{\n    vector[D] f;       // approximate GP\n    vector[D] alphaD;  // approx. GP with centered on average strength\n  vector[M] delta;   // match differences\n  real eta_theta[R];\n  \n  for(r in 1:R){\n    \n    eta_theta[r] = eta + theta[r];\n    \n    vector[B] diagSPD_f_r = diagSPD_exp_quad(tau[r], rho[r], L, B);\n    \n    // basis function approx. to Gaussian Process\n    f[date_index_R[r]:(date_index_R[r+1]-1)] =\n    PHI_f[date_index_R[r]:(date_index_R[r+1]-1),] * (diagSPD_f_r .* segment(zeta, 1 + (r-1)*B, B));\n\n    // time dependent rider strength\n    alphaD[date_index_R[r]:(date_index_R[r+1]-1)] =  alpha0[r] + f[date_index_R[r]:(date_index_R[r+1]-1)];\n  }\n      \n    for(m in 1:M){\n      delta[m] = \n       alphaD[date_index_R[winner_id[m]] + winner_date_no[m]] - alphaD[date_index_R[loser_id[m]] + loser_date_no[m]] +\n      (winner_at_home[m] * eta_theta[winner_id[m]]) - (loser_at_home[m] * eta_theta[loser_id[m]]);\n    }\n}\n\nmodel{\n  // (hierarchical) priors - strength\n  sigma ~ gamma(80,60);\n  alpha0 ~ normal(0,sigma); \n  \n  // (hierarchical) priors - home effect\n  eta ~ gamma(2,4);\n  upsilon ~ normal(0,0.2);\n  theta ~ normal(0, upsilon);\n\n  // Gaussian process lengthscale/magnitude\n  rho_pr ~ inv_gamma(6,3);\n  tau_pr ~ inv_gamma(11,1);\n    rho ~ normal(rho_pr, 0.1);      // lengthscale GP\n    tau ~ normal(tau_pr, 0.05);     // magnitude GP\n    zeta ~ normal(0,1);       // latent variables for approx. Gaussian Process\n    \n  // likelihood\n  head(sprints, T) ~ match_logit(head(delta, T));\n}\n\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_match_log_loss;\n  real evaluation_match_log_loss;\n\n  training_accuracy = accuracy(delta,1, T);\n  training_match_log_loss = match_log_loss(sprints, delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_match_log_loss = match_log_loss(sprints, delta, T+1, M);\n}\n\n\n\nBT5. Time varying strengths.\nThe posterior plots below shows how the GP fits pick up changes in the riders strenghts over time - and suggest that Lee Wai Sze’s form may be starting to decline.\n\nShow code\nrider_days <- read_remote_target(\"rider_days\") %>% mutate(date_index = 1:n())\n\n    alphaD_draws <- read_remote_target(\"bt_draws_bt5\")  %>%\n      filter(.draw %% 100 == 0) %>%\n      gather(variable, value, -starts_with(\".\")) %>%\n      filter(str_detect(variable, 'alphaD')) %>%\n      mutate(date_index = str_match(variable, \"\\\\[(.*)\\\\]\")[,2] %>% as.numeric()) %>%\n      left_join(rider_days) %>%\n      left_join(riders) %>%\n      filter(rider_name %in% c(\"LEE WAI SZE\", \"VOINOVA ANASTASIIA\", \"HINZE EMMA\"))\n\nggplot() +\n  geom_line(data = alphaD_draws %>% filter(rider_name == 'LEE WAI SZE'), aes(x=date, y=value,color=rider_name, group = .draw), alpha = 0.1) +\n  geom_smooth(data = alphaD_draws %>% filter(rider_name == 'LEE WAI SZE'), aes(x=date, y=value,color=rider_name), se = FALSE, size = 2) +\n  \n  geom_line(data = alphaD_draws %>% filter(rider_name == 'HINZE EMMA'), aes(x=date, y=value,color=rider_name, group = .draw), alpha = 0.1) +\n  geom_smooth(data = alphaD_draws %>% filter(rider_name == 'HINZE EMMA'), aes(x=date, y=value,color=rider_name), se = FALSE, size = 2) +\n  \n  geom_line(data = alphaD_draws %>% filter(rider_name == \"VOINOVA ANASTASIIA\"), aes(x=date, y=value,color=rider_name, group = .draw), alpha = 0.1) +\n  geom_smooth(data = alphaD_draws %>% filter(rider_name == \"VOINOVA ANASTASIIA\"), aes(x=date, y=value,color=rider_name), se = FALSE, size = 2) +\n  theme(legend.title = element_blank()) +\n  ylab(\"Strength\") + xlab(\"\")\n\n\n\n\nThe model summary below indicates that introducing the GP appears to lower the accuracy on the evaluation data: although in reality this is the difference of just a single match.\n\n\nAccuracy\n\n\n\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n0.648\n0.716\n0.773\nbt2\nevaluation\n0.648\n0.716\n0.773\nbt3\nevaluation\n0.648\n0.716\n0.773\nbt4\nevaluation\n0.648\n0.716\n0.773\nbt5\nevaluation\n0.636\n0.705\n0.761\nbt1\ntraining\n0.764\n0.787\n0.807\nbt2\ntraining\n0.770\n0.791\n0.810\nbt3\ntraining\n0.768\n0.789\n0.808\nbt4\ntraining\n0.770\n0.791\n0.808\nbt5\ntraining\n0.780\n0.803\n0.824\n\n\n\n\n\n\nMatch Log Loss\nData\nShow code\nlog_loss <- measures %>%\n  # totals are given in round_id = 6\n  filter(measure == \"match_log_loss\")\n\nlog_loss %>% \n  select(model, split,  q5, median, q95) %>%\n  arrange(split, model) %>%\n  kable(\"pipe\", col.names =c(\"Model\", \"Split\", \"5%\", \"50% (Median)\", \"95%\"), digits =3)\n\n\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n-1.087\n-0.919\n-0.800\nbt2\nevaluation\n-1.169\n-0.951\n-0.809\nbt3\nevaluation\n-1.096\n-0.928\n-0.809\nbt4\nevaluation\n-1.080\n-0.916\n-0.798\nbt5\nevaluation\n-1.130\n-0.942\n-0.808\nbt1\ntraining\n-0.642\n-0.612\n-0.586\nbt2\ntraining\n-0.635\n-0.603\n-0.575\nbt3\ntraining\n-0.612\n-0.588\n-0.567\nbt4\ntraining\n-0.613\n-0.588\n-0.566\nbt5\ntraining\n-0.589\n-0.559\n-0.532\n\n\n\n\n\n\nPerhaps against better judgement I’m going to retain the GP contribution in the model going forward. This might be the result of bias - a reluctance to throw away my hard work - but I’d like to think I have a few more rational justifications:\nThe difference in accuracy is negligible, and given the small sample size of the evaluation data I don’t consider this to be particularly reliable.\nThe worsening in the log loss is due to some of the more extreme strengths assigned to Lee Wai Sze at her peak. The fit suggests she is now on a decline, and if that is the case we want the model to account for this as her data will be the single biggest driver of the odds.\n\nLee Wai Sze is reaching retirement, and in addition is reported to have had challenges off the bike.\nThe next, and final, model adaptation will reduce the contribution of the Gaussian Process.\nI’ll reflect on this decision in the final post, once predictions are made and the actual results are known.\nQualifying Times\nThe final revision I’ll make to the model is to take into account one final dynamic of the event format - the qualifying round.\nPrior to any individual matches taking place, each athlete completes a sprint on their own (usually a 200m effort, with a flying start). The times for this sprint decide the seeding of the matches: with the fastest rider facing the slowest qualifying rider, and so on.\nAssuming that athletes complete the qualifying round with the aim of being the fastest this will be a good predictor of the pure speed they currently have. The combination of this current form, and their tactical skill (measurable from our historic match data) should combine to improve the model.\nFor a given match \\(m\\), I’ll denote \\(q_{r}^{(m)}\\) for the time the athlete posted in the qualifying round for the event, and will update the rider’s strength for the match to have an additional factor \\(-\\kappa q_{r}^{(m)}\\). The parameter \\(\\kappa\\) will be fixed across all riders and events.\n\nThe reason for using \\(-\\kappa\\) rather than \\(+\\kappa\\) is to retain the convention that larger parameter values favour the winning rider.\nAs with a number of the other parameters it makes sense to assume this can vary: since speeds in velodromes are highly dependent on the the conditions (air pressure, temperature, altitude) so it makes sense to allow \\(\\kappa\\) to vary between events. I will therefore consider separate coefficients for each event:\n\\[ \\kappa^{(m)} = \\kappa + \\phi_{E_m}\\] where \\(m\\) is a match taking place during event \\(E_m\\).\nAs usual we need to choose a prior for \\(\\kappa\\) - I’ll do a back of the envelope calculation to set an appropriate scale. First off, qualifying time differences are usually between 0 and 0.5 seconds: so lets take the common scenario of a time difference of 0.2 seconds.\nSuppose then that this time difference is observed, but otherwise two riders are believed to have the same strength. In that scenario I’d back the faster qualifier to win with a probability between 0.55 and 0.8. Converting back, that implies values of \\(\\kappa\\) between 1 and 7.\n\nBy taking \\(\\text{logit}(0.55)/0.2 \\approx 6.9\\) and \\(\\text{logit}(0.55)/0.2 \\approx 6.9\\).\nI don’t want to rule out the scenario that perhaps qualifying faster is detrimental - so overall I’ll pick a fairly broad prior of \\(\\kappa \\sim \\text{Normal}(4,4)\\) which allows for that scenario, whilst favouring my estimated range above.\nFor the hierarchical event level term I’ll suppose \\(\\phi_{E_m} \\sim \\text{Normal}(0,\\psi)\\) and put a prior on the unknown standard deviation \\(\\psi \\sim \\text{HalfNormal}(0, \\frac12)\\).\n\n\nBT6 \\[\n\\begin{align*}\n\\bf{\\text{Priors}} \\\\\n\\sigma & \\sim \\text{Gamma}(80,60) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\eta & \\sim \\text{Gamma}(2,4) \\\\\n\\upsilon & \\sim \\text{Normal}(0, 0.2) \\\\\n\\theta_r & \\sim \\text{Normal}(0,\\upsilon) \\\\\n\\\\\n\\tau_0 & \\sim \\text{InvGamma}(11,1)\\\\\n\\rho_0 & \\sim \\text{InvGamma}(6,3)\\\\\n\\tau_r & \\sim \\text{Normal}(\\tau_0, 0.05)\\\\\n\\rho_r & \\sim \\text{Normal}(\\rho_0, 0.1)\\\\\nf_r^{(m)} & \\sim \\text{GaussianProcess}(0, K^{SE}(\\tau_r, \\rho_r)) \\\\\n\\\\\n\\kappa & \\sim \\text{Normal}(4,4)\\\\\n\\psi & \\sim \\text{HalfNormal}(0,\\frac12)\\\\\n\\phi_{E_m} & \\sim \\text{Normal}(0,\\psi)\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_r^{(m)} & = \\alpha_r + (\\eta + \\theta_r)\\mathbf{\\large 1}_r^{(m)} + f_r^{(m)} - (\\kappa + \\phi_{E_m}) q_r^{(m)}\\\\\n\\alpha_w^{(m)} - \\alpha_l^{(m)} | S & \\sim L_{\\text{match}}(\\alpha_r^{(m)} - \\alpha_s^{(m)} | S)\n\\end{align*}\n\\]\n\n\nExample Data\nAs in model BT5, we need two data sets. In addition to the data introduced there, we now have the further winner_qual_time_diff column with the qualifying time differences between the two riders.\n\nShow code\n# read match data from remote _target store\nmatches <- read_remote_target(\"matches\")\n\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id, sprints, winner_at_home, loser_at_home,\n         winner_date_no, loser_date_no, winner_qual_time_diff) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\nsprints\n2\nwinner_at_home\n1\nloser_at_home\n0\nwinner_date_no\n21\nloser_date_no\n23\nwinner_qual_time_diff\n-0.257999999999999\n\nThese then link to a separate data set that provides information about the dates on which the athlete has raced\n\nShow code\nrider_days <- read_remote_target(\"rider_days\")\n\nrider_days_sample <-  rider_days[100,] %>%\n  mutate(across(everything(), as.character))\n\nrider_days_sample <- tibble(Field = names(rider_days_sample), Example = unlist(rider_days_sample[1,]))\nrider_days_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\ndate\n2021-08-02\nrider_id\n16\nrider_date_no\n1\ndays_to_prev\n1082\ndays_to_start\n1082\nyears_to_start\n2.96235455167693\n\n\n\nStan code\n\nfunctions {\n  real match_logit_lpmf(int[] s, vector delta){\n    real lpmf = 0;\n    vector[num_elements(delta)] p = inv_logit(delta);\n    \n    for(n in 1:num_elements(s)){\n      if (s[n] == 1) lpmf += log(p[n]);\n      else if (s[n] == 2) lpmf += 2*log(p[n]);\n      else lpmf += log(2) + 2*log(p[n]) + log(1-p[n]);\n    }\n    return lpmf;\n  }\n  \n  real accuracy(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n    real acc = 0;\n    \n    for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n    \n    return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real ll = 0;\n    \n    return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n  \n  real match_log_loss(int[] s, vector delta, int start, int end){\n    int s_seg[end - start + 1]  = segment(s, start, end - start + 1);\n    vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n    real mll = 0;\n    \n    return inv(num_elements(s_seg)) * match_logit_lpmf(s_seg | delta_seg);\n  }\n  \n  // basis function approx. to GPs, developed by Riutort-Mayol et al. arxiv.org/2004.11408\n  // implementation from\n  // https://github.com/avehtari/casestudies/blob/master/Motorcycle/gpbasisfun_functions.stan\n  vector diagSPD_exp_quad(real alpha, real rho, real L, int B) {\n  return sqrt((alpha^1) * sqrt(2*pi()) * rho * exp(-0.5*(rho*pi()/2/L)^2 * linspaced_vector(B, 1, B)^2));\n  }\n  \n  matrix PHI(int N, int B, real L, vector x) {\n  return sin(diag_post_multiply(rep_matrix(pi()/(2*L) * (x+L), B), linspaced_vector(B, 1, B)))/sqrt(L);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n  int<lower=0> D; // Dates\n  int<lower=0> E; // Events\n  \n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n  int<lower=1,upper=3> sprints[M];\n    \n  // Home advantage effects\n  real<lower=0,upper=1> winner_at_home[M];\n  real<lower=0,upper=1> loser_at_home[M];\n  \n  // Time series effects\n  int<lower=0> winner_date_no[M];\n  int<lower=0> loser_date_no[M];\n  int<lower=0> date_index_R[R+1];\n  vector[D] rider_dates;\n  \n  // basis function approx.\n  int<lower=1> B;\n  \n  // qualifying time difference;\n  real qual_diff[M];\n  int event[M];\n}\n\ntransformed data {\n  vector[D] d_sc;  // rider dates centred/scaled\n  real<lower=0> L; // GP basis function window length\n  matrix[D,B] PHI_f; // GP basis function matrix\n  \n  {\n    real mu_d = mean(rider_dates);\n    real sd_d = sd(rider_dates);\n    d_sc = (rider_dates-mu_d)/sd_d;\n    L = 1.5 * max(d_sc); // set based on https://avehtari.github.io/casestudies/Motorcycle/motorcycle.html\n  }\n  \n PHI_f = PHI(D, B, L, d_sc);\n}\n\nparameters {\n  // Rider average strength\n  real alpha0[R];\n  real<lower=0>sigma;\n    \n  // home advantage effect\n  real<lower=0>eta;\n  real<lower=0>upsilon;\n  real theta[R];\n  \n  // GP parameters and hyperparameters\n    real<lower=0> rho_pr;   // lengthscale hyperparameter\n    real<lower=0> tau_pr;   // magnitude hyperparameter\n    vector<lower=0>[R] rho; // lengthscale\n    vector<lower=0>[R] tau; // magnitude\n    vector[R*B] zeta;       // latent variables\n    \n    // qualifying time differences;\n    real kappa;\n    real<lower=0> psi;\n    real phi[E];\n}\n\ntransformed parameters{\n    vector[D] f;       // approximate GP\n    vector[D] alphaD;  // approx. GP with centered on average strength\n  vector[M] delta;   // match differences\n  real eta_theta[R];\n  \n  for(r in 1:R){\n    \n    eta_theta[r] = eta + theta[r];\n    \n    vector[B] diagSPD_f_r = diagSPD_exp_quad(tau[r], rho[r], L, B);\n    \n    // basis function approx. to Gaussian Process\n    f[date_index_R[r]:(date_index_R[r+1]-1)] =\n    PHI_f[date_index_R[r]:(date_index_R[r+1]-1),] * (diagSPD_f_r .* segment(zeta, 1 + (r-1)*B, B));\n\n    // time dependent rider strength\n    alphaD[date_index_R[r]:(date_index_R[r+1]-1)] =  alpha0[r] + f[date_index_R[r]:(date_index_R[r+1]-1)];\n  }\n      \n    for(m in 1:M){\n      delta[m] = alphaD[date_index_R[winner_id[m]] + winner_date_no[m]] - alphaD[date_index_R[loser_id[m]] + loser_date_no[m]] +\n      (winner_at_home[m] * eta_theta[winner_id[m]]) - (loser_at_home[m] * eta_theta[loser_id[m]]) -\n      (kappa + phi[event[m]]) * qual_diff[m];\n    }\n}\n\nmodel{\n  // (hierarchical) priors - strength\n  sigma ~ gamma(80,60);\n  alpha0 ~ normal(0,sigma); \n  \n  // (hierarchical) priors - home effect\n  eta ~ gamma(2,4);\n  upsilon ~ normal(0,0.2);\n  theta ~ normal(0, upsilon);\n\n  // Gaussian process lengthscale/magnitude\n  rho_pr ~ inv_gamma(6,3);\n  tau_pr ~ inv_gamma(11,1);\n    rho ~ normal(rho_pr, 0.1);      // lengthscale GP\n    tau ~ normal(tau_pr, 0.05);     // magnitude GP\n    zeta ~ normal(0,1);       // latent variables for approx. Gaussian Process\n\n    // qualifying time difference;\n    kappa ~ normal(4,4);\n    psi ~ normal(0,0.5);\n    phi ~ normal(0, psi);\n    \n\n  head(sprints, T) ~ match_logit(head(delta, T));\n}\n\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_match_log_loss;\n  real evaluation_match_log_loss;\n\n  training_accuracy = accuracy(delta,1, T);\n  training_match_log_loss = match_log_loss(sprints, delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_match_log_loss = match_log_loss(sprints, delta, T+1, M);\n}\n\n\n\nBT6. Qualifying Times.\n\n\n\nIncluding the qualifying data turns out to be the magic bullet to boost the model’s performance, substantially lifting both the accuracy and match log loss.\n\n\nAccuracy\n\n\n\nData\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n0.648\n0.716\n0.773\nbt2\nevaluation\n0.648\n0.716\n0.773\nbt3\nevaluation\n0.648\n0.716\n0.773\nbt4\nevaluation\n0.648\n0.716\n0.773\nbt5\nevaluation\n0.636\n0.705\n0.761\nbt6\nevaluation\n0.761\n0.818\n0.852\nbt1\ntraining\n0.764\n0.787\n0.807\nbt2\ntraining\n0.770\n0.791\n0.810\nbt3\ntraining\n0.768\n0.789\n0.808\nbt4\ntraining\n0.770\n0.791\n0.808\nbt5\ntraining\n0.780\n0.803\n0.824\nbt6\ntraining\n0.843\n0.860\n0.877\n\n\n\n\n\n\nMatch Log Loss\nData\nShow code\nlog_loss <- measures %>%\n  # totals are given in round_id = 6\n  filter(measure == \"match_log_loss\")\n\nlog_loss %>% \n  select(model, split,  q5, median, q95) %>%\n  arrange(split, model) %>%\n  kable(\"pipe\", col.names =c(\"Model\", \"Split\", \"5%\", \"50% (Median)\", \"95%\"), digits =3)\n\n\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n-1.087\n-0.919\n-0.800\nbt2\nevaluation\n-1.169\n-0.951\n-0.809\nbt3\nevaluation\n-1.096\n-0.928\n-0.809\nbt4\nevaluation\n-1.080\n-0.916\n-0.798\nbt5\nevaluation\n-1.130\n-0.942\n-0.808\nbt6\nevaluation\n-0.909\n-0.751\n-0.625\nbt1\ntraining\n-0.642\n-0.612\n-0.586\nbt2\ntraining\n-0.635\n-0.603\n-0.575\nbt3\ntraining\n-0.612\n-0.588\n-0.567\nbt4\ntraining\n-0.613\n-0.588\n-0.566\nbt5\ntraining\n-0.589\n-0.559\n-0.532\nbt6\ntraining\n-0.445\n-0.423\n-0.404\n\n\n\n\n\n\nRecall that in the first post I defined the prior on the main athlete strengths, \\(\\alpha_r\\), with the belief that the odds that the weakest athlete can beat the strongest would be between 1-100 and 1-1,000.\nIn all our models this prior stopped the model from erring towards really extreme values for these odds - but never brought them down to the scale I believe plausible. The table below summarises the posterior odds under the two latest models, showing that including qualifying data finally makes these extremal odds look plausible.\n\nShow code\nbt5_max_odds <- bt5_summ %>%\n  filter(str_detect(variable, 'delta_max')) %>%\n  select(q5, median, q95) %>%\n  mutate(\n    # calculation is based on the fact that the strength difference is the log odds\n    # on the exponential scale\n    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = \",\"))\n  )\n\nbt6_max_odds <- bt6_summ %>%\n  filter(str_detect(variable, 'delta_max')) %>%\n  select(q5, median, q95) %>%\n  mutate(\n    # calculation is based on the fact that the strength difference is the log odds\n    # on the exponential scale\n    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = \",\"))\n  )\n\nbind_rows(\n  bt5_max_odds %>% add_column(model = 'bt5', .before = 0),\n  bt6_max_odds %>% add_column(model = 'bt6', .before = 0)\n) %>%\n  arrange(model) %>%\n  kable(col.names = c(\"\", \"5%\", \"50%\", \"95%\"), digits = 2) %>%\n  kable_styling(full_width=FALSE)\n\n\n\n\n\n5%\n\n\n50%\n\n\n95%\n\n\nbt5\n\n\n1,100\n\n\n4,600\n\n\n30,200\n\n\nbt6\n\n\n100\n\n\n500\n\n\n2,500\n\n\n\nFigures should be read as 1 in …\nOne limitation of this final model is that we won’t be able to use its fully predictive power until the qualifying rounds at Tokyo 2020 have taken place. I’ll be looking to make some bets before and after qualifying, so will need to make use of the time dependent strength values from this model fit in the pre-qualifying bets.\nThe Final Fit and Reflections\nBefore wrapping up I’ll fit the model one more time, this time using all of the data.\nThe plot below provides the rider strengths as forecast for the 2nd August 2021 - the week over which the Individual Sprint medals will be contested. Importantly this model not only puts the rider strengths on a tighter scale, but also changes some of the athlete orders.\n\nHome effects have been included for Japanese athletes, in orange.\n\nShow code\nrider_days <- rider_days %>% mutate(date_id = 1:n())\n\nbt_final_draws <- read_remote_target(\"bt_draws_bt_final\")\n\nbt_final_draws <- bt_final_draws %>%\n  gather(variable, value, -starts_with(\".\"))\n\nbt_final_rider_home_effect <- left_join(japanese_riders, bt_final_draws) %>%\n  select(-variable) %>%\n  rename(rider_home_effect = value)\n\nbt_final_home_effect <- bt_final_draws %>%\n  filter(str_detect(variable, '^eta')) %>%\n  rename(home_effect = value) %>%\n  select(-variable) %>%\n  left_join(bt_final_rider_home_effect) %>%\n  mutate(rider_home_effect = rider_home_effect + home_effect)\n\nbt_final_strengths <- bt_final_draws %>%\n  filter(str_detect(variable, 'alphaD')) %>%\n  mutate(date_id = str_match(variable, '\\\\[(.*)\\\\]')[,2] %>% as.numeric()) %>%\n  left_join(rider_days) %>%\n  left_join(olympic_riders, .) %>%\n  filter(date == max(date)) %>%\n  left_join(bt_final_home_effect) %>%\n  replace_na(list(rider_home_effect = 0))\n\nbt_final_strength_summ <- bt_final_strengths %>%\n  group_by(rider_name) %>%\n  summarise(\n    is_not_home = all(is.na(home_effect )),\n    q5 = quantile(value + rider_home_effect, 0.05),\n    median = quantile(value+ rider_home_effect,  0.5),\n    q95 = quantile(value+ rider_home_effect, 0.95)\n  ) %>%\n  mutate(rider_name = fct_reorder(rider_name,median))\n\nggplot(bt_final_strength_summ, ) +\n  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name, color = is_not_home)) +\n  labs(y=\"\",x=\"Strength, α\") +\n  theme(legend.position = \"none\")\n\n\n\n\nOn reflection the first model development should have been to include the qualifying data - though this is easier to say in hindsight having seen the performance metrics.\nHaving said this - there is a give away that I got my prioritisation wrong that’s visible in my priors. For the qualifying time difference I felt an appropriate prior would swing an otherwise equal match to give the a rider who qualified 0.2 seconds faster a probability of winning between 0.55 - 0.8; for larger, and still common, differences of 0.4 seconds that grows further. On the other hand I felt that the impact of the home advantage was likely between 0.55 - 0.75.\nThat’s to say that my priors implicitly show my expectation was that qualifying difference would have a larger effect size than home advantage - and yet I started by coding up the effect I believed to be less important.\nTo be fair there are mitigating reasons - the home advantage is a text-book adaptation of the model whilst the qualifying difference is bespoke to this application - and required further data preparation.\nThe prior on the GP model was on a similar scale - and it felt important to let the model pick out trends. However - this is implicitly handled in the qualifying time data (as qualifying time is a predictor of form), and further the GP work required a lot of tweaking for little gain (perhaps some detriment even).\nFinally the very early detour to move to the match likelihood was something that principally was the correct change - it does better model the underlying race dynamic - but the key here is how little the change actually has to the model, as the bulk of matches are still single sprints.\nSo the key take away I’ll pull from this is to prioritise my work flow in the order of my prior distributions on effect sizes. Even if the priors turn out to be way off, its a better justification than a try-it-and-see attitude.\nMoving on - In the next post I’ll take the model as it now stands and turn this into betting odds, ahead of events kicking off in the velodrome.\nComments\nAcknowledgments\nThis post makes heavy use of R and Stan, and so benefits from the work of all those who contribute to their development.\n\n\n\n",
    "preview": "posts/2021-07-28-tokyo-2020-ii/img/best-of-three.png",
    "last_modified": "2021-07-31T16:32:17+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-23-tokyo-2020-i/",
    "title": "Tokyo 2020 Betting I: Predictive Models for Pairwise Matches",
    "description": "The first in a series of posts building towards betting on the track cycling at the Tokyo Olympics. This post introduces the series, and the basic model behind my attempt to win big at the bookies!",
    "author": [],
    "date": "2021-07-23",
    "categories": [
      "Bayesian",
      "Cycling"
    ],
    "contents": "\n\nContents\nTrack Sprinting and Data Overview\nThe Bradley-Terry Model\nPrior Predictive Checks\nAn Informative Hyperprior\nEvaluation Metrics\nNext Steps\nComments\n\nThis is the first post in a series: Click for links\nTokyo 2020 Betting I: Predictive Models for Pairwise Matches (This Post)\nTokyo 2020 Betting II: Model Refinement and Feature Engineering\nTokyo 2020 Betting III: From Matches to Medals… and Bookies\nThe Olympics are officially under way, and I’m primed to become an armchair expert in sports that I haven’t expressed any interest in for four years. As a proactive way to engage with the games I thought I’d explore how to integrate predictive modelling with betting strategies.\nStatistical models are common place amongst bookmakers, and I’m not naive enough to believe a model I threw together over a few days could beat a concerted effort by experienced sports analysts. However, I first explored this project for the 2019 Track Cycling World Championships, but UK bookmakers weren’t offering odds, let alone those derived from machine learning: so that gives me hope that a team of dedicated track cyling modelers simply doesn’t exist!\nI’m tentatively optimistic odds will be available this time around, what with it being the olympics. It’d be particularly fitting given cycling is one of only four legal betting markets in Japan.\nIn this post I’ll provide a brief introduction to the event I’m hoping to bet on, and the statistical model which will form the base of my strategy.\nSubsequent posts will refine the model through feature engineering, and derive the betting strategy itself. If the bookies play-ball and provide odds, I’ll place some bets and summarise how this experiment turned out!\nI’m also hoping this series will serve as an example of my analytical workflow: to that end I’ll explain some of the errors and simplifications I made along the way, how I spotted and resolved them.\n\nThere are likely modelling errors I haven’t noticed yet, comments are welcome!\nNot least I’ll say up front this work might not yield usable odds! Like many predictive models, this one is susceptible to Covid 19 impacts: most of the track cycling calendar for 2020 and 2021 was cancelled so we simply don’t have much recent data.\nBut I won’t let that spoil the fun, after all you can’t win if you don’t play the game…\nAssumptions\nThis post makes some assumptions about you: that you have some prior exposure to (or are keen to learn) regression, and evaluation metrics for classification models.\nSpecifically I’ll assume you’re familiar with logistic regression and evaluating predictive performance with accuracy and log-loss.\nIf you’re interested in reading the underlying code, this is in R and Stan.\nTrack Sprinting and Data Overview\nA rule of thumb in modelling the outcome of sports matches is that rather than predicting binary win/lose probabilities, its better to predict expected score (or time) differences and infer the winner from this.\nAs Andrew Gelman puts it (in the context of election modelling): there’s a lot of information in the score (or vote) differential that’s thrown away if you just look at win/loss.\nThe Individual Sprint discipline in track cycling is interesting from a modelling perspective, as it forces us to diverge from this wisdom.\nThe sprint is contested by two athletes with the winner being the first across the line. Unlike its track and field equivalent, the sprint isn’t contested from the start: the race starts slowly with a tactical game of cat-and-mouse, before one rider tries to catch the other unaware and starts their sprint. This dynamic renders race times meaningless for prediction.\n\nA great example is the London 2012 women’s sprint final between Victoria Pendleton and Anna Meares.\nAbout the Data\nDetailed match results for most large track cycling events are maintained by Tissot. These results are made available in a common PDF format, that I have parsed to extract structured data from.\n\nThis work is available separately on GitHub\n\n\n\nThe data I’m using spans ~4 years from November 2017 to July 2021, and for this post I’ll focus on the women’s competition. During that period I have data for a total of 610 matches between 123 athletes.\nThe sample below shows the fields that are immediately relevant to the initial model; I’ll explore the predictive strength of further information in the next post.\n\n\nField\n\n\nExample\n\n\nevent\n\n\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\n\n\ndate\n\n\n2020-02-28\n\n\ngender\n\n\nWomen\n\n\nround\n\n\nFinals\n\n\nrider_1\n\n\nHINZE EMMA\n\n\nrider_2\n\n\nVOINOVA ANASTASIIA\n\n\nrider_id_1\n\n\n44\n\n\nrider_id_2\n\n\n123\n\n\nwinner_id\n\n\n44\n\n\nloser_id\n\n\n123\n\n\n\nThe rider ID fields are derived, and are used for numeric indexing in the models\nIn all model runs I’ll use data up to the end of 2019 for training, and then evaluate against data from 2020 onward: a total of 1118 matches in the training data, and 176 held out for evaluation.\nThe Bradley-Terry Model\nThe Bradley-Terry model is a well studied approach to paired comparisons, and particularly to win/lose matches, which originates in work by Zermelo, who used it to compare chess players abilities.\n\nThe model is the theoretical underpinning of the Elo Rating system used in chess ladders.\nThe model was re-discovered and popularised by Bradley and Terry in the 1950s - their example application is somewhat more esoteric, as they used it to rank pork roasts!\nI’ll introduce the model in the context of athletes competing in the Individual Sprint. Each of \\(R\\) athletes involved in the league are assumed to have some strength \\(\\beta_r\\), \\(r = 1,\\ldots, R\\), that describes their ability.\nThe Bradley-Terry model assumes that a rider \\(r\\) beats \\(s\\) with probability\n\\[\\mathbf P \\left[ r \\text{ beats } s\\right] = \\frac{\\beta_r}{\\beta_r + \\beta_s}.\\]\nThe aim is to estimate the strength parameters from historical matches, and then use these to predict future games.\nOne useful step is to change the scale of the strength parameters, considering \\(\\alpha = \\log \\beta\\), which changes the model to\n\\[ \n\\begin{align*}\n\\mathbf P \\left[ r \\text{ beats } s\\right] & = \\frac{e^{\\alpha_r}}{e^{\\alpha_r} + e^{\\alpha_s}} \\\\\n& = \\frac{e^{\\alpha_r - \\alpha_s}}{1 + e^{\\alpha_r - \\alpha_s}} \\\\\n& = \\text{logit}^{-1}\\big(\\alpha_r - \\alpha_s\\big)\n\\end{align*}\n\\]\nThis transformation has converted a complex fractional relationship between rider strengths into a linear relationship, inside a link function.\nSpecifically, this model is now equivalent to a classic logistic regression problem. To see this more clearly we can look at how we might implement this in code:\nWe create a data frame with a row per match, and \\(R + 1\\) columns. For a match betwee n riders \\(r\\) and \\(s\\)the row will have entries of 0 in each of the rider columns, except for an entry of +1 in the column for \\(r\\), and -1 for \\(s\\). The result column is set to 1 if the rider marked as +1 wins, and 0 if -1 wins.\n\nEx. \\(\\left(0,0,0,1,0,-1,0 ,\\, \\underline{1}\\right)\\) would denote a match between riders 4 and 6, in a league of 7, in which rider 4 won.\nPassing this data to your favouritetool for logistic regression (eg. glm in R) will fit the Bradley-Terry model as its defined above. Under the hood this will run an optimisation algorithm to find the vector of strengths \\(\\alpha^*\\) that maximises the likelihood.\nThere is a challenge however, as this solution won’t be unique: any translation by a constant, \\(\\alpha^* + C\\) will have the same likelihood. In the frequentist approach this is resolved by introducing a constraint that \\(\\sum_r \\alpha_r = 0\\).\nThis CrossValidated post has good answers explaining how to achieve this with R’s glm function.\nThe Bayesian Bradley-Terry Model\nRather than following the frequentist approach, I’ll fit a Bayesian Bradley-Terry model. My general rationale for working with Bayesian models is here, but in this instance there’s an added incentive as it opens an alternative method to resolving the identifiability issue above.\nThe idea is that rather than forcing the coefficients to sum to 0, we can use priors on the rider strengths that helps the likelihood to focus on one specific translation over all others. In our case we’ll assume athlete strengths are normally distributed, with mean \\(0\\), which will lean the model towards configurations with \\(\\sum_r \\alpha_r = 0\\), without the need to code up this constraint explicitly.\n\nThis is referred to as soft centering in the Stan guidance, and is explained alongside other constraint methods here\nRather than specifying the standard deviation for the prior I will treat this as an unknown hyperparameter, \\(\\sigma\\). This is known as partial pooling, or hierarchical effects, and has the effect of ensuring the athlete strengths are on a common scale.\nThe prior for \\(\\sigma\\) needs to be constrained to be positive (since standard deviation is positive), and for now I’ll pick a common default of a half-normal distribution, with variance of 1. Putting this together we have the following model specification:\n\n\nBT1 \\[\n\\begin{align*}\n\\bf{\\text{Priors}}\\\\\n\\sigma & \\sim \\text{Half-Normal}(0,1) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_r - \\alpha_s | W_{r,s} &\\sim \\text{logit}^{-1}(\\alpha_r - \\alpha_s)\n\\end{align*}\n\\]\n\n\nExample Data\n\nShow code\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\n\n\n\nStan code\n\n\nwriteLines(readLines(paste0(remote_project_path, 'stan/bt1.stan')))\n\n\n\nfunctions {\n  real accuracy(vector delta, int start, int end){\n      vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n      real acc = 0;\n      \n      for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n\n      return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n      vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n      real ll = 0;\n      \n      return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n  \n  // Match results\n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n}\n\nparameters {\n  // rider ratings\n  real<lower=0> sigma;\n  vector[R] alpha0;\n}\n\ntransformed parameters {\n  // difference of winner and loser rating\n  vector[M] delta =alpha0[winner_id] - alpha0[loser_id];\n}\n\nmodel {\n  // (hierarchical) priors - strength\n  sigma ~ normal(0,1);\n  alpha0 ~ normal(0,sigma);\n  \n  // likelihood\n  1 ~ bernoulli_logit(head(delta, T));\n}\n\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_log_loss;\n  real evaluation_log_loss;\n  \n  training_accuracy = accuracy(delta,1, T);\n  training_log_loss = log_loss(delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_log_loss = log_loss(delta, T+1, M);\n}\n\n\n\nBT1. Basic Bradley-Terry model.\nThroughout this series I’ll fit the models using Stan. The code for the model is available in the tab above. In these posts I’ll focus less on the implementation, and more on the outputs of the models.\n\nThe full model code, including the R pipeline to handle multiple models, is here.\nThe plot below shows the posterior 90% credible intervals for each rider’s strength, for clarity I’ve filtered the data down to those riders who have competed since the start of 2020.\n\nShow code\nriders <- read_remote_target(\"riders_Women\")\n\nbt1_summ <- read_remote_target(\"bt_summary_bt1_Women\")\n\nbt1_strength_summ <- bt1_summ %>%\n  filter(str_detect(variable, 'alpha')) %>%\n  mutate(rider_id = str_match(variable, '\\\\[(.*)\\\\]')[,2] %>% as.numeric()) %>%\n  left_join(riders) %>%\n  mutate(rider_name = fct_reorder(rider_name,median)) %>%\n  filter(max_date >= ymd(20200101))\n\np_strengths_bt1 <- ggplot(bt1_strength_summ) +\n  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name), color = infreq_palette[\"darkblue\"]) +\n  labs(y=\"\",x=\"Strength, α\")\n\np_strengths_bt1\n\n\n\n\n\n\n\nFrom a first pass the strengths seem to fit what we might expect: for instance the top four riders have all appeared in the Finals of the World Championships in either 2019 or 2020.\nWe can also see riders, such as Nicole Rodriguez, who appear in the evaluation date but not the training data: they have wider uncertainty intervals as the model can only assume they are average in absence of any data.\nThe next question is whether the scale of the strengths looks right. A good test scenario for this is to compare the weakest and strongest riders: suppose those two riders were to compete, what odds would we put on the weaker rider winning?\nThe table below summarises the posterior odds for that extremal match\n\nShow code\nbt1_max_odds <- bt1_summ %>%\n  filter(str_detect(variable, 'delta_max')) %>%\n  select( q5, median, q95) %>%\n  mutate(\n    # calculation is based on the fact that the strength difference is the log odds\n    # on the exponential scale\n    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = \",\"))\n  )\n\nbt1_max_odds %>%\n  kable(col.names = c( \"5%\", \"50%\", \"95%\"), digits = 2) %>%\n  kable_styling(full_width=FALSE)\n\n\n\n5%\n\n\n50%\n\n\n95%\n\n\n1,500\n\n\n12,000\n\n\n211,900\n\n\n\nFigures should be read as 1 in …\nThe model is incredibly sceptical that the weakest rider could beat the strongest: I certainly can’t imagine a bookmaker offering odds on these scales!\nDoes the data genuinely support such high odds, or is this behaviour being encouraged by the somewhat arbitrary choice of prior distribution?\nPrior Predictive Checks\nPrior predictive checks provide results from the model in absence of any data: i.e. using only our prior assumptions. They provide a good test for whether the model is sensibly defined, and in particular whether the prior distributions you’re using are suitable.\nThe plot below shows the prior distribution for the odds that the weakest rider beats the strongest.\n\nA similar example of side effects of default priors on large parameter spaces is given in Gelman et al.’s Bayesian workflows paper, Fig. 3.\n\nShow code\nsigma_draws_halfnormal <- \n  tibble(draw = 1:10000, hyperprior = 'HalfNormal(0,1)') %>% mutate(sigma = abs(rnorm(n())))\n\nprior_draws_halfnormal <- crossing(rider = 1:nrow(riders), sigma_draws_halfnormal) %>%\n  mutate(alpha = rnorm(n(), sd = sigma))\n\nmax_diff_halfnormal <- prior_draws_halfnormal %>%\n  group_by(draw,hyperprior) %>%\n  summarise(\n    alpha_max = max(alpha),\n    alpha_min = min(alpha),\n    max_diff = alpha_max - alpha_min,\n    odds_weaker_wins_log10 = max_diff / log(10)\n  ) %>%\n  ungroup()\n \nggplot(max_diff_halfnormal, aes(odds_weaker_wins_log10)) + geom_histogram(binwidth = 0.1, color = infreq_palette[\"beige\"]) +\n  scale_x_continuous(breaks = seq(0,8,by = 2), labels = scales::math_format(10^.x)) +\n  labs(x = 'Odds Weakest Beats Strongest', y = '')  +\n  theme(axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank())\n\n\n\n\nThe prior spans several orders of magnitude: putting mass on odds from 1-1, up to 1-1,000,000.\nIn hindsight this feels too vague, and the long tail might be promoting the model to tend towards extreme scenarios.\nAn Informative Hyperprior\nMy personal instinct is that the odds should be more in the range of 1-100 up to 1-1,000, so as a first model development I’ll introduce a more informative prior to reflect my personal intuition about the scale of the odds.\nSince the model is defined in terms of strengths I’ll need to convert that range of 1-100 to 1-1,000 odds to a range for the difference in strengths. In practice this conversion is almost immediate as the strength differences are none-other than the (base-\\(e\\)) logarithm of the odds. That’s just the magic of logistic regression!\nSo my preferred odds range of between 1-100 and 1-1,000 translates to a maximum difference in athlete’s strength of between 4.6 and 6.9.\n\nEg. \\(1-10^k\\) odds are equivalent to a strength difference of \\(k/\\log_{10}(e^1) \\sim 0.43k\\).\nI’ve decided to work with a Gamma distribution for the hyperprior, and settled on shape and rate parameters of 80, and 60 respectively: these were chosen by trial and error, to create a prior distribution that nicely spanned the strength range above.\n\n\nσ - Hyperprior\n\nShow code\nsigma_draws_gamma <- tibble(draw = 1:10000, hyperprior = 'Gamma(80,60)') %>% mutate(sigma = rgamma(n(), 80, 60))\n\nsigma_draws <- bind_rows(sigma_draws_gamma, sigma_draws_halfnormal)\n\nggplot(sigma_draws, aes(sigma,fill=hyperprior)) + \n  geom_histogram(aes(y=..density..),binwidth = 0.05, color = infreq_palette[\"beige\"]) +\n  scale_x_continuous(limits = c(0,3)) +\n  facet_grid(rows = vars(hyperprior)) +\n  labs(x = 'σ', y = '') + \n  theme(\n    axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank(),\n          strip.background = element_blank(), strip.text = element_blank(), legend.title = element_blank()\n    )\n\n\n\n\n\n\nPrior on Maximum Strength Difference\n\nShow code\nprior_draws_gamma <- crossing(rider = 1:nrow(riders), sigma_draws_gamma) %>%\n  mutate(alpha = rnorm(n(), sd = sigma))\n\nmax_diff_gamma <- prior_draws_gamma %>%\n  group_by(draw,hyperprior) %>%\n  summarise(\n    alpha_max = max(alpha),\n    alpha_min = min(alpha),\n    max_diff = alpha_max - alpha_min,\n    odds_weaker_wins_log10 = max_diff / log(10)\n  )\n\nmax_diff <- bind_rows(max_diff_gamma, max_diff_halfnormal)\n\n\nggplot(max_diff, aes(max_diff,fill=hyperprior)) +\n  geom_histogram(aes(y=..density..),binwidth = 0.25, color = infreq_palette[\"beige\"]) +\n  scale_x_continuous(limits = c(0,15)) +\n  facet_grid(rows = vars(hyperprior)) +\n  # scale_x_continuous(breaks = seq(0,8,by = 2), labels = paste0(\"10^\",seq(0,8,by = 2) )) +\n  labs(x = 'Max. Strength Difference', y = '') +\n  theme(\n    axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank(),\n          strip.background = element_blank(), strip.text = element_blank(), legend.title = element_blank()\n    )\n\n\n\n\n\n\nThis gives us the minor change to our original model specification:\n\n\nBT2 \\[\n\\begin{align*}\n\\bf{\\text{Priors}}\\\\\n\\sigma & \\sim \\text{Gamma}(80,60) \\\\\n\\alpha_r & \\sim \\text{Normal}(0,\\sigma) \\\\\n\\\\\n\\bf{\\text{Likelihood}}\\\\\n\\alpha_r - \\alpha_s | W_{r,s} &\\sim \\text{logit}^{-1}(\\alpha_r - \\alpha_s)\n\\end{align*}\n\\]\n\n\nExample Data\n\nShow code\nmatches_sample <-  matches %>%\n  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%\n  mutate(across(everything(), as.character)) %>%\n  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id) %>%\n  slice(1)\n\nmatches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))\nmatches_sample %>%\n  kable(\"pipe\") %>%\n  kable_styling(full_width=FALSE, bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),font_size = 10)\n\n\nField\nExample\nevent\n2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS\ndate\n2020-02-28\ngender\nWomen\nround\nFinals\nrider_1\nHINZE EMMA\nrider_2\nVOINOVA ANASTASIIA\nrider_id_1\n44\nrider_id_2\n123\nwinner_id\n44\nloser_id\n123\n\n\n\nStan code\n\n\nfunctions {\n  \n  real accuracy(vector delta, int start, int end){\n      vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1);\n      real acc = 0;\n      \n      for(n in 1:num_elements(delta_seg)) acc += (delta_seg[n] > 0);\n\n      return inv(num_elements(delta_seg)) * acc;\n  }\n  \n  real log_loss(vector delta, int start, int end){\n      vector[end - start + 1] delta_seg = segment(delta, start, end - start + 1); \n      real ll = 0;\n      \n      return inv(num_elements(delta_seg)) * bernoulli_logit_lpmf(1 | delta_seg);\n  }\n}\n\ndata {\n  // Dimensions\n  int<lower=0> R; // Riders\n  int<lower=0> M; // Matches\n  int<lower=0,upper = M> T; // Training matches\n\n  // Match results\n  int<lower=1,upper=R> winner_id[M]; // ID's specifying riders in match\n  int<lower=1,upper=R> loser_id[M];\n}\n\nparameters {\n  // rider ratings\n  real<lower=0> sigma;\n  vector[R] alpha0;\n}\n\ntransformed parameters {\n  // difference of winner and loser rating\n  vector[M] delta =alpha0[winner_id] - alpha0[loser_id];\n}\n\nmodel {\n  // (hierarchical) priors - strength\n  sigma ~ gamma(80,60);\n  alpha0 ~ normal(0,sigma);\n  \n  // likelihood\n  1 ~ bernoulli_logit(head(delta, T));\n}\ngenerated quantities {\n  // maximum theoretical strength difference between two riders\n  real<lower=0> delta_max = max(alpha0) - min(alpha0);\n  \n  // Calculate log-loss, match log-loss and accuracy. A separate value is returned\n  // for training/evaluation data, and within each of these the metrics are further\n  // broken down by round (5 rounds in total) and the total (6th entry in vectors).\n  real training_accuracy;\n  real evaluation_accuracy;\n  real training_log_loss;\n  real evaluation_log_loss;\n  \n  training_accuracy = accuracy(delta,1, T);\n  training_log_loss = log_loss(delta, 1,T);\n  evaluation_accuracy = accuracy(delta, T+1, M);\n  evaluation_log_loss = log_loss(delta, T+1, M);\n}\n\n\n\nBT2. Basic Bradley-Terry model, with Gamma hyperprior.\nVisually the plot of posterior strengths looks pretty similar under the revised model to the original, so I’ll skip presenting it.\nHowever, the change in prior does have a material impact on the maximum odds statistic, this is summarised in the table below which shows both the prior and posterior ranges for this statistic, for the two models.\n\nShow code\nbt2_summ <- read_remote_target(\"bt_summary_bt2_Women\")\n\nbt2_max_odds <- bt2_summ %>%\n  filter(str_detect(variable, 'delta_max')) %>%\n  select(q5, median, q95) %>%\n  mutate(\n    # calculation is based on the fact that the strength difference is the log odds\n    # on the exponential scale\n    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = \",\"))\n  )\n\nprior_max_odds <- max_diff %>%\n  mutate(model = if_else(hyperprior == 'Gamma(80,60)', 'bt2 - Prior', 'bt1 - Prior')) %>%\n  group_by(model) %>%\n  summarise(\n    q5 = quantile(10^odds_weaker_wins_log10, 0.05) %>% round(-1) %>% format(big.mark = \",\"),\n    median = quantile(10^odds_weaker_wins_log10,0.5) %>% round(-1) %>% format(big.mark = \",\"),\n    q95 = quantile(10^odds_weaker_wins_log10, 0.95) %>% round(-1) %>% format(big.mark = \",\")\n  )\n\nbind_rows(\n  bt1_max_odds %>% add_column(model = 'bt1 - Posterior', .before = 0),\n  bt2_max_odds %>% add_column(model = 'bt2 - Posterior', .before = 0),\n  prior_max_odds\n)  %>%\nmutate(\n  model = factor(model, levels = c(\"bt1 - Prior\", \"bt1 - Posterior\", \"bt2 - Prior\", \"bt2 - Posterior\"))\n)%>%\n  arrange(model) %>%\n  kable(col.names = c(\"\", \"5%\", \"50%\", \"95%\"), digits = 2) %>%\n  kable_styling(full_width=FALSE)\n\n\n\n\n\n5%\n\n\n50%\n\n\n95%\n\n\nbt1 - Prior\n\n\n0\n\n\n40\n\n\n32,950\n\n\nbt1 - Posterior\n\n\n1,500\n\n\n12,000\n\n\n211,900\n\n\nbt2 - Prior\n\n\n190\n\n\n940\n\n\n7,090\n\n\nbt2 - Posterior\n\n\n900\n\n\n3,800\n\n\n27,600\n\n\n\nFigures should be read as 1 in …\nIn both cases we can see that the posterior distributions are quite different from the priors. In the case of BT2 we can see that the extreme behaviour allowed by the naive prior choice has been reined in (even then its still much higher than my prior thought).\nEvaluation Metrics\nWith two models to hand, its time to consider how we can choose between the two; I’ll consider two formal evaluation metircs. Model accuracy answers the simple question If you always bet on the stronger athlete, how often would you win?. The log loss gives a more nuanced measure: penalising predictions when the model put a very low probability on the actual observed outcome.\n\nMy accuracy measure implicitly assumes a cutoff of 0.5; using a different cut-off is important when there is class imbalance, but our data is naturally balanced as every match has 1 winner and 1 loser.\n\\[\n\\begin{align}\n\\text{Accuracy} & = \\text{Prop. of matches where the modelled stronger rider wins.} \\\\\n\\text{Log Loss} & = \\text{Average log-probability assigned to the observed outcome.} \\\\\n\\end{align}\n\\]\nThe definition of the log loss is equivalent to the log likelihood of the data, but divided by the number of data points so that we can more readily compare training/test performance.\n\nLog loss is normally multiplied by -1 to be positive: I avoid this convention as this way better models will\nAs a benchmark for model performance we can compare to the model in which we randomly guess the winner. On average this algorithm would have an accuracy of \\(\\frac12\\) (as we’d expect half of its guesses to be correct), and the log loss is fixed at \\(\\log(\\frac12) \\approx -0.693\\).\nModels with better predictive power will have higher accuracy (closer to 1), and higher log loss (closer to 0).\n\n\nAccuracy\n\nShow code\nbt1_measures <- bt1_summ %>%\n  filter(str_detect(variable,'(accuracy|log_loss)')) %>%\n  extract(variable, into = c(\"split\", \"measure\"), \"(.*?)_(.*)\") %>%\n  mutate(\n    model = 'bt1',\n    model_split = paste(model, \"-\", split)\n  )\n\nbt2_measures <- bt2_summ %>%\n  filter(str_detect(variable,'(accuracy|log_loss)')) %>%\n  extract(variable, into = c(\"split\", \"measure\"), \"(.*?)_(.*)\") %>%\n  mutate(\n    model = 'bt2',\n    model_split = paste(model, \"-\", split)\n  )\n\nmeasures <- bind_rows(bt1_measures, bt2_measures)\n\n\n\n\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n0.648\n0.716\n0.773\nbt2\nevaluation\n0.648\n0.716\n0.773\nbt1\ntraining\n0.770\n0.791\n0.810\nbt2\ntraining\n0.764\n0.787\n0.807\n\n\n\n\n\n\nLog Loss\n\nShow code\nlog_loss <- measures %>%\n  filter(measure == \"log_loss\")\n\nlog_loss %>% \n  select(model, split,  q5, median, q95) %>%\n  arrange(split, model) %>%\n  kable(\"pipe\", col.names =c(\"Model\", \"Split\", \"5%\", \"50% (Median)\", \"95%\"), digits =3)\n\n\nModel\nSplit\n5%\n50% (Median)\n95%\nbt1\nevaluation\n-0.780\n-0.638\n-0.536\nbt2\nevaluation\n-0.731\n-0.619\n-0.532\nbt1\ntraining\n-0.467\n-0.440\n-0.417\nbt2\ntraining\n-0.474\n-0.450\n-0.429\n\n\n\n\n\n\nBoth models have accuracies that are well above the benchmark, indicating that using either model is better than random guessing, though there’s little distinction between the two models. This is to be expected as the accuracy will only change if the two models differ in their opinion about who the stronger rider is in each match, and we wouldn’t expect that to happen in many cases.\nThe log loss is more interesting: on the training data the log loss is better than guessing, but this isn’t the case in the evaluation data. Amongst the evaluation data, there’s some weak evidence that the Gamma prior (bt2) fairs marginally better.\nThis poor performance in log loss is driven by upsets: where the model is highly confident in one athlete winning, and then they don’t. For instance suppose a model thinks that a given athlete will win with probability 0.999, if they then fail to win that contributes a factor of \\(\\log(1 - 0.999) \\approx -6.908\\) to the log loss, as opposed to if they win in which case the contribution is \\(-0.001\\).\nThe Gamma prior fairs better as it was designed to lead the model away from scenarios where it offers astronomical odds.\nNext Steps\nIn this post I’ve introduced the basic Bradley-Terry model, and shown that it out performs a strategy of random guessing.\nIn the next post I’ll start to deviate from the standard model, accounting for features available in the data that should help to improve the predictive power of the model.\nComments\nAcknowledgments\nPrevious implementations of Bradley-Terry models using Stan have been posted by Bob Carpenter and opisthokonta.net (Jonas).\nThis post makes heavy use of R and Stan, and so benefits from the many people who contribute to their development.\n\n\n\n",
    "preview": "posts/2021-07-23-tokyo-2020-i/img/strengths_wc20.png",
    "last_modified": "2021-08-06T18:50:15+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-09-bromans-socks/",
    "title": "A Fully Bayesian Analysis of Broman's Socks",
    "description": "When Karl Broman tweeted about his laundry he likely didn't imagine that people would still be estimating how many socks he washed 7 years later. In this post my willingness to derive some exact formulae will enable a fully Bayesian, sampling free, approach to laundry quantification.",
    "author": [],
    "date": "2021-07-09",
    "categories": [
      "Bayesian",
      "Discrete Probability"
    ],
    "contents": "\n\nContents\nAssumptions\nBååth’s Model\nFrom Prior to Posterior\nStopping Time Model\nComments\nAcknowledgments\nComputational Considerations\nBååth’s Prior\nUtility Functions\n\n\n\n\n\nThat the 1st 11 socks in the laundry are each distinct suggests there are a lot more socks. pic.twitter.com/EGSo9P6rw7— Karl Broman (@kwbroman) October 17, 2014\n\n\nI belatedly found my way to the puzzle of estimating exactly how many socks Karl Broman washed through Rasmus Bååth’s excellent blog post, which uses the problem to illustrate Approximate Bayesian Computation.\nRasmus wraps-up the post by presenting three potential criticisms of his analysis, of which one is Why use approximate Bayesian computation at all? I’ll take up his challenge and derive an explicit formula for the likelihood function, enabling a Bayesian analysis without the need for sampling methods.\nI’ll also propose an alternative model to take into account my personal belief that the Tweet was sent in the knowledge that the twelfth sock was going to break the run of distinct socks!\nAssumptions\nThis post makes some assumptions about you: that you have some experience with (or a willingness to learn) statistics and discrete probability.\nI’ll assume that you’re familiar twith the concepts of Bayesian statistics: a model (the likelihood) describes our understanding of how some data is generated, it can be combined with initial assumptions about plausible parameter values (prior distributions) and combining these with observed data leads to refined assumptions (posterior distributions).\nTo follow the derivation of the likelihood you’ll need to know some common constructs from discrete probability/combinatorics: e.g. understanding of binomial coefficients, and how these relate to counting problems.\nBååth’s Model\nOur aim is to estimate the total number of socks that Karl Broman washed given his Tweet that the first 11 removed from the washing machine were distinct.\nI’ll use the following notation throughout:\n\nI use the convention that data is denoted by letters from the Roman alphabet, whilst unknown parameters use the Greek alphabet.\n\\[\n\\begin{align*} \\bf{\\text{Data}}\\\\\nd & = \\text{No. successive distinct socks observed before Tweeting}\\\\\n\\\\\n\\bf{\\text{Parameters}}\\\\\n\\rho & = \\text{No. pairs of socks in the wash}\\\\\n\\sigma & = \\text{No. singleton socks in the wash} \\\\\n\\end{align*}\n\\]\nIn the case of the Tweet, \\(d = 11\\). Using two parameters \\(\\rho\\) and \\(\\sigma\\) allows us to handle the scenario that whilst most socks come in pairs, in some households stray singleton socks are not uncommon. I will however assume socks don’t come in multiples of more than two, rulling out the not-uncommon scenario in which two pairs of identical socks are washed.\nThe likelihood, \\(L(d|\\rho,\\sigma)\\), describes how the unknown parameters generate the obsered data: Assuming there were \\(\\rho\\) pairs of socks and \\(\\sigma\\) singletons how likely is it that the first \\(d\\) socks are all distinct?\nAs a warm up for deriving a complete formula, I’ll consider some of the edge cases which have logical heuristics:\nScenario\nHeuristic\nLikelihood \\(L(d|\\rho,\\sigma)\\)\n\\(\\rho, \\, \\sigma < 0\\)\nLet’s not be silly: you can’t have negative socks.\n0\n\\(d > 2\\rho + \\sigma\\)\nWe can’t observe more socks than were washed.\n0\n\\(d > \\rho + \\sigma\\)\nWe can’t observe more distinct socks than the number of distinct socks that were washed.\n0\n\\(\\rho = 0, \\, \\sigma \\geq d\\)\nIf all the socks were different, then of course all the observed socks are different.\n1\nWith the edge cases handled, I’ll press on and handle the substantive problem.\n\n\nLikelihood \\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\n\n\nExamples \\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\nAs a soft check that the formula above is correct, let’s take a look at some specific cases.\nExample: \\(\\bf{\\rho = 1, \\, \\sigma = 1, \\,d = 2}\\).\nThis is the smallest non-trivial scenario, and we can check this easily by hand. Denoting the socks by {S,P1,P2}, there are three ways to choose two of them: {S,P1}, {S,P2} and {P1,P2}. In two of the scenarios the socks are distinct, so the likelihood is 2/3.\nPlugging the parameters/data into the formula above: \\[\n\\begin{align}\nL(2,1|2) & =   \\binom{3}{2}^{-1} \\left\\{ 2^2 \\binom{1}{0}\\binom{1}{2} + 2^1 \\binom{1}{1}\\binom{1}{1}\\right\\} = 3^{-1}\\left(0 + 2\\right)  = \\frac23\n\\end{align}\n\\]\nExample: \\(\\bf{\\rho=3,\\,\\sigma=4, \\, d = 4}\\)\nWhilst this example doesn’t sound much more complex, crunching numbers directly would be pretty tedious (admittedly, tricky) as the denominator of the likelihood formula suggests there are 210 scenarios to check.\nEvaluating the formula for these parameters indicates the likelihood is 129/210 ~ 0.614.\nTo validate this claim, I’ll simulate drawing 4 socks from 3 pairs and 4 singletons, and calculate the proportion of these simulations that return distinct socks.\n\nShow code\nset.seed(1414214)\n\nrho <- 3\nsigma <- 4\nd <- 4\n\n# vector of all the socks\nall_socks <- c(rep(paste0(\"P\",1:rho), 2), paste0(\"S\", 1:sigma))\n\n# a function to sample d socks without replacement from all_socks, and return\n# 1 if all socks are distinct, and 0 otherwise\nsample_socks <- function(all_socks, d){\n  sock_sample <- sample(x = all_socks, size = d, replace = FALSE)\n  return( 1 * (length(sock_sample) == length(unique(sock_sample))) )\n}\n\n# draw samples \ndraws <- tibble(draw = map_dbl(1:1e05, ~sample_socks(all_socks, d)))\n\n\n\n\nSummary\n\nNo. Draws\n100,000\nNo. All Distinct\n61,360\nProb. All Distinct\n0.614\n\n\n\nProof\n\\[\nL(\\rho,\\sigma|d) = \n\\binom{2\\rho + \\sigma}{d}^{-1} \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\n\\]\nProof\nIn words, the likelihood is given by the following fraction\n\\[\n\\frac{\\text{No. ways to choose d }{\\bf{distinct}}\\text{ socks from $\\rho$ pairs and $\\sigma$ singletons.}}{\\text{No. ways to choose d socks from  $\\rho$ pairs and $\\sigma$ singletons.}} \n\\]\nStarting with the denominator, this is none other than the total number of ways to choose \\(d\\) objects without replacement from a total of \\(2 \\rho + \\sigma\\) (the factor of two is because \\(\\rho\\) pairs of socks equates to \\(2 \\rho\\) individual socks). That is given by the binomial coefficient \\(\\binom{2\\rho + \\sigma}{d}\\), and explains the leading term in the likelihood formula above.\nTurning to the numerator, I’ll break this down by conditioning on the number of singleton socks. counting:\n\\[\\text{No. of ways to choose $d$ distinct socks, given that $j$ of them are singletons.}\\]\nStarting with the sigletons, there are \\(\\binom{\\sigma}{j}\\) ways to choose exactly \\(j\\) of these. The remaining \\(d-j\\) socks need to come from the pairs, there are \\(\\rho\\) distinct socks that form \\(2\\rho\\) pairs, so there are \\(\\binom{\\rho}{d-j}\\) ways to choose the type of socks. But then for each of these \\(d-j\\) socks we need an additional factor of 2 as we could have chosen between two (left, and right) socks. Bringing this together, we have:\n\\[2^{d-j}\\binom{\\rho}{d-j}\\binom{\\sigma}{j}.\\]\nThe full formula for numerator, and then the likelihood, follows by summing over the possible values of \\(j = 0,\\ldots,\\sigma\\).\nFor a similar proof, I previously posted an answer to a question on Cross Validated with a slightly different sock related problem.\n\n\nIn practice when evaluating the likelihood (and later the posterior distribution) there are some computational tricks we employ to avoid running into problems of integer overflow; these are detailed in the end-notes.\nIn the plot below we visualise the likelihood for the case of interest, \\(d = 11\\).\n\nShow code\n# log likelihood for Baath's model\nbaath_log_likelihood <- function(rho, sigma, d){\n  \n  # it is not possible to choose more than p+s distinct socks\n  if(d > rho + sigma) return(-Inf)\n  \n  # log summation terms, for the log-sum-exp trick.\n  log_summation_terms <- purrr::map(0:min(d, sigma), function(j){\n    (d-j)*log(2) + lchoose(sigma,j) + lchoose(rho,d-j)\n  })\n  \n  log_likelihood <- -lchoose(2*rho + sigma,d) +  logSumExp(log_summation_terms)\n  \n  return(log_likelihood)\n}\n\n# compute grid of likelihood values, fix d = 11\n# calculate_sock_grid defined in the Utility Functions appendix\nbaath_likelihood_grid <- \n  calculate_sock_grid(\n    rho_max = 30, sigma_max = 20,\n    log_likelihood = function(rho,sigma){baath_log_likelihood(rho,sigma,d=11)}\n  )\n\n# plot likelihood grid\n# plot_sock_grid defined in the Utility Functions appendix\nplot_sock_grid(baath_likelihood_grid, var = likelihood)\n\n\n\n\n\nBååth’s Likelihood\nThe plot demonstrates why a frequentist, maximum likelihood estimate (MLE) approach to solving this problem is guaranteed to give unsatisfactory results: insisting that the most likely scenario is that all of the socks in the wash were unique, and being non-committal about how many there were in total (the likelihood is maximised simultaneously for all scenarios where \\(\\sigma \\geq 11\\), and \\(\\rho = 0\\)).\nEven introducing an assumption that there’s at least one pair is unsatisfactory: in this case the maximum is never achieved as the likelihood increases as \\(\\rho,\\sigma \\rightarrow \\infty\\).\nFrom Prior to Posterior\nTo avoid getting stuck in trivial edge scenarios, I’ll introduce a prior distribution over \\((\\rho,\\sigma)\\), and conduct a Bayesian Analysis. The prior captures our beliefs about the number of socks in the washing machine, in absence of any data.\nI’ll put separate priors on the number of singleton and pairs of socks, and then add a restriction that the total socks can’t exceed a fixed (large) amount. This differs from Rasmus’ approach, but produces formulae that are easier to work with: since Rasmus was using sampling, complexity in the formulae wasn’t an issue.\n\nIt is possible to derive a formula for Rasmus’ prior - but its messy! For the really invested, see the footnotes.\nCapping the maximum number of socks has a natural interpretation as we know there are limits on how many socks could fit in a domestic washing machine. More importantly the cap is essential for computing the normalising constant that crops up when using Bayes rule. To dig into that in more detail, I’ll denote \\(p(\\rho,\\sigma)\\) for the prior distribution, \\(p(\\rho, \\sigma) |d)\\) for the posterior distribution, and \\(L(d|\\rho,\\sigma)\\) the likelihdood as before, then Bayes rule gives\n\\[p(\\rho,\\sigma | d) = Z^{-1}L(d | \\rho, \\sigma) p(\\rho,\\sigma), \\qquad \\text{where } Z = \\sum_{\\rho,\\sigma} L(d | \\rho, \\sigma) p(\\rho,\\sigma).\\]\nFor arbitrary choices of priors, we cannot expect to be able to derive a formula for \\(Z\\) directly - and thefore we need to evaluate it computationally. By restricting \\(\\rho, \\,\\sigma\\) to a finite range we can calculate \\(Z\\) without having to make estimates of the tail behaviour.\nThe prior distribution on \\(\\rho\\) and \\(\\sigma\\) should be a discrete distribution, on the positive integers. Like Rasmus I’ll use Negative Binomial distributions, which are a flexible generalisation of the Poisson distribution, allowing us to separately control the mean and variance.\n\n\n\nI’ll choose parameters for the Negative Binomial distributions so that the marginal distributions of the product prior closely match those of Rasmus’ prior, which he derived based on assumptions about the plausible amount of washing a family might produce. This leads to me using the priors:\n\nParameters were found using the Method of Moments and then rounded to give nice values\n\\[\\rho \\sim \\text{NegBinom}\\left(3 \\frac{1}{4}, \\frac{1}{5}\\right), \\qquad \\sigma \\sim \\text{NegBinom}\\left(2, \\frac{1}{3}\\right),\\]\nI’ll cap the maximum number of socks that could plausibly have been washed at 300; to justify this: Googling the average weight of a sock returns a range of estimates, but low estimates said a sock weighs 50g. 300 socks would therefore come in at 15kg, which is the maximum drum size of any domestic washing machine I could find on Amazon!\nWriting out the prior explicitly gives:\n\\[p(\\rho,\\sigma) = \\textstyle \\mathbf{\\large 1}_{2\\rho + \\sigma \\leq 300} \\, \\times \\, \\binom{\\rho + 2\\frac{1}{4}}{\\rho} \\left(1 - \\frac{1}{5}\\right)^{3 \\frac{1}{4}} \\left(\\frac{1}{5}\\right)^{\\rho} \\,\\,\\times\\,\\, \\binom{\\sigma + 1}{\\sigma} \\left(1 - \\frac{1}{3}\\right)^{2} \\left(\\frac{1}{3}\\right)^{\\sigma}\\]\nThe plots/table below compare samples from the product prior and the prior Rasmus uses.\n\n\n\n\n\nMarginals\n\n\n\n\nMeasure\nPrior\n10%\n25%\n50%\n75%\n90%\nTotal socks\nBååth\n13\n19\n28\n38\n50\n\nProduct\n12\n18\n27\n39\n52\nPairs of socks\nBååth\n5\n8\n12\n17\n22\n\nProduct\n4\n7\n12\n17\n24\nSingleton socks\nBååth\n1\n2\n3\n5\n8\n\nProduct\n0\n1\n3\n6\n9\nProportion single\nBååth\n0.03\n0.06\n0.1\n0.16\n0.22\n\nProduct\n0\n0.05\n0.12\n0.23\n0.37\n\n\n\nDensity\n\n\n\n\n\n\nComparison of Bååth’s Prior and the Product Prior.\nThe marginal distributions align closely, with the exception that the product prior has larger tails for the proportion of singleton socks. This is visible in the (empirical) density plot where Rasmus’ prior clearly has a sharper peak, and is less dispersed in the \\(\\sigma\\)-axis.\nThe density plot also shows that the product prior is smoother over the parameter space - this is to be expected as defining Rasmus’ prior requires a rounding/floor calculation (which is itself not smooth) to separate the total socks into pairs/singletons.\nFinally I will go ahead and derive the posterior using Bayes rule. The animation below shows how the posterior distribution changes with the observation of each additional distinct sock, up to \\(d=11\\).\n\nShow code\n# define the log product prior\nlog_product_prior <- function(rho, sigma, r_rho, p_rho, r_sigma, p_sigma, cap){\n\n  if(2*rho + sigma > cap){\n    log_prior <- -Inf\n  }\n  else{\n    log_prior <- dnbinom(rho, size = r_rho, prob = p_rho, log = TRUE) +\n      dnbinom(sigma, size = r_sigma, prob = p_sigma, log = TRUE)\n  }\n  \n  return(log_prior)\n}\n\n# calculate posterior grid for each of d = 0,...,11 for animation\niterated_posterior_grid <- map(0:11, .f =function(d){\n  \n  if(d == 0){\n    posterior_grid <- calculate_sock_grid(150, 300,\n        log_likelihood = NULL,\n        log_prior = function(rho,sigma){\n          log_product_prior(rho,sigma, r_rho, p_rho, r_sigma, p_sigma, cap = 300)\n          }\n      ) %>%\n      filter(2*rho+sigma <= 300) %>%\n      mutate(posterior =prior) %>%\n      add_column(d = d, .before = 0)\n  } else {\n    posterior_grid <- calculate_sock_grid(100, 100,\n        log_likelihood = function(rho,sigma){baath_log_likelihood(rho,sigma,d=d)},\n        log_prior = function(rho,sigma){\n          log_product_prior(rho,sigma, r_rho, p_rho, r_sigma, p_sigma, cap = 300)\n          }\n      ) %>%\n      filter(2*rho+sigma <= 300) %>%\n      add_column(d = d, .before = 0)\n  }\n  \n  return(posterior_grid)\n}) %>% bind_rows()\n\n# actual posterior grid\nposterior_grid <- iterated_posterior_grid %>% filter(d==11)\n\n\n\n\n\nTotal Socks\n\n\n\n\n\n\n\n\nPosterior Grid\n\n\n\n\n\n\n\nMode\n10%\n25%\n50%\n75%\n90%\nPrior\n22\n11\n17\n27\n38\n52\nBååth’s Posterior\n38\n27\n34\n44\n56\n69\n\nFor completeness I’ll replicate Rasmus’ ABC methodology (with his original prior), to comparethe posterior distributions:\n\n\nMode\n10%\n25%\n50%\n75%\n90%\nABC Posterior\n35\n29\n35\n43\n54\n65\n\nThe posterior mode (or Maximum a posteriori estimate, MAP) is the most likely number of socks under the posterior distribution, which we find to be 38 socks, with a 90% credible interval of [27, 69].\nFortunately for us there is no mystery to the true answer of the number of socks that were washed, which Karl Broman confirmed to be 45, which is consistent with the posterior estimates:\n\n@rabaath @sgrifter There were 21 pairs and 3 singletons. Will spend the rest of the evening working out what my est would have been.— Karl Broman (@kwbroman) October 17, 2014\n\n\nStopping Time Model\nRasmus’ model assumes that having drawn the eleventh sock, Karl Broman took to Twitter without checking whether or not the twelfth sock was also distinct.\nCall me a sceptic, but I can’t help but think that in fact the twelfth sock was observed, seen to break the run, and the Tweet was made in this knowledge. To finish off this post I’ll derive the likelihood takes into account this additional information, and see how this changes our posterior estimates.\n\nAnother limitation in both Rasmus and my model is publication bias; Thomas Lumley discusses this point in detail in this blog post.\nFor my model rather than working with the data \\(d\\), I’ll introduce\n\\[\n\\begin{align*} \\bf{\\text{Data}}\\\\\nf & = \\text{No. socks prior to the first duplicate}\n\\end{align*}\n\\] The parameters of the model will remain as before \\(\\rho, \\, \\sigma\\). To be clear, in this notation the first \\(f\\) socks are distinct, with sock \\(f+1\\) matching one of the socks already drawn. So in the case of the Tweet, \\(f = 11\\).\n\nWe call this a stopping time as the random variable is defined by a process that we continue until a particular rule is achieved, at which point we stop: in this the rule is stop when a duplicate sock is drawn.\n\n\nLikelihood \\[\nL(\\rho,\\sigma|f) = \n\\left\\{(2\\rho + \\sigma -f)\\binom{2\\rho + \\sigma}{f}\\right\\}^{-1} \\sum_{j=0}^{\\sigma} 2^{f-j} \\binom{\\sigma}{j} \\binom{\\rho}{f-j} \\left(f-j\\right)\n\\]\n\n\nExamples \\[\nL(\\rho,\\sigma|f) = \n\\left\\{(2\\rho + \\sigma -f)\\binom{2\\rho + \\sigma}{f}\\right\\}^{-1} \\sum_{j=0}^{\\sigma} 2^{f-j} \\binom{\\sigma}{j} \\binom{\\rho}{f-j} \\left(f-j\\right)\n\\]\nExample: \\(\\bf{\\rho=3,\\,\\sigma=4, \\, f = 4}\\)\nAs before I will check the formula by comparing it to an estimate derived by sampling.\n\n\n\nFor the parameter choices set out above, the likelihood formula evaluates to give 0.210.\n\nShow code\nset.seed(1414214)\n\nrho <- 3\nsigma <- 4\nf <- 4\n\n# vector of all the socks\nall_socks <- c(rep(paste0(\"P\",1:rho), 2), paste0(\"S\", 1:sigma))\n\n# a function to sample d socks without replacement from all_socks, and return\n# 1 if all socks are distinct, and 0 otherwise\nsample_socks <- function(all_socks, f){\n  # draw one more sock than f, as stopping time is checked at f+1\n  sock_sample <- sample(x = all_socks, size = f + 1, replace = FALSE)\n\n  # is f+1 the stopping time? i.e. the first non-distinct sock\n  stopping_time <-  1 *\n    # first f socks are distinct\n    (length(sock_sample[-1]) == length(unique(sock_sample[-1]))) *\n    # last sock is not distinct\n    (sock_sample[1] %in% sock_sample[-1])\n\n  return(stopping_time)\n}\n\n# draw samples \ndraws <- tibble(draw = map_dbl(1:1e05, ~sample_socks(all_socks, f)))\n\n\n\n\nSummary\n\nNo. Draws\n100,000\nNo. Stopping Time\n20,851\nProb. Stopping Time\n0.209\n\n\n\nProof\n\\[\nL(\\rho,\\sigma|f) = \n\\left\\{(2\\rho + \\sigma -f)\\binom{2\\rho + \\sigma}{f}\\right\\}^{-1} \\sum_{j=0}^{\\sigma} 2^{f-j} \\binom{\\sigma}{j} \\binom{\\rho}{f-j} \\left(f-j\\right)\n\\]\nProof\nAs with my derivation of Bååth’s likelihood, the key will be to condition on the number, \\(j\\), of singleton songs amongst the first \\(f\\) drawn. In particular we’re looking to calculate\n\\[\\text{No. ways to choose $f$ distinct socks, followed by a duplicate,}\\\\ \\text{given that $j$ of the first $f$ are singletons.}\\]\nWe know from the previous proof that the number of ways to choose \\(f\\) distinct socks, with \\(j\\) singletons is\n\\[2^{f-j} \\binom{\\sigma}{j}\\binom{\\rho}{f-j},\\]\nto this we just need to multiply the number of ways that we can pick one further sock, that matches one of the first \\(f\\). Only \\(f-j\\) of the socks chosen so far have a pair (the others being singletons), so therefore there are \\(f-j\\) socks we could pick on the next draw that would be a duplicate. Putting this together gives the summation part of the liklihood.\nIt remains to derive the denominator: in our previous calculation this was the number of ways to pick unordered socks, and was given by a binomial coefficient. This time we want to pick \\(f\\) socks, for which the order does not matter, and then one additional sock that is singled out as the last sock. That last sock can be chosen from amongst the \\(2\\rho + \\sigma - f\\) remaining socks, from which we get the denominator:\n\\[\\binom{2\\rho + \\sigma}{f}(2\\rho + \\sigma - f).\\]\n\n\nRecall that Rasmus’ likelihood preferred configurations that either had only singleton socks, or infinitely many socks. This is not the case for the stopping time model: in particular the requirement that a duplicate is drawn rules out the scenario of all singleton socks, and in the limit \\(\\rho \\rightarrow \\infty\\) the probability of drawing a duplicate becomes negligible.\nThis later observation gives us some intuition that we can expect the posterior distribution under the stopping time distribution to be focused on configurations with fewer socks than Rasmus’ model. This is confirmed in the posterior summaries below.\n\nI’ve used the same product prior as in the previous section.\n\nShow code\n# log likelihood for Baath's model\nst_log_likelihood <- function(rho, sigma, f){\n  \n  # it is not possible to choose more than p+s distinct socks\n  if(f > rho + sigma) return(-Inf)\n  \n  # it is not possible to get a duplicate if all socks are distinct\n  if(rho == 0) return(-Inf)\n  \n  # its not possible to pick more socks than were washed\n  if(f+1 > 2*rho + sigma) return(-Inf)\n  \n  # log summation terms, for the log-sum-exp trick.\n  log_summation_terms <- purrr::map(0:min(f, sigma), function(j){\n    (f-j)*log(2) + lchoose(sigma,j) + lchoose(rho,f-j) + log(f-j)\n  })\n  \n  log_likelihood <- -lchoose(2*rho + sigma,f) - log(2*rho + sigma - f) +  logSumExp(log_summation_terms)\n  \n  return(log_likelihood)\n}\n\nst_posterior_grid <- calculate_sock_grid(100, 100,\n        log_likelihood = function(rho,sigma){st_log_likelihood(rho,sigma,f=11)},\n        log_prior = function(rho,sigma){log_product_prior(rho,sigma, r_rho, p_rho, r_sigma, p_sigma, cap = 300)}\n      ) %>%\n      filter(rho <= 150, sigma <= 300)\n\n\n\n\n\nTotal Socks\n\n\n\n\n\nPosterior Grid\n\n# A tibble: 10,201 x 8\n# Rowwise: \n     rho sigma log_likelihood log_prior log_posterior likelihood\n   <int> <int>          <dbl>     <dbl>         <dbl>      <dbl>\n 1     0     0           -Inf     -7.43          -Inf          0\n 2     0     1           -Inf     -7.14          -Inf          0\n 3     0     2           -Inf     -7.14          -Inf          0\n 4     0     3           -Inf     -7.26          -Inf          0\n 5     0     4           -Inf     -7.44          -Inf          0\n 6     0     5           -Inf     -7.66          -Inf          0\n 7     0     6           -Inf     -7.91          -Inf          0\n 8     0     7           -Inf     -8.19          -Inf          0\n 9     0     8           -Inf     -8.47          -Inf          0\n10     0     9           -Inf     -8.77          -Inf          0\n# … with 10,191 more rows, and 2 more variables: prior <dbl>,\n#   posterior <dbl>\n\n\n\n\n\n\n\n\nMode\n10%\n25%\n50%\n75%\n90%\nPrior\n22\n11\n17\n27\n38\n52\nBååth’s Posterior\n38\n27\n34\n44\n56\n69\nStopping Time Posterior\n34\n24\n30\n38\n49\n61\n\nAlthough the posterior mode is lower for the stopping time model, the true answer of 45 socks is still comfortably within the central 50% credible interval.\nComments\nAcknowledgments\nThis post was inspired by Karl Broman’s tweet, and Rasmus Bååth’s analysis.\nComputation has been done using R.\nComputational Considerations\nIn practice we use a couple of tricks when computing the likelihood function: avoiding the risk of encountering integer overflow when working with large sums of binomial coefficients.\nThe first trick is probably familiar: rather than working with the likelihood, we’ll use the log-likelihood. The log-likelihood for Bååth’s model is:\n\\[l(d|\\rho,\\sigma) = -\\log \\textstyle{\\binom{2\\rho + \\sigma}{d}} + \\log \\bigg(\\textstyle \\sum_{j=0}^{\\sigma} 2^{d-j} \\binom{\\sigma}{j} \\binom{\\rho}{d-j}\\bigg)\\]\nIf we naively go ahead and calculate that sum and then take logs - then we won’t avoid the risk of overflow at all: we’d still calculate a sum of large integers, and taking the logarithm becomes an after thought.\nWhat we really want to do is take the logarithm of the terms inside the summation - which will return values well within computational comfort zone. That is we want to turn a computation of the form \\(\\log \\left( x_1 + \\cdots + x_n\\right)\\) into a calculation in \\(\\log(x_1),\\ldots, \\log(x_n)\\).\nThis is where trick number two comes in: the log-sum-exp trick.\n\n\nLog-Sum-Exp Trick Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\n\n\nExample Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\nExample: \\(\\bf{\\rho = 30, \\, \\sigma = 2, \\,d = 11}\\).\nThis particular set of parameters admits a relatively concise likelihood, which will help us to write the complete formulae, starting with the log-likelihood:\n\\[\nl(11 | 2,30) = -\\log \\textstyle  \\binom{62}{11} + \\log \\bigg(2^{11} \\binom{2}{0}\\binom{30}{11} + 2^{10} \\binom{2}{1}\\binom{30}{10} + 2^{9} \\binom{2}{2}\\binom{30}{9} \\bigg).\n\\]\nIf we do this by brute force, using R to evaluate each of the powers, and binomial coefficients this becomes:\n\n\n\n\\[\n\\begin{align}\n l(11|2,30) & = -\\log 508271323092 \\\\\n & \\qquad + \\log \\big ( \n  111876710400 + 61532190720 + 7325260800\n \\big) \\\\\n & = - 26.95 + 25.92 \\\\\n & = -1.034\n\\end{align}\\]\nAnd back on the natural scale, that implies a likelihood of \\(L(11 |2,30) \\approx 0.36\\).\nThe bit that feels uncomfortable there is the sum of all those +10 digit numbers - and in real cases we could be summing more terms - each with more digits. So now let’s see how that looks using the log-sum-exp trick.\nAs per the statement above, we’ll first calculate the log of each of the terms (and include the denominator term, denoted \\(A\\) here, which we’ll need for the full calculation):\n\n\n\n\\[\n\\begin{align}\n\\textstyle \\log \\binom{62}{11} & = A  \\approx 26.9543 \\\\\n\\textstyle \\log 2^{11} \\binom{2}{0}\\binom{30}{11} &= l_1 \\approx 25.4407 \\\\\n\\textstyle \\log  2^{10} \\binom{2}{1}\\binom{30}{10} &= l_2 \\approx 24.8428 \\\\\n\\textstyle \\log 2^{9} \\binom{2}{2}\\binom{30}{9} &= l_3 \\approx 22.7146 \\\\\n\\end{align}\n\\] The maximum is \\(l^* = l_1\\), and we can go ahead and apply the trick: \\[\n\\begin{align}\nl(11|2,30) & = - A + l^* + \\log \\big ( e^{l_1 - l^*} + e^{l_2 - l^*} + e^{l_3 - l^*}\\big) \\\\\n& = -1.514 + \\log \\big( e^{0} + e^{-0.5978} + e^{-2.726} \\big) \\\\\n& = -1.514 + \\log \\big( 1+ {0.55} + {0.06548} \\big) \\\\\n& = -1.514 + 0.4796 \\\\\n& =  -1.034\n\\end{align}\n\\]\nAs expected we get exactly the same result - with the benefit that the largest value we used in any of the sums was \\(A \\approx 26.95\\).\n\n\nProof Given \\(x_1,\\ldots,x_n > 0\\), let \\(l_k = \\log(x_k)\\) and \\(l^* = \\max_k l_k\\). Then\n\\[\n\\log(x_1 + \\cdots +  x_n) =  l^* + \\log \\bigg ( \\exp(l_1 - l^*) + \\cdots + \\exp(l_n - l^*)\\bigg)\n\\]\nProof\nThe trick itself follows some from some fairly routine manipulation of exponentials. In the below, we have \\(x_k,\\,l_k\\) defined as above, and let \\(A\\) be any constant:\n\\[\n\\begin{align*}\nx_1 + \\cdots + x_n & = \\exp( \\log x_1) + \\cdots + \\exp( \\log x_n) \\\\\n& = \\exp(l_1) + \\cdots + \\exp(l_n) \\\\\n& = \\exp(A) \\bigg(\\exp(l_1 - A) + \\cdots + \\exp (l_n - A) \\bigg)\n\\end{align*}\n\\] Taking logarithms of both sides\n\\[\\log(x_1 + \\cdots + x_n) = A + \\log \\bigg ( \\exp(l_1 - A) + \\cdots + \\exp(l_n - A)\\bigg),\\] and the trick follows by choosing \\(A = l^* = \\max_k l_k\\).\n\n\nImportantly subtracting the maximum guarantees that each of the terms to be exponentiated satisfies \\(l_k - l^* \\leq 0\\), and hence each exponential is bounded above by 1, avoiding any calculations at the limits of machine precision.\nBååth’s Prior\nRasmus defined his prior on \\(\\rho,\\,\\sigma\\) through the following steps:\nSuppose the total number of socks, \\(\\eta\\), follows a negative binomial distribution: \\(\\eta \\sim \\text{NBinom}(\\mu, \\tau)\\).\nSuppose that the proportion of socks that are in pairs, \\(\\theta\\), follows a beta distribution: \\(\\theta \\sim \\text{Beta}(\\alpha,\\beta)\\).\nCalculate the number of pairs of socks: \\(\\rho = \\left[ \\lfloor \\eta/2 \\rfloor \\theta \\right]\\), where \\(\\lfloor \\, \\cdot \\, \\rfloor\\) denotes the floor, and \\(\\left[ \\, \\cdot \\, \\right]\\) denotes rounding.\nCalculate the number of singleton socks: \\(\\sigma = \\eta - 2\\rho\\).\nFor Rasmus this choice doesn’t create an issues, as its perfectly easy to sample from this prior, and hence to conduct ABC. However as we are looking to do exact Bayesian inference, we have to derive the explicit formula for the prior - and that isn’t trivial. It is doable, but leads to a formula that is not particularly intuitive, which was my motivation for working with the product prior.\n\n\nBååth’s Prior Let \\(P_{\\mu,\\tau} \\sim \\text{NBinom}(\\mu,\\,\\tau)\\) and \\(P_{\\alpha,\\beta} \\sim \\text{Beta}(\\alpha,\\beta)\\) then\\[ P(\\rho, \\sigma) = P_{\\mu, \\tau}(2\\rho + \\sigma) \\left\\{ F_{\\alpha,\\beta}\\left( \\frac{2\\rho + 1}{2 \\lfloor \\rho + \\sigma/2\\rfloor}\\right) - F_{\\alpha,\\beta}\\left( \\frac{2\\rho - 1}{2 \\lfloor \\rho + \\sigma/2\\rfloor}\\right) \\right\\}\\] where \\(F_{\\alpha,\\beta}(t) = P_{\\alpha,\\beta}(\\theta < t)\\) is the CDF of the Beta distribution.\n\n\nProof Let \\(P_{\\mu,\\tau} \\sim \\text{NBinom}(\\mu,\\,\\tau)\\) and \\(P_{\\alpha,\\beta} \\sim \\text{Beta}(\\alpha,\\beta)\\) then\\[ P(\\rho, \\sigma) = P_{\\mu, \\tau}(2\\rho + \\sigma)\\left\\{F_{\\alpha,\\beta}\\left( \\frac{\\rho + \\textstyle \\frac12}{\\lfloor \\rho + \\sigma/2 \\rfloor} \\right) - F_{\\alpha,\\beta}\\left( \\frac{\\rho - \\textstyle \\frac12}{\\lfloor \\rho + \\sigma/2 \\rfloor} \\right)  \\right\\}\\] where \\(F_{\\alpha,\\beta}(t) = P_{\\alpha,\\beta}(\\theta < t)\\) is the CDF of the Beta distribution.\nAn application of the law of total probability gives us:\n\\[\n\\begin{align}\nP(\\rho, \\sigma) & = \\sum_{\\eta} P(\\rho,\\sigma | \\eta) P(\\eta) \\\\\n& = P(\\rho, \\sigma | \\eta = 2\\rho + \\sigma) P(\\eta = 2\\rho + \\sigma) \\\\\n& = P_{\\mu,\\tau}(2\\rho + \\sigma) P(\\rho,\\sigma|\\eta = 2\\rho + \\sigma).\n\\end{align}\n\\] The first term is as in the formula above, so now we focus on the conditional term. For this we note\n\\[\n\\begin{align}\nP(\\rho,\\sigma | \\eta) & = P\\big( \\big[ \\lfloor \\eta/2 \\rfloor \\theta \\big] = \\rho\\big) \\\\\n & = P_{\\alpha,\\beta}\\big( \\rho - \\textstyle \\frac12 < \\lfloor \\eta/2 \\rfloor \\theta < \\rho + \\textstyle \\frac12 \\big) \\\\\n & = P_{\\alpha,\\beta}\\left( \\frac{\\rho - \\textstyle \\frac12}{\\lfloor \\eta/2 \\rfloor} <  \\theta < \\frac{\\rho - \\textstyle \\frac12}{\\lfloor \\eta/2 \\rfloor} \\right) \\\\\n & = P_{\\alpha,\\beta}\\left( \\frac{\\rho - \\textstyle \\frac12}{\\lfloor \\rho + \\sigma/2 \\rfloor} <  \\theta < \\frac{\\rho - \\textstyle \\frac12}{\\lfloor  \\rho + \\sigma/2 \\rfloor} \\right) \\\\\n\\end{align}\n\\] The final statement above follows since the CDF satisifes \\(P_{\\alpha,\\beta}(a < \\theta < b) = F_{\\alpha,\\beta}(b) - F_{\\alpha,\\beta}(a)\\).\n\n\nUtility Functions\n\nShow code\n# function to calculate grid of log posterior probabilities.\n#   - if prior_func = NULL returns the likelihood,\n#   - if likelihood_func = NULL returns the prior distribution\ncalculate_sock_grid <- function(rho_max,sigma_max, log_likelihood = NULL, log_prior = NULL){\n    \n  if(is.null(log_likelihood) & is.null(log_prior)){\n    stop(\"At least one of log_likelihood or log_prior must be provided\")\n  }\n\n  # initialise grid\n  grid <- crossing(\n      rho = 0:rho_max,\n      sigma = 0:sigma_max,\n      log_likelihood = NA_real_,\n      log_prior = NA_real_,\n      log_posterior = NA_real_\n    ) %>% rowwise()\n  \n\n  # populate prior/likelihood\n  if(!is.null(log_likelihood)){\n    grid <- grid %>% mutate(log_likelihood = log_likelihood(rho,sigma))\n  }\n  \n  if(!is.null(log_prior)){\n    grid <- grid %>% mutate(log_prior = log_prior(rho,sigma))\n  }\n  \n  # populate posterior, first calculating without the additive constant, Z, then\n  # evaluating this and adding.\n  grid <- grid %>% mutate(prop_log_posterior = log_likelihood + log_prior)\n\n  log_Z <- logSumExp(grid$prop_log_posterior)\n    \n  grid <- grid %>% \n    mutate(log_posterior = -log_Z + prop_log_posterior) %>%\n    select(-prop_log_posterior)\n    \n  # calculate non-log terms\n  grid <- grid %>%\n    mutate(across(starts_with(\"log_\"), ~exp(.), .names = \"exp_{col}\")) %>%\n    rename_with(~str_remove(.,\"exp_log_\"))\n  \n  return(grid)\n}\n\nplot_sock_grid <- function(grid, var){\n  \n  var_name <- rlang::as_name(enquo(var))\n  \n  if(str_detect(var_name, \"log\")){\n    \n    filled_grid <- grid %>% filter(!is.infinite({{var}}))\n    empty_grid <- grid %>% filter(is.infinite({{var}}))\n  } else {\n    filled_grid <- grid %>% filter(({{var}} != 0))\n    empty_grid <- grid %>% filter(({{var}} == 0))\n  }\n  \n  p <- ggplot() +\n    geom_point(data =filled_grid,aes(x = rho, y = sigma, color ={{var}}), size =4) +\n    geom_point(data = empty_grid, aes(x = rho, y = sigma, ), color = \"grey\", shape = 1, size =4) +\n    scale_x_continuous(breaks = seq(0,max(grid$rho),by=10)) +\n    scale_y_continuous(breaks = seq(0,max(grid$sigma),by=10)) +\n    scale_color_gradientn(\n      colours = wesanderson::wes_palette(\"Zissou1\", 1000, type = \"continuous\"),\n      name = \"\") +\n    coord_fixed() + \n    labs(x = \"ρ\", y = \"σ\") +\n    theme(\n      axis.title.y = element_text(angle = 0, vjust = 0.5),\n      axis.line = element_blank(), legend.key.width = unit(1.4, \"cm\")\n    )\n \n return(p)\n}\n\n\n\n\n\n\n",
    "preview": "posts/2021-07-09-bromans-socks/img/socks-posterior.png",
    "last_modified": "2021-07-12T18:47:12+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-25-tdf-shape-21/",
    "title": "The Changing Shape of the Tour de France",
    "description": "On the eve of the Grand Départ of the 2021 Tour de France, we'll use animation to quickly draw some insights from the race's history.",
    "author": [],
    "date": "2021-06-25",
    "categories": [
      "Visualisation",
      "Cycling"
    ],
    "contents": "\n\nContents\nMaillot Jaune\nLanterne Rouge\nR Code\nComments\nAcknowledgments\n\n\n\n\n\nTour de France stage Start/Finishes, 1903-2021.\nEven if you’ve never tuned in to watch a stage of the Tour de France, likely you’re familiar with the symbolic Yellow Jersey, aware of the super-human strength required to cycle thousands of kilometers over the course of a month, and the unfortunate lengths people will go to to acquire that strength.\nWhilst these symbols have been a part of the race since the early days, the route over which the drama plays out is ever changing. I created the animation above to get a better understanding of the shape of the Tour de France.\n\nThe Yellow Jersey was added in 1919, but cheating was an issue from the second edition: the four top placed riders were all later disqualified.\nIn the sections below I’ll summarise what I like about the visualisation and what I think could be improved.\nMaillot Jaune\nThe Maillot Jaune, Yellow Jersey, is famously worn by the rider in the lead of the race at the start of each stage, and finally awarded to the overall fastest rider when the race concludes in Paris.\nThe highlight of the visualisation for me is that it provides a compelling example of the benefits of animation for data story telling: enabling me to draw insights about the race that I’d have struggled to do from tables and static maps alone.\nHere’s a few points that I found interesting.\nThe early editions of the Tour had very few stages\nThe first and second editions of the Tour comprised just 6 stages, compared to today’s 21. Further digging into the data indicates the stage distances were however significantly longer than modern editions.\n\nThe longest stage in the 1903 Tour spanned 471km, compared to a maximum of 250km in 2021.\nA tour of the perimeter?\nThe Tour used to be largely about traversing the outer regions of France: between 1905 and 1938 the only detour away from the boarders seems to be the requisite annual visit to Paris.\nVariation was guaranteed for the riders every few years though, when the race organisers would change their preference for a clockwise or anti-clockwise route.\nThe Missing Years\nThe lack of animation in the periods 1915-1918 and 1940-46 are a reminder of the impact that the First and Second World Wars had on the routines and attractions that we take for granted.\nFurther reading led me to the interesting fact that a 1940 Tour was planned, with the intent that riders would be drafted from soldiers stationed in France.\n\nThere’s a lot more information on the Wikipedia page Tour de France during World War II.\nLate to the Party\nEven into the 1990s its possible to pick out by eye some département that were yet to host the start or finish of a Tour stage. Hosting a tour stage costs and it could well be that these areas of the country had different ideas on how local taxes should be spent.\nPierre Breteau created an app for Le Monde that allows you to explore the frequency that the Tour has passed through each department. With the exception of Corsica (that was finally visited in 2013) and overseas territories, his data indicates that the final department to be visited was Indre in 1992.\nLanterne Rouge\nThe Lanterne Rouge, Red Lantern, is the dubious award given to the last placed rider in the Tour. Somewhat ironically its a title riders fight for as it comes with an offer to ride (and hence appearance fees) in the post-Tour Criterium races.\nThe animation is far from perfect, and could do with some tweaks and improvements - some of which I’ve listed below. Maybe I’ll get around to improving these ahead of Copenhagen 2022!\nStraight Lines, and Pedalo Races\nEvident from the very first frame of the animation is the simplification to straight line paths between stage start and end points. This is particularly stark when the route follows the coast, occasionally suggesting that the riders took to the water to complete the stage.\nA search for Tour de France shapefiles suggests that these don’t exist for public consumption, but detailed road books are published for recent editions of the race - so in theory perhaps more accurate paths could be traced from this data.\nTour de… Belgique?\nWhilst it took until 1992 for the Tour to visit every mainland department, the race first left France in 1947 with detours to both Brussels, and Luxembourg City.\nSince then the tour has made regular detours to nearby countries: for now I’ve removed these overseas excursions for want of a suitable way to incorporate them into the visualisation - but this feels like an easy fix for the future.\n5 of the last 10 Grand Departs have taken place elsewhere in Europe.\nImplausible Locations\nI’ve used geocoding to infer longitude/latitude coordinates from the start/finish location names, using the R tidygeocoder package. This isn’t always accurate, returning for instance Cherbourg, Australia instead of Cherbourg, France, or suggesting the 2010 Tour started at Rue de Rotterdam, Tours rather than Rotterdam in the Netherlands.\nThis leads to some pretty obvious errors in the animation, with implausible distances covered in a stage. An immediate data quality check that could be built in is to calculate the straight line distance between start/end points of a stage and flag if this is greater than the stage distance.\nR Code\n\n\n\n\nShow code\nlibrary(infreqthemes)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(gganimate)\nlibrary(tdfData) # install.github(\"odaniel1/tdfData\")\n\n## ---- prepare stage data --------------------------------------------------------------\n\n# TdF stage data was scraped from Wikipedia; it has some limitations (discussed above).\n# Its available in the tdfData package from my GitHub account\ntdf_stages <-tdf_stages %>% clean_names()\n\n# the TdF did not take place during WWI and WWII; create 'empty' stages so that\n# the animation marks these years.\nmissing_years <- crossing(year = c(1915:1918, 1940:1946), stage = as.character(1:10)) %>%\n  mutate(date = as.Date(paste0(year, \"-07-\",stage), '%Y-%m-%d'))\n\n# add missing years to the data; the stage_no column defines the order for the animation.\ntdf_stages <- bind_rows(tdf_stages, missing_years) %>%\n  arrange(date) %>%\n  mutate(stage_no=1:n())\n\n## ---- prepare plot data ---------------------------------------------------------------\n\n# The approach to getting lines to fade in gganimate is adapted from:\n# stackoverflow.com/questions/58271332/gganimate-plot-where-points-stay-and-line-fades\nplot_data <- tdf_stages %>%\n  uncount(nrow(tdf_stages), .id = \"frame\") %>%\n  filter(stage_no <= frame) %>%\n  arrange(frame, stage_no) %>%\n  group_by(frame) %>%\n  mutate(\n    # which edition of the TdF does the current frame visualise?\n    frame_year = max(year),\n    # how many stages ago was the stage first visualised created?\n    tail = last(stage_no) - stage_no,\n    # transparency set to be 1 (full) for first visualiation, and then fade.\n    # size set to be larger within same edition of TdF, and then shrink.\n    point_alpha = if_else(tail == 0, 1, 0.7),\n    point_size = if_else(year == frame_year, 2, 1),\n    # Fade lines over 20 stages, and remove once a new edition of the TdF starts.\n    segment_alpha = pmax(0, (20-tail)/20) * (year == frame_year)\n  ) %>%\n  ungroup()\n\n\n## ---- create plot ---------------------------------------------------------------------\n\n# raw (non-animated) plot\np <- ggplot(plot_data) +\n  \n  # used to dynamically add the edition of the TdF\n  geom_text(\n    data=plot_data, aes(-4, 50.5, label = frame_year),\n    size = 12, hjust = 0, color =infreq_palette[\"darkblue\"]\n  ) +\n  \n  # background map of france (coord_map fixes the aspect ratio)\n  geom_polygon(\n    data = map_data(\"france\"), aes(x=long, y = lat, group = group),\n    fill = \"#f3be02\", color = \"#f6f4e6\", size = 0.1\n  ) +\n  coord_map() +\n  \n  # stage start and finish locations as points\n  geom_point(\n    aes(x=start_longitude,y=start_latitude, alpha = point_alpha),\n    size = plot_data$point_size\n  ) +\n  geom_point(\n    aes(x=finish_longitude,y=finish_latitude,group=stage_no, alpha = point_alpha),\n    size = plot_data$point_size\n  ) +\n  \n  # line segment connecting stage start/finish points.\n  geom_segment(\n    aes(\n      x=start_longitude,\n      xend=finish_longitude,\n      y=start_latitude,\n      yend=finish_latitude,\n      alpha = segment_alpha\n    ), color = infreq_palette[\"orange\"]\n  ) +\n  \n  # set themes and legend\n  scale_alpha(range = c(0,1)) +\n  guides(alpha = \"none\") +\n  theme_void() +\n  transition_manual(frame)\n\n# create animation; speed is set to display 10 stages per second; completes in ~4mins.\nanim_stages <- animate(p,\n  nframes = max(plot_data$frame),\n  duration = max(tdf_stages$stage_no) * 0.1)\n\n\n\n\n\n\nComments\nAcknowledgments\nI first became interested in visualising how the Tour de France had changed over time after coming across Pierre Breteau’s visualisation in Le Monde, which shows the frequency with which each département of France has been visited by the race.\nTo create the data set I’m indebted to the contributors to Wikipedia for their efforts to tabulate all of the stages in the Tour’s history.\nTo obtain and visualise the data I relied heavily on R, and and in particular the work of the creators of the rvest, gganimate, and tidygeocoder packages.\n\n\n\n",
    "preview": "posts/2021-06-25-tdf-shape-21/img/tdf_shape_1915.gif",
    "last_modified": "2021-07-02T20:13:59+01:00",
    "input_file": {}
  }
]
