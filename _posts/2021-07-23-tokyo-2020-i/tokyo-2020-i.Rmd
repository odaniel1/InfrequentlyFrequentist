---
title: "Tokyo 2020 Betting I: Predictive Models for Pairwise Matches"
description: |
  The first in a series of posts building towards me betting on the track cycling at the Tokyo Olympics. This post introduces the series, and the basic model behind my attempt to win big at the bookies!
date: 07-23-2021
preview: img/strengths_wc20.png
output:
  distill::distill_article:
    highlight: /home/od/Documents/R Projects/InfrequentlyFrequentist/infreq.theme
    toc: true
    toc_depth: 1
  self_contained: false
categories:
  - Bayesian
  - Cycling
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, code_folding = TRUE)
options(kableExtra.html.bsTable = T)

library(tidyverse)
library(lubridate)
library(infreqthemes)
library(xaringanExtra)
library(knitr)
library(kableExtra)
library(fst)

xaringanExtra::use_panelset()

remote_project_path <- '~/Documents/R Projects/track-cycling/'
remote_targets_path <- paste0(remote_project_path, '_targets/objects/')

read_remote_target <- function(name, path = remote_targets_path){
  
  file_path <- paste0(path,name)
  
  tar <- try(read_rds(file_path),silent=TRUE)
  
  if(class(tar) != 'try-error'){return(tar)}
  
  tar <- try(read_fst(file_path),silent=TRUE)
  
  if(class(tar) != 'try-error'){return(tar)}
  else{stop("Failed to read remote target")}
}

```

The Olympics are officially under way, and I'm primed to become an armchair expert in sports that I haven't expressed any interest in for four years (I'll have to wait another 6 months before I crack out my curling top tips though). As a more proactive way to engage with the games I thought I'd explore how to integrate predictive modelling with betting strategies.

Statistical models are common place amongst bookmakers, and I'm not naive enough to believe that a model I threw together over a few days as a hobby could beat a concerted effort by a team of experienced sports analysts. I first developed this project for the 2019 Track Cycling World Championships, but it turned out UK bookmakers weren't offering odds, let alone those derived from machine learning: so that gives me hope that a team of dedicated track cyling modelers simply doesn't exist!

I'm tentatively optimistic odds will be available, after all the olympics. It'd be particularly fitting given cycling betting is one of only four [legal](https://iclg.com/practice-areas/gambling-laws-and-regulations/japan) betting markets in Japan.

<aside>[This](https://www.bbc.co.uk/sport/cycling/52283598) article/mini-documentary depicts the cultural significance of track cycling in Japan.
</aside>

In this post I'll provide a brief introduction to the event I'm hoping to bet on, and the base statistical model which for my betting strategy.

Subsequent posts will refine the model through feature engineering, and then derive the betting strategy itself. If the bookies play-ball and provide odds, I'll place some bets and summarise how this experiment turned out!

I'm also hoping this series will serve as an example of my analytical workflow: to that end I'll explain some of the errors and simplifications I made along the way, how I spotted and resolved them.

<aside>There are likely modelling errors I haven't noticed yet, comments are welcome!</aside>

Not least I'll say up front this work might not yield usable odds! Like many predictive models, this one is susceptible to Covid 19 impacts: putting aside the model itself, most of the track cycling calendar for 2020 and 2021 was cancelled, or attended by few athletes, so we simply don't have much recent data to feed it.

But I won't let that spoil the fun, after all you can't win if you don't play the game...


## Assumptions
This post makes some assumptions about you: that you have some prior exposure to regression, and evaluation metrics for classification models.

I'll assume you're familiar with logistic regression and evaluating predictive performance with the *log-loss* - which I'll tweak for the specifics of the competition format.

If you're interested in reading underlying code, you'll likely need some experience (or be keen to learn) a mix of R and Stan.

# Track Sprinting and Data Overview

A rule of thumb in modelling the outcome of sports matches is that rather than predicting binary win/lose probabilities, its better to predict expected score (or time) differences and infer the winner from this.

As Andrew Gelman [puts it](https://statmodeling.stat.columbia.edu/2014/02/25/basketball-stats-dont-model-probability-win-model-expected-score-differential/) (in the context of election modelling): *there's a lot of information in the score (or vote) differential thatâ€™s thrown away if you just look at win/loss.*

The Individual Sprint discipline in track cycling is interesting from a modelling perspective, as it forces us to diverge from this wisdom.

The sprint is contested by two athletes with the winner being the first across the line. Unlike its track and field equivalent, the sprint isn't contested from the start: the race starts slowly with a tactical game of cat-and-mouse, before one rider tries to catch the other unaware and starts their sprint. This dynamic renders race times meaningless for prediction.

<aside>A great [example](https://www.youtube.com/watch?v=CkmmXSs-ooQ) is the London 2012 women's sprint final between Victoria Pendleton and Anna Meares.</aside>


## About the Data

Detailed match results for most large track cycling events are maintained by [Tissot](https://www.tissottiming.com/). These results are made available in a common PDF format, that I have parsed to extract structured data from.

<aside>This work is available separately on [GitHub](https://github.com/odaniel1/tissot-scraper)</aside>

```{r matches, echo = FALSE}
# read match data from remote _target store
matches <- read_remote_target("matches")

# summary info
matches_summary <- matches %>%
  summarise(
    min_date = min(date),
    max_date = max(date),
    span_days = as.numeric(max_date - min_date),
    n_matches = n(),
    n_riders = n_distinct(rider_id_1) + n_distinct(setdiff(rider_id_2,rider_id_1))
  )
```

The  data I'm using spans ~`r round(matches_summary$span_days/365)` years from `r format(matches_summary$min_date, '%B %Y')` to  `r format(matches_summary$max_date, '%B %Y')`, and for this post I'll focus on the women's competition. During that period I have data for a total of `r matches_summary$n_matches` matches between `r matches_summary$n_riders` athletes.

The sample below shows the fields that are immediately relevant to the initial model I'll fit in this post; I'll explore other information I could extract from the result PDFs in later model fits.

```{r matches-sample, echo = FALSE}
matches_sample <-  matches %>%
  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%
  mutate(across(everything(), as.character)) %>%
  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id) %>%
  slice(1)

matches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))
matches_sample %>%
  kable() %>%
  kable_styling(full_width=FALSE, bootstrap_options = c("striped", "hover", "condensed"),font_size = 10)
```

<aside>The rider ID fields are derived, and are used for numeric indexing in the models</aside>

In all model runs I'll use data up to the end of 2019 for training, and then evaluate against data from 2020 onward: a total of `r matches %>% filter(split == 'training') %>% nrow()` matches in the training data, and `r matches %>% filter(split == 'evaluation') %>% nrow()` held out for evaluation.

# The Bradley-Terry Model

The Bradley-Terry model is a well studied approach to paired comparisons, and particularly to win/lose matches, which originates in work by [Zermelo](https://link.springer.com/article/10.1007/BF01180541), who used it to compare chess players abilities.

<aside>The model is the theoretical underpinning of the Elo Rating system used in chess ladders.</aside> 

The model was re-discovered and popularised by [Bradley and Terry](https://www.jstor.org/stable/2334029) in the 1950s - their example application is somewhat more esoteric, as they used it to rank pork roasts!

I'll introduce the model in the context of athletes competing in the Individual Sprint. Each of $R$ athletes involved in the league are assumed to have some strength $\beta_r$, $r = 1,\ldots, R$, that describes their ability.

The Bradley-Terry model assumes that a rider $r$ beats $s$ with probability

$$\mathbf P \left[ r \text{ beats } s\right] = \frac{\beta_r}{\beta_r + \beta_s}.$$

The aim is to estimate the strength parameters from historical matches, and then use these to predict future games.

One useful step is to change the scale of the strength parameters, considering $\alpha = \log \beta$, which changes the model to

$$ 
\begin{align*}
\mathbf P \left[ r \text{ beats } s\right] & = \frac{e^{\alpha_r}}{e^{\alpha_r} + e^{\alpha_s}} \\
& = \frac{e^{\alpha_r - \alpha_s}}{1 + e^{\alpha_r - \alpha_s}} \\
& = \text{logit}^{-1}\big(\alpha_r - \alpha_s\big)
\end{align*}
$$

This transformation has converted a complex *fractional* relationship between rider strengths into a linear relationship, inside a *link* function.

Specifically, this model is now equivalent to a classic logistic regression problem. To see this more clearly we can look at how we might implement this in code:

We create a data frame with a row per match, and $R + 1$ columns: one for each athlete, and one with the match outcome. The match between $r$ and $s$ then has 0 entries in each of the rider columns except for an entry of +1 in the column for $r$, and -1 for $s$. Finally the result column is set to 1 if the rider marked as +1 wins, and 0 if -1 wins.

<aside> Ex. $\left(0,0,0,1,0,-1,0 ,\, \underline{1}\right)$ would denote a match between riders 4 and 6, in a league of 7, in which rider 4 won.</aside> 

This could now be passed to your favourite tool for logistic regression (eg. `glm` in R) and you would fit the Bradley-Terry model above. Under the hood that will run an optimisation algorithm to find the vector of strengths $\alpha^*$ that maximises the likelihood.

There is a challenge however, as this solution won't be unique: any translation by a constant $\alpha^* + \text{constant}$ will have the same likelihood. In the frequentist approach this is resolved by introducing a constraint that $\sum_r \alpha_r = 0$.

<aside>[This](https://stats.stackexchange.com/questions/3143/linear-model-with-constraints) CrossValidated post has good answers explaining how to achieve this with R's `glm` function.</aside>

## The Bayesian Bradley-Terry Model

Rather than following the frequentist approach, I'll fit a Bayesian Bradley-Terry model. My general rationale for working with Bayesian models is [here](www.infreq.com/about/#why-infrequently-frequentist), but in this instance there's an added incentive as it opens an alternative method to resolving the identifiability issue above.

The idea is that rather than forcing the coefficients to sum to 0, we can use priors on the rider strengths that helps the likelihood to focus on one specific translation over all others. In our case we'll assume athlete strengths are normally distributed, with mean $0$, which will lean the model towards configurations with $\sum_r \alpha_r = 0$, without the need to code up this constraint explicitly.

<aside> This is referred to as *soft centering* in the Stan guidance, and is explained alongside other constraint methods [here](https://mc-stan.org/docs/2_27/stan-users-guide/parameterizing-centered-vectors.html)</aside>

Rather than specifying the standard deviation for the prior I will treat this as an unknown *hyperparameter*, $\sigma$. This is known as [partial pooling](https://mc-stan.org/rstanarm/articles/pooling.html#partial-pooling), or hierarchical effects, and has the effect of ensuring the athlete strengths are on a common scale.

The prior for $\sigma$ needs to be constrained to be positive (since standard deviation is positive), and for now I'll  pick a [common](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations#general-principles) default of a half-normal distribution, with variance of 1. Putting this together we have the following model specification:

::::: {.panelset}

::: {.panel}
[BT1]{.panel-name}
$$
\begin{align*}
\bf{\text{Priors}}\\
\sigma & \sim \text{Half-Normal}(0,1) \\
\alpha_r & \sim \text{Normal}(0,\sigma) \\
\\
\bf{\text{Likelihood}}\\
\alpha_r - \alpha_s | W_{r,s} &\sim \text{logit}^{-1}(\alpha_r - \alpha_s)
\end{align*}
$$
:::

::: {.panel}
[Example Data]{.panel-name}
```{r}
matches_sample <-  matches %>%
  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%
  mutate(across(everything(), as.character)) %>%
  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id) %>%
  slice(1)

matches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))
matches_sample %>%
  kable("pipe") %>%
  kable_styling(full_width=FALSE, bootstrap_options = c("striped", "hover", "condensed"),font_size = 10)
```
:::
  
::: {.panel}
[Stan code]{.panel-name}

```{r, code_folding = FALSE}
writeLines(readLines(paste0(remote_project_path, 'stan/bt1.stan')))
```

:::
  
:::::
  
<aside>**BT1**. Basic Bradley-Terry model.</aside>

Throughout this series I'll fit the models using [Stan](https://mc-stan.org/). The code for the model is available in the tab above. In these posts I'll focus less on the implementation, and more on the outputs of the models.

<aside>The full model code, including the R pipeline to handle multiple models, is [here](www.github.com/odaniel1/track-cycling).</aside>
  
The plot below shows the posterior 90% credible intervals for each rider's strength, for clarity I've filtered the data down to those riders who have competed since the start of 2020.

```{r p_strengths_bt1, fig.height = 8, warning = FALSE, message = FALSE}
riders <- read_remote_target("riders")

bt1_summ <- read_remote_target("bt_summary_bt1")

bt1_strength_summ <- bt1_summ %>%
  filter(str_detect(variable, 'alpha')) %>%
  mutate(rider_id = str_match(variable, '\\[(.*)\\]')[,2] %>% as.numeric()) %>%
  left_join(riders) %>%
  mutate(rider_name = fct_reorder(rider_name,median)) %>%
  filter(max_date >= ymd(20200101))

p_strengths_bt1 <- ggplot(bt1_strength_summ) +
  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name), color = infreq_palette["darkblue"]) +
  labs(y="",x="Strength, Î±")

p_strengths_bt1
```

```{r preview-plot,   echo = FALSE, eval = FALSE}
riders <- read_remote_target("riders")

riders_WC20 <- matches %>% filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS') %>%
  select(rider_1,rider_2)

bt1_strength_summ <- bt1_summ %>%
  filter(str_detect(variable, 'alpha')) %>%
  mutate(rider_id = str_match(variable, '\\[(.*)\\]')[,2] %>% as.numeric()) %>%
  left_join(riders) %>%
  mutate(rider_name = fct_reorder(rider_name,median)) %>%
  filter(rider_name %in% c(riders_WC20$rider_1, riders_WC20$rider_2))

p_preview <- ggplot(bt1_strength_summ) +
  geom_segment(aes(x=q5,xend=q95,y=rider_name,yend=rider_name), color = infreq_palette["darkblue"]) +
  labs(y="",x="Strength, Î±")

p_preview

ggsave('img/strengths_wc20.png', plot = p_preview, device = 'png', width = 14, height = 13, units = "cm")
```

From a first pass the strengths seem to fit what we might expect: for instance the top four riders have all appeared in the Finals of the World Championships in either 2019 or 2020.

We can also see riders, such as Nicole Rodriguez, who appear in the evaluation date but not the training data: they have wider uncertainty intervals as the model can only assume they are *average* in absence of any data.

The next question is whether the scale of the strengths looks right. A good test scenario for this is to compare the weakest and strongest riders: suppose those two riders were to compete, what odds would we put on the weaker rider winning?

The table below summarises the posterior odds for that extremal match

```{r}
bt1_max_odds <- bt1_summ %>%
  filter(str_detect(variable, 'delta_max')) %>%
  select( q5, median, q95) %>%
  mutate(
    # calculation is based on the fact that the strength difference is the log odds
    # on the exponential scale
    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = ","))
  )

bt1_max_odds %>%
  kable(col.names = c( "5%", "50%", "95%"), digits = 2) %>%
  kable_styling(full_width=FALSE)
```
<aside> Figures should be read as *1 in ...*</aside>

The model is incredibly sceptical that the weakest rider could beat the strongest: I certainly can't imagine a bookmaker offering odds on these scales!

Does the data genuinely support such high odds, or is this behaviour being encouraged by the somewhat arbitrary choice of prior distribution?

# Prior Predictive Checks

Prior predictive checks provide results from the model in absence of any data: i.e. using only our prior assumptions. They provide a good test for whether the model is sensibly defined, and in particular whether the prior distributions you're using are suitable.

The plot below shows the prior distribution for the odds that the weakest rider beats the strongest. 

<aside>A similar example of side effects of default priors on large parameter spaces is given in Gelman et al.'s Bayesian workflows [paper](https://arxiv.org/pdf/2011.01808.pdf), Fig. 3.
</aside>

```{r max_diff_halfnormal}
sigma_draws_halfnormal <- 
  tibble(draw = 1:10000, hyperprior = 'HalfNormal(0,1)') %>% mutate(sigma = abs(rnorm(n())))

prior_draws_halfnormal <- crossing(rider = 1:nrow(riders), sigma_draws_halfnormal) %>%
  mutate(alpha = rnorm(n(), sd = sigma))

max_diff_halfnormal <- prior_draws_halfnormal %>%
  group_by(draw,hyperprior) %>%
  summarise(
    alpha_max = max(alpha),
    alpha_min = min(alpha),
    max_diff = alpha_max - alpha_min,
    odds_weaker_wins_log10 = max_diff / log(10)
  ) %>%
  ungroup()
 
ggplot(max_diff_halfnormal, aes(odds_weaker_wins_log10)) + geom_histogram(binwidth = 0.1, color = infreq_palette["beige"]) +
  scale_x_continuous(breaks = seq(0,8,by = 2), labels = scales::math_format(10^.x)) +
  labs(x = 'Odds Weakest Beats Strongest', y = '')  +
  theme(axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank())
```

The prior spans several orders of magnitude: putting mass on odds from 1-1, up to 1-1,000,000.

In hindsight this feels too vague, and the long tail might be promoting the model to tend towards extreme scenarios.


# An Informative Hyperprior

My personal instinct is that the odds should be more in the range of 1-100 up to 1-1,000, so as a first model development I'll introduce a more informative prior to reflect my personal intuition about the scale of the odds.

Since the model is defined in terms of strengths I'll need to convert that range of 1-100 to 1-1,000 odds to a range for the difference in strengths. In practice this conversion is almost immediate as the strength differences are none-other than the (base-$e$) logarithm of the odds. That's just the magic of logistic regression!

So my preferred odds range of between 1-100 and 1-1,000 translates to a maximum difference in athlete's strength of between `r round(2/log10(exp(1)),1)` and  `r round(3/log10(exp(1)),1)`.

<aside>Eg. $1-10^k$ odds are equivalent to a strength difference of  $k/\log_{10}(e^1) \sim 0.43k$.</aside>

I've decided to work with a Gamma distribution for the hyperprior, and settled on shape and rate parameters of 80, and 60 respectively: these were chosen by trial and error, to create a prior distribution that nicely spanned the strength range above.

::::: {.panelset}

::: {.panel}
[Ïƒ - Hyperprior]{.panel-name}
```{r}
sigma_draws_gamma <- tibble(draw = 1:10000, hyperprior = 'Gamma(80,60)') %>% mutate(sigma = rgamma(n(), 80, 60))

sigma_draws <- bind_rows(sigma_draws_gamma, sigma_draws_halfnormal)

ggplot(sigma_draws, aes(sigma,fill=hyperprior)) + 
  geom_histogram(aes(y=..density..),binwidth = 0.05, color = infreq_palette["beige"]) +
  scale_x_continuous(limits = c(0,3)) +
  facet_grid(rows = vars(hyperprior)) +
  labs(x = 'Ïƒ', y = '') + 
  theme(
    axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank(),
          strip.background = element_blank(), strip.text = element_blank(), legend.title = element_blank()
    )
```
:::
  
::: {.panel}
[Prior on Maximum Strength Difference]{.panel-name}

```{r}
prior_draws_gamma <- crossing(rider = 1:nrow(riders), sigma_draws_gamma) %>%
  mutate(alpha = rnorm(n(), sd = sigma))

max_diff_gamma <- prior_draws_gamma %>%
  group_by(draw,hyperprior) %>%
  summarise(
    alpha_max = max(alpha),
    alpha_min = min(alpha),
    max_diff = alpha_max - alpha_min,
    odds_weaker_wins_log10 = max_diff / log(10)
  )

max_diff <- bind_rows(max_diff_gamma, max_diff_halfnormal)


ggplot(max_diff, aes(max_diff,fill=hyperprior)) +
  geom_histogram(aes(y=..density..),binwidth = 0.25, color = infreq_palette["beige"]) +
  scale_x_continuous(limits = c(0,15)) +
  facet_grid(rows = vars(hyperprior)) +
  # scale_x_continuous(breaks = seq(0,8,by = 2), labels = paste0("10^",seq(0,8,by = 2) )) +
  labs(x = 'Max. Strength Difference', y = '') +
  theme(
    axis.ticks.y = element_blank(), axis.text.y=element_blank(), axis.line.y = element_blank(),
          strip.background = element_blank(), strip.text = element_blank(), legend.title = element_blank()
    )
```

:::
  
:::::

This gives us the minor change to our original model specification:


::::: {.panelset}

::: {.panel}
[BT2]{.panel-name}
$$
\begin{align*}
\bf{\text{Priors}}\\
\sigma & \sim \text{Gamma}(80,60) \\
\alpha_r & \sim \text{Normal}(0,\sigma) \\
\\
\bf{\text{Likelihood}}\\
\alpha_r - \alpha_s | W_{r,s} &\sim \text{logit}^{-1}(\alpha_r - \alpha_s)
\end{align*}
$$
:::

::: {.panel}
[Example Data]{.panel-name}
```{r}
matches_sample <-  matches %>%
  filter(event == '2020 UCI TRACK WORLD CYCLING CHAMPIONSHIPS', round == 'Finals', gender == 'Women') %>%
  mutate(across(everything(), as.character)) %>%
  select(event, date, gender, round, rider_1,rider_2,rider_id_1,rider_id_2, winner_id,loser_id) %>%
  slice(1)

matches_sample <- tibble(Field = names(matches_sample), Example = unlist(matches_sample[1,]))
matches_sample %>%
  kable("pipe") %>%
  kable_styling(full_width=FALSE, bootstrap_options = c("striped", "hover", "condensed"),font_size = 10)
```
:::

::: {.panel}
[Stan code]{.panel-name}

```{r, code_folding = FALSE, echo = FALSE}
writeLines(readLines(paste0(remote_project_path, 'stan/bt2.stan')))
```

:::
  
:::::
  
<aside>**BT2**. Basic Bradley-Terry model, with Gamma hyperprior.</aside>

Visually the plot of posterior strengths looks pretty similar under the revised model to the original, so I'll skip presenting it.

However, the change in prior does have a material impact on the maximum odds statistic, this is summarised in the table below which shows both the prior and posterior ranges for this statistic, for the two models.

```{r}
bt2_summ <- read_remote_target("bt_summary_bt2")

bt2_max_odds <- bt2_summ %>%
  filter(str_detect(variable, 'delta_max')) %>%
  select(q5, median, q95) %>%
  mutate(
    # calculation is based on the fact that the strength difference is the log odds
    # on the exponential scale
    across(everything(), ~format(round(10^(./log(10)),-2),big.mark = ","))
  )

prior_max_odds <- max_diff %>%
  mutate(model = if_else(hyperprior == 'Gamma(80,60)', 'bt2 - Prior', 'bt1 - Prior')) %>%
  group_by(model) %>%
  summarise(
    q5 = quantile(10^odds_weaker_wins_log10, 0.05) %>% round(-1) %>% format(big.mark = ","),
    median = quantile(10^odds_weaker_wins_log10,0.5) %>% round(-1) %>% format(big.mark = ","),
    q95 = quantile(10^odds_weaker_wins_log10, 0.95) %>% round(-1) %>% format(big.mark = ",")
  )

bind_rows(
  bt1_max_odds %>% add_column(model = 'bt1 - Posterior', .before = 0),
  bt2_max_odds %>% add_column(model = 'bt2 - Posterior', .before = 0),
  prior_max_odds
)  %>%
mutate(
  model = factor(model, levels = c("bt1 - Prior", "bt1 - Posterior", "bt2 - Prior", "bt2 - Posterior"))
)%>%
  arrange(model) %>%
  kable(col.names = c("", "5%", "50%", "95%"), digits = 2) %>%
  kable_styling(full_width=FALSE)
```

<aside> Figures should be read as *1 in ...*</aside>

In both cases we can see that the posterior distributions are quite different from the priors. In the case of BT2 we can see that the extreme behaviour allowed by the naive prior choice has been reined in (even then its still much higher than my prior thought).

# Evaluation Metrics

With two models to hand, its time to consider how we can choose between the two; I'll consider two formal evaluation metircs. Model accuracy answers the simple question *If you always bet on the stronger athlete, how often would you win?*. The log loss gives a more nuanced measure: penalising predictions when the model put a very low probability on the actual observed outcome.

<aside>My accuracy measure implicitly assumes a cutoff of 0.5; using a different cut-off is important when there is class imbalance, but our data is naturally balanced as every match has 1 winner and 1 loser.</aside>

$$
\begin{align}
\text{Accuracy} & = \text{Prop. of matches where the modelled stronger rider wins.} \\
\text{Log Loss} & = \text{Average log-probability assigned to the observed outcome.} \\
\end{align}
$$

The definition of the log loss is equivalent to the log likelihood of the data, but divided by the number of data points so that we can more readily compare training/test performance.

<aside> Log loss is normally multiplied by -1 to be positive: I avoid this convention as this way better models will</aside>

As a benchmark for model performance we can compare to the model in which we randomly guess the winner. On average this algorithm would have an accuracy of $\frac12$ (as we'd expect half of its guesses to be correct), and the log loss is fixed at $\log(\frac12) \approx -0.693$.

Models with better predictive power will have higher accuracy (closer to 1), and higher log loss (closer to 0).

::::: {.panelset}

::: {.panel}
[Accuracy]{.panel-name}
```{r}
bt1_measures <- bt1_summ %>%
  filter(str_detect(variable,'(accuracy|log_loss)\\[6\\]')) %>%
  extract(variable, into = c("split", "measure"), "(.*?)_(.*)\\[6\\]") %>%
  mutate(
    model = 'bt1',
    model_split = paste(model, "-", split)
  )

bt2_measures <- bt2_summ %>%
  filter(str_detect(variable,'(accuracy|log_loss)\\[6\\]')) %>%
  extract(variable, into = c("split", "measure"), "(.*?)_(.*)\\[6\\]") %>%
  mutate(
    model = 'bt2',
    model_split = paste(model, "-", split)
  )

measures <- bind_rows(bt1_measures, bt2_measures)
```

```{r accuracy_table, echo = FALSE}
accuracy <- measures %>%
  filter(measure == "accuracy")

accuracy %>% 
  select(model, split,  q5, median, q95) %>%
  arrange(split, model) %>%
  kable("pipe", col.names =c("Model", "Split", "5%", "50% (Median)", "95%"), digits =3) %>%
  kable_styling(full_width = FALSE) %>%
  collapse_rows(1)
```

```{r accuracy_plot, echo = FALSE}
ggplot(accuracy, aes(y=fct_rev(model))) +
  facet_grid(rows = vars(split)) +
  geom_point(aes(x=median), color= infreq_palette["darkblue"], size =2) +
  geom_segment(aes(x=q5,xend=q95,yend=model), color= infreq_palette["darkblue"]) +
  scale_x_continuous(limits = c(0,1)) +
  labs(x= "Accuracy", y="") +
  theme(axis.line.y=element_blank(), axis.text.y=element_text(size=12),
        strip.text=element_text(size = 14, color = infreq_palette["darkblue"]),
        strip.background = element_rect(color=infreq_palette["beige"], fill=infreq_palette["beige"]),
        legend.position = "none"
  ) +
  geom_vline(xintercept=0.5, linetype = "dashed", color = infreq_palette["orange"]) + 
  geom_text(data = tibble(x=0.40,y=0.6,split = 'training', text = 'Benchmark', model = 'bt2'), aes(x=x,label=text, color = infreq_palette["orange"],size=3))
```
:::
  
::: {.panel}
[Log Loss]{.panel-name}

```{r log_loss_table}
log_loss <- measures %>%
  filter(measure == "log_loss")

log_loss %>% 
  select(model, split,  q5, median, q95) %>%
  arrange(split, model) %>%
  kable("pipe", col.names =c("Model", "Split", "5%", "50% (Median)", "95%"), digits =3)
```

```{r log_loss_plot, echo = FALSE}
ggplot(log_loss,aes(y=fct_rev(model))) +
  facet_grid(rows = vars(split)) +
  geom_point(aes(x=median), color= infreq_palette["darkblue"], size =2) +
  geom_segment(aes(x=q5,xend=q95,yend=model), color= infreq_palette["darkblue"]) +
  scale_x_continuous(limits = c(-1.3,0)) +
  labs(x= "Log Loss", y = "") +
  theme(axis.line.y=element_blank(), axis.text.y=element_text(size=12),
        strip.text=element_text(size = 14, color = infreq_palette["darkblue"]),
        strip.background = element_rect(color=infreq_palette["beige"], fill=infreq_palette["beige"]),
        legend.position = "none"
  ) +
  geom_vline(xintercept=log(1/2), linetype = "dashed", color=infreq_palette["orange"]) + 
  geom_text(data = tibble(x=log(1/2) - 0.2,y=0.6,split = 'training', text = 'Benchmark', model = 'bt2'), aes(x=x,label=text, color = infreq_palette["orange"],size=3))
```
:::
  
:::::

Both models have accuracies that are well above the benchmark, indicating that using either model is better than random guessing, though there's little distinction between the two models. This is to be expected as the accuracy will only change if the two models differ in their opinion about who the stronger rider is in each match, and we wouldn't expect that to happen in many cases.

The log loss is more interesting: on the training data the log loss is better than guessing, but this isn't the case in the evaluation data. Amongst the evaluation data, there's some weak evidence that the Gamma prior (bt2) fairs marginally better.

This poor performance in log loss is driven by *upsets*: where the model is highly confident in one athlete winning, and then they don't. For instance suppose a model thinks that a given athlete will win with probability 0.999, if they then fail to win that contributes a factor of $\log(1 - 0.999) \approx `r round(log(1 - 0.999),3)`$ to the log loss, as opposed to if they win in which case the contribution is $`r round(log(0.999),3)`$.

The Gamma prior fairs better as it was designed to lead the model away from scenarios where it offers astronomical odds.

# Next Steps

In this post I've introduced the basic Bradley-Terry model, and shown that it out performs a strategy of random guessing. 

In the next post I'll start to deviate from the standard model, accounting for features available in the data that should help to improve the predictive power of the model.

# Comments

## Acknowledgments {.appendix}

Previous implementations of Bradley-Terry models using Stan have been posted by [Bob Carpenter](https://github.com/stan-dev/example-models/blob/master/knitr/bradley-terry/bradley-terry.Rmd) and [opisthokonta.net (Jonas)](https://opisthokonta.net/?p=1589).

This post makes heavy use of R and Stan, and so benefits from the many people who contribute to their development.